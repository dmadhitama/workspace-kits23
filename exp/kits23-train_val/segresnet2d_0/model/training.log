_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 1
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: false
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: false
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 4
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1721GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 1 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/293 loss: 2.8608 acc [ 0.361  0.417  0.172]  time 13.43s 
Epoch 0/600 1/293 loss: 2.1998 acc [ 0.234  0.417  0.172]  time 0.03s 
Epoch 0/600 2/293 loss: 2.2769 acc [ 0.288  0.395  0.178]  time 0.02s 
Epoch 0/600 3/293 loss: 2.0827 acc [ 0.288  0.395  0.178]  time 0.02s 
Epoch 0/600 4/293 loss: 1.9724 acc [ 0.261  0.395  0.178]  time 0.02s 
Epoch 0/600 5/293 loss: 2.0617 acc [ 0.333  0.327  0.163]  time 0.02s 
Epoch 0/600 6/293 loss: 2.1160 acc [ 0.355  0.307  0.177]  time 0.02s 
Epoch 0/600 7/293 loss: 2.0390 acc [ 0.355  0.307  0.177]  time 0.02s 
Epoch 0/600 8/293 loss: 1.9802 acc [ 0.332  0.267  0.155]  time 2.44s 
Epoch 0/600 9/293 loss: 1.9313 acc [ 0.298  0.267  0.155]  time 0.02s 
Epoch 0/600 10/293 loss: 1.9052 acc [ 0.319  0.231  0.135]  time 4.48s 
Epoch 0/600 11/293 loss: 1.8695 acc [ 0.303  0.204  0.117]  time 0.02s 
Epoch 0/600 12/293 loss: 1.8403 acc [ 0.295  0.181  0.104]  time 0.02s 
Epoch 0/600 13/293 loss: 1.8161 acc [ 0.292  0.169  0.096]  time 0.02s 
Epoch 0/600 14/293 loss: 1.7950 acc [ 0.292  0.169  0.096]  time 0.76s 
Epoch 0/600 15/293 loss: 1.7753 acc [ 0.286  0.154  0.096]  time 0.02s 
Epoch 0/600 16/293 loss: 1.7679 acc [ 0.287  0.158  0.087]  time 0.02s 
Epoch 0/600 17/293 loss: 1.7863 acc [ 0.297  0.173  0.091]  time 5.08s 
Epoch 0/600 18/293 loss: 1.7701 acc [ 0.293  0.160  0.091]  time 0.02s 
Epoch 0/600 19/293 loss: 1.7605 acc [ 0.301  0.159  0.089]  time 0.02s 
Epoch 0/600 20/293 loss: 1.7474 acc [ 0.301  0.152  0.084]  time 0.02s 
Epoch 0/600 21/293 loss: 1.7353 acc [ 0.303  0.146  0.079]  time 1.58s 
Epoch 0/600 22/293 loss: 1.7251 acc [ 0.303  0.146  0.079]  time 0.02s 
Epoch 0/600 23/293 loss: 1.7187 acc [ 0.300  0.146  0.078]  time 0.02s 
Epoch 0/600 24/293 loss: 1.7094 acc [ 0.301  0.143  0.075]  time 0.02s 
Epoch 0/600 25/293 loss: 1.7001 acc [ 0.295  0.138  0.072]  time 1.42s 
Epoch 0/600 26/293 loss: 1.6912 acc [ 0.303  0.138  0.072]  time 0.02s 
Epoch 0/600 27/293 loss: 1.6894 acc [ 0.302  0.137  0.072]  time 0.02s 
Epoch 0/600 28/293 loss: 1.6828 acc [ 0.313  0.131  0.068]  time 2.46s 
Epoch 0/600 29/293 loss: 1.6726 acc [ 0.311  0.131  0.068]  time 0.02s 
Epoch 0/600 30/293 loss: 1.6655 acc [ 0.309  0.131  0.068]  time 0.02s 
Epoch 0/600 31/293 loss: 1.6603 acc [ 0.309  0.131  0.068]  time 0.46s 
Epoch 0/600 32/293 loss: 1.6569 acc [ 0.324  0.125  0.068]  time 6.97s 
Epoch 0/600 33/293 loss: 1.6508 acc [ 0.317  0.120  0.065]  time 0.02s 
Epoch 0/600 34/293 loss: 1.6455 acc [ 0.313  0.118  0.063]  time 0.02s 
Epoch 0/600 35/293 loss: 1.6449 acc [ 0.307  0.118  0.062]  time 0.02s 
Epoch 0/600 36/293 loss: 1.6397 acc [ 0.307  0.116  0.060]  time 2.95s 
Epoch 0/600 37/293 loss: 1.6390 acc [ 0.322  0.116  0.060]  time 0.02s 
Epoch 0/600 38/293 loss: 1.6357 acc [ 0.321  0.115  0.058]  time 0.02s 
Epoch 0/600 39/293 loss: 1.6323 acc [ 0.321  0.115  0.058]  time 0.02s 
Epoch 0/600 40/293 loss: 1.6653 acc [ 0.328  0.118  0.061]  time 2.74s 
Epoch 0/600 41/293 loss: 1.6613 acc [ 0.329  0.117  0.059]  time 0.02s 
Epoch 0/600 42/293 loss: 1.6576 acc [ 0.329  0.117  0.059]  time 4.09s 
Epoch 0/600 43/293 loss: 1.6530 acc [ 0.336  0.114  0.057]  time 0.02s 
Epoch 0/600 44/293 loss: 1.6496 acc [ 0.336  0.114  0.057]  time 0.02s 
Epoch 0/600 45/293 loss: 1.6464 acc [ 0.336  0.114  0.057]  time 0.02s 
Epoch 0/600 46/293 loss: 1.6420 acc [ 0.331  0.111  0.055]  time 1.97s 
Epoch 0/600 47/293 loss: 1.6377 acc [ 0.335  0.109  0.054]  time 0.02s 
Epoch 0/600 48/293 loss: 1.6349 acc [ 0.335  0.109  0.054]  time 0.02s 
Epoch 0/600 49/293 loss: 1.6388 acc [ 0.338  0.110  0.054]  time 0.02s 
Epoch 0/600 50/293 loss: 1.6465 acc [ 0.342  0.112  0.054]  time 14.05s 
Epoch 0/600 51/293 loss: 1.6415 acc [ 0.344  0.109  0.053]  time 0.02s 
Epoch 0/600 52/293 loss: 1.6388 acc [ 0.344  0.109  0.053]  time 0.02s 
Epoch 0/600 53/293 loss: 1.6362 acc [ 0.344  0.109  0.053]  time 0.03s 
Epoch 0/600 54/293 loss: 1.6433 acc [ 0.356  0.112  0.054]  time 5.20s 
Epoch 0/600 55/293 loss: 1.6397 acc [ 0.351  0.109  0.053]  time 0.02s 
Epoch 0/600 56/293 loss: 1.6372 acc [ 0.351  0.109  0.053]  time 0.02s 
Epoch 0/600 57/293 loss: 1.6336 acc [ 0.348  0.107  0.051]  time 0.02s 
Epoch 0/600 58/293 loss: 1.6314 acc [ 0.348  0.107  0.051]  time 5.49s 
Epoch 0/600 59/293 loss: 1.6270 acc [ 0.348  0.105  0.050]  time 0.02s 
Epoch 0/600 60/293 loss: 1.6227 acc [ 0.346  0.103  0.049]  time 0.02s 
Epoch 0/600 61/293 loss: 1.6207 acc [ 0.346  0.103  0.049]  time 0.02s 
Epoch 0/600 62/293 loss: 1.6188 acc [ 0.346  0.103  0.049]  time 2.75s 
Epoch 0/600 63/293 loss: 1.6170 acc [ 0.346  0.103  0.049]  time 0.02s 
Epoch 0/600 64/293 loss: 1.6155 acc [ 0.349  0.101  0.048]  time 0.02s 
Epoch 0/600 65/293 loss: 1.6122 acc [ 0.352  0.101  0.048]  time 0.02s 
Epoch 0/600 66/293 loss: 1.6096 acc [ 0.350  0.099  0.048]  time 0.76s 
Epoch 0/600 67/293 loss: 1.6060 acc [ 0.350  0.097  0.048]  time 0.02s 
Epoch 0/600 68/293 loss: 1.6024 acc [ 0.352  0.095  0.048]  time 0.02s 
Epoch 0/600 69/293 loss: 1.6013 acc [ 0.349  0.095  0.047]  time 0.10s 
Epoch 0/600 70/293 loss: 1.5985 acc [ 0.350  0.095  0.047]  time 2.43s 
Epoch 0/600 71/293 loss: 1.5958 acc [ 0.353  0.094  0.046]  time 0.98s 
Epoch 0/600 72/293 loss: 1.5922 acc [ 0.356  0.094  0.046]  time 0.02s 
Epoch 0/600 73/293 loss: 1.5899 acc [ 0.352  0.092  0.046]  time 0.02s 
Epoch 0/600 74/293 loss: 1.5877 acc [ 0.359  0.091  0.045]  time 8.75s 
Epoch 0/600 75/293 loss: 1.5865 acc [ 0.359  0.091  0.045]  time 0.02s 
Epoch 0/600 76/293 loss: 1.5834 acc [ 0.363  0.089  0.044]  time 0.02s 
Epoch 0/600 77/293 loss: 1.5828 acc [ 0.365  0.089  0.044]  time 0.02s 
Epoch 0/600 78/293 loss: 1.5802 acc [ 0.364  0.088  0.043]  time 1.24s 
Epoch 0/600 79/293 loss: 1.5776 acc [ 0.364  0.086  0.043]  time 0.02s 
Epoch 0/600 80/293 loss: 1.5746 acc [ 0.367  0.086  0.043]  time 0.02s 
Epoch 0/600 81/293 loss: 1.5737 acc [ 0.367  0.086  0.043]  time 0.02s 
Epoch 0/600 82/293 loss: 1.5728 acc [ 0.367  0.086  0.043]  time 7.15s 
Epoch 0/600 83/293 loss: 1.5722 acc [ 0.365  0.085  0.043]  time 0.02s 
Epoch 0/600 84/293 loss: 1.5821 acc [ 0.368  0.086  0.043]  time 0.02s 
Epoch 0/600 85/293 loss: 1.5801 acc [ 0.368  0.085  0.043]  time 0.02s 
Epoch 0/600 86/293 loss: 1.5773 acc [ 0.367  0.084  0.042]  time 1.73s 
Epoch 0/600 87/293 loss: 1.5754 acc [ 0.364  0.083  0.042]  time 0.02s 
Epoch 0/600 88/293 loss: 1.5735 acc [ 0.371  0.081  0.041]  time 0.04s 
Epoch 0/600 89/293 loss: 1.5727 acc [ 0.371  0.081  0.041]  time 0.04s 
Epoch 0/600 90/293 loss: 1.5697 acc [ 0.372  0.080  0.041]  time 4.21s 
Epoch 0/600 91/293 loss: 1.5675 acc [ 0.374  0.079  0.040]  time 0.02s 
Epoch 0/600 92/293 loss: 1.5668 acc [ 0.374  0.079  0.040]  time 0.02s 
Epoch 0/600 93/293 loss: 1.5660 acc [ 0.374  0.079  0.040]  time 0.77s 
Epoch 0/600 94/293 loss: 1.5628 acc [ 0.376  0.079  0.040]  time 1.60s 
Epoch 0/600 95/293 loss: 1.5713 acc [ 0.382  0.080  0.041]  time 0.03s 
Epoch 0/600 96/293 loss: 1.5689 acc [ 0.381  0.080  0.041]  time 0.02s 
Epoch 0/600 97/293 loss: 1.5667 acc [ 0.379  0.079  0.041]  time 0.83s 
Epoch 0/600 98/293 loss: 1.5650 acc [ 0.378  0.079  0.041]  time 0.28s 
Epoch 0/600 99/293 loss: 1.5629 acc [ 0.376  0.078  0.040]  time 0.02s 
Epoch 0/600 100/293 loss: 1.5805 acc [ 0.380  0.078  0.041]  time 0.02s 
Epoch 0/600 101/293 loss: 1.5776 acc [ 0.380  0.077  0.040]  time 1.39s 
Epoch 0/600 102/293 loss: 1.5768 acc [ 0.380  0.077  0.040]  time 4.34s 
Epoch 0/600 103/293 loss: 1.5752 acc [ 0.384  0.076  0.040]  time 1.34s 
Epoch 0/600 104/293 loss: 1.5742 acc [ 0.380  0.076  0.040]  time 0.02s 
Epoch 0/600 105/293 loss: 1.5725 acc [ 0.379  0.075  0.039]  time 0.02s 
Epoch 0/600 106/293 loss: 1.5719 acc [ 0.376  0.075  0.039]  time 0.59s 
Epoch 0/600 107/293 loss: 1.5710 acc [ 0.372  0.074  0.039]  time 0.95s 
Epoch 0/600 108/293 loss: 1.5704 acc [ 0.375  0.074  0.039]  time 7.17s 
Epoch 0/600 109/293 loss: 1.5697 acc [ 0.375  0.074  0.039]  time 0.02s 
Epoch 0/600 110/293 loss: 1.5691 acc [ 0.375  0.074  0.039]  time 0.02s 
Epoch 0/600 111/293 loss: 1.5685 acc [ 0.375  0.074  0.039]  time 0.02s 
Epoch 0/600 112/293 loss: 1.5879 acc [ 0.380  0.074  0.040]  time 5.55s 
Epoch 0/600 113/293 loss: 1.5872 acc [ 0.380  0.074  0.040]  time 0.02s 
Epoch 0/600 114/293 loss: 1.5864 acc [ 0.383  0.074  0.040]  time 0.02s 
Epoch 0/600 115/293 loss: 1.5851 acc [ 0.383  0.074  0.040]  time 0.03s 
Epoch 0/600 116/293 loss: 1.5850 acc [ 0.386  0.074  0.040]  time 8.27s 
Epoch 0/600 117/293 loss: 1.5843 acc [ 0.386  0.074  0.040]  time 0.02s 
Epoch 0/600 118/293 loss: 1.5824 acc [ 0.388  0.073  0.040]  time 0.02s 
Epoch 0/600 119/293 loss: 1.5817 acc [ 0.388  0.073  0.040]  time 0.02s 
Epoch 0/600 120/293 loss: 1.5805 acc [ 0.392  0.072  0.039]  time 7.42s 
Epoch 0/600 121/293 loss: 1.5786 acc [ 0.393  0.072  0.039]  time 0.02s 
Epoch 0/600 122/293 loss: 1.5796 acc [ 0.393  0.072  0.040]  time 0.02s 
Epoch 0/600 123/293 loss: 1.5781 acc [ 0.392  0.072  0.040]  time 0.02s 
Epoch 0/600 124/293 loss: 1.5772 acc [ 0.388  0.071  0.040]  time 0.62s 
Epoch 0/600 125/293 loss: 1.5766 acc [ 0.388  0.071  0.040]  time 0.02s 
Epoch 0/600 126/293 loss: 1.5760 acc [ 0.388  0.071  0.040]  time 0.02s 
Epoch 0/600 127/293 loss: 1.5821 acc [ 0.391  0.071  0.041]  time 0.02s 
Epoch 0/600 128/293 loss: 1.5815 acc [ 0.391  0.071  0.041]  time 1.69s 
Epoch 0/600 129/293 loss: 1.5807 acc [ 0.390  0.071  0.041]  time 0.04s 
Epoch 0/600 130/293 loss: 1.5801 acc [ 0.390  0.071  0.041]  time 1.69s 
Epoch 0/600 131/293 loss: 1.5795 acc [ 0.390  0.071  0.041]  time 0.02s 
Epoch 0/600 132/293 loss: 1.5789 acc [ 0.390  0.071  0.041]  time 6.46s 
Epoch 0/600 133/293 loss: 1.5783 acc [ 0.390  0.071  0.041]  time 0.02s 
Epoch 0/600 134/293 loss: 1.5777 acc [ 0.390  0.071  0.041]  time 0.02s 
Epoch 0/600 135/293 loss: 1.5761 acc [ 0.389  0.071  0.041]  time 0.02s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 1
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: false
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: false
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 4
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1723GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 1 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/293 loss: 1.8965 acc [ 0.238  0.131  0.048]  time 11.52s 
Epoch 0/600 1/293 loss: 1.6946 acc [ 0.147  0.131  0.048]  time 0.03s 
Epoch 0/600 2/293 loss: 1.6297 acc [ 0.147  0.131  0.048]  time 0.02s 
Epoch 0/600 3/293 loss: 1.8155 acc [ 0.172  0.131  0.048]  time 0.02s 
Epoch 0/600 4/293 loss: 1.7524 acc [ 0.172  0.131  0.048]  time 0.02s 
Epoch 0/600 5/293 loss: 1.7103 acc [ 0.172  0.131  0.048]  time 0.02s 
Epoch 0/600 6/293 loss: 1.6877 acc [ 0.217  0.073  0.027]  time 0.02s 
Epoch 0/600 7/293 loss: 1.7078 acc [ 0.243  0.066  0.027]  time 2.55s 
Epoch 0/600 8/293 loss: 1.6845 acc [ 0.203  0.066  0.027]  time 0.02s 
Epoch 0/600 9/293 loss: 1.6601 acc [ 0.226  0.054  0.018]  time 0.02s 
Epoch 0/600 10/293 loss: 1.6455 acc [ 0.226  0.054  0.018]  time 0.02s 
Epoch 0/600 11/293 loss: 1.6299 acc [ 0.242  0.048  0.014]  time 0.89s 
Epoch 0/600 12/293 loss: 1.6177 acc [ 0.234  0.043  0.012]  time 0.02s 
Epoch 0/600 13/293 loss: 1.6093 acc [ 0.234  0.043  0.012]  time 0.02s 
Epoch 0/600 14/293 loss: 1.6177 acc [ 0.259  0.054  0.014]  time 1.57s 
Epoch 0/600 15/293 loss: 1.6104 acc [ 0.259  0.054  0.014]  time 0.02s 
Epoch 0/600 16/293 loss: 1.6010 acc [ 0.269  0.050  0.012]  time 0.02s 
Epoch 0/600 17/293 loss: 1.5941 acc [ 0.269  0.048  0.012]  time 0.02s 
Epoch 0/600 18/293 loss: 1.5892 acc [ 0.269  0.048  0.012]  time 13.80s 
Epoch 0/600 19/293 loss: 1.5827 acc [ 0.269  0.045  0.011]  time 0.02s 
Epoch 0/600 20/293 loss: 1.5879 acc [ 0.273  0.053  0.014]  time 0.02s 
Epoch 0/600 21/293 loss: 1.5821 acc [ 0.272  0.049  0.013]  time 0.02s 
Epoch 0/600 22/293 loss: 1.5786 acc [ 0.281  0.045  0.013]  time 1.60s 
Epoch 0/600 23/293 loss: 1.5816 acc [ 0.289  0.045  0.013]  time 0.02s 
Epoch 0/600 24/293 loss: 1.5791 acc [ 0.280  0.053  0.020]  time 0.02s 
Epoch 0/600 25/293 loss: 1.5834 acc [ 0.300  0.053  0.020]  time 0.02s 
Epoch 0/600 26/293 loss: 1.5904 acc [ 0.318  0.050  0.020]  time 7.76s 
Epoch 0/600 27/293 loss: 1.5852 acc [ 0.319  0.047  0.019]  time 0.02s 
Epoch 0/600 28/293 loss: 1.5842 acc [ 0.336  0.045  0.018]  time 0.02s 
Epoch 0/600 29/293 loss: 1.5825 acc [ 0.332  0.045  0.018]  time 0.02s 
Epoch 0/600 30/293 loss: 1.5799 acc [ 0.332  0.045  0.018]  time 10.43s 
Epoch 0/600 31/293 loss: 1.5774 acc [ 0.332  0.045  0.018]  time 0.02s 
Epoch 0/600 32/293 loss: 1.6196 acc [ 0.336  0.062  0.025]  time 0.02s 
Epoch 0/600 33/293 loss: 1.6168 acc [ 0.330  0.064  0.025]  time 0.02s 
Epoch 0/600 34/293 loss: 1.6219 acc [ 0.330  0.062  0.026]  time 8.94s 
Epoch 0/600 35/293 loss: 1.6169 acc [ 0.324  0.059  0.024]  time 0.02s 
Epoch 0/600 36/293 loss: 1.6138 acc [ 0.324  0.059  0.024]  time 0.02s 
Epoch 0/600 37/293 loss: 1.6108 acc [ 0.324  0.059  0.024]  time 0.02s 
Epoch 0/600 38/293 loss: 1.6072 acc [ 0.337  0.065  0.024]  time 3.66s 
Epoch 0/600 39/293 loss: 1.6049 acc [ 0.343  0.063  0.024]  time 0.02s 
Epoch 0/600 40/293 loss: 1.6074 acc [ 0.353  0.064  0.026]  time 0.02s 
Epoch 0/600 41/293 loss: 1.6024 acc [ 0.361  0.064  0.026]  time 0.02s 
Epoch 0/600 42/293 loss: 1.5961 acc [ 0.364  0.064  0.026]  time 0.95s 
Epoch 0/600 43/293 loss: 1.5939 acc [ 0.364  0.064  0.026]  time 0.02s 
Epoch 0/600 44/293 loss: 1.5921 acc [ 0.375  0.062  0.026]  time 0.02s 
Epoch 0/600 45/293 loss: 1.5901 acc [ 0.375  0.062  0.026]  time 0.02s 
Epoch 0/600 46/293 loss: 1.5935 acc [ 0.380  0.061  0.026]  time 3.02s 
Epoch 0/600 47/293 loss: 1.5915 acc [ 0.380  0.061  0.026]  time 0.02s 
Epoch 0/600 48/293 loss: 1.5896 acc [ 0.380  0.061  0.026]  time 0.02s 
Epoch 0/600 49/293 loss: 1.5857 acc [ 0.380  0.061  0.026]  time 0.03s 
Epoch 0/600 50/293 loss: 1.5809 acc [ 0.386  0.059  0.026]  time 3.00s 
Epoch 0/600 51/293 loss: 1.5793 acc [ 0.386  0.059  0.026]  time 0.02s 
Epoch 0/600 52/293 loss: 1.5778 acc [ 0.386  0.059  0.026]  time 0.02s 
Epoch 0/600 53/293 loss: 1.5758 acc [ 0.387  0.064  0.034]  time 0.02s 
Epoch 0/600 54/293 loss: 1.5746 acc [ 0.387  0.062  0.033]  time 1.59s 
Epoch 0/600 55/293 loss: 1.5733 acc [ 0.387  0.062  0.033]  time 0.02s 
Epoch 0/600 56/293 loss: 1.5713 acc [ 0.386  0.061  0.032]  time 0.02s 
Epoch 0/600 57/293 loss: 1.5682 acc [ 0.384  0.061  0.032]  time 0.02s 
Epoch 0/600 58/293 loss: 1.5660 acc [ 0.384  0.060  0.031]  time 2.12s 
Epoch 0/600 59/293 loss: 1.5634 acc [ 0.388  0.058  0.030]  time 0.02s 
Epoch 0/600 60/293 loss: 1.5656 acc [ 0.389  0.058  0.030]  time 0.02s 
Epoch 0/600 61/293 loss: 1.5636 acc [ 0.394  0.061  0.033]  time 6.73s 
Epoch 0/600 62/293 loss: 1.5603 acc [ 0.403  0.063  0.037]  time 0.39s 
Epoch 0/600 63/293 loss: 1.5579 acc [ 0.406  0.061  0.036]  time 0.02s 
Epoch 0/600 64/293 loss: 1.5553 acc [ 0.405  0.060  0.036]  time 0.02s 
Epoch 0/600 65/293 loss: 1.5797 acc [ 0.411  0.061  0.037]  time 1.37s 
Epoch 0/600 66/293 loss: 1.5785 acc [ 0.411  0.061  0.037]  time 2.72s 
Epoch 0/600 67/293 loss: 1.5888 acc [ 0.416  0.061  0.036]  time 0.02s 
Epoch 0/600 68/293 loss: 1.5873 acc [ 0.416  0.060  0.036]  time 0.02s 
Epoch 0/600 69/293 loss: 1.6018 acc [ 0.422  0.062  0.037]  time 3.61s 
Epoch 0/600 70/293 loss: 1.6004 acc [ 0.422  0.062  0.037]  time 0.02s 
Epoch 0/600 71/293 loss: 1.6018 acc [ 0.422  0.064  0.039]  time 0.02s 
Epoch 0/600 72/293 loss: 1.6009 acc [ 0.429  0.063  0.038]  time 0.02s 
Epoch 0/600 73/293 loss: 1.5982 acc [ 0.428  0.063  0.038]  time 1.11s 
Epoch 0/600 74/293 loss: 1.5957 acc [ 0.427  0.062  0.039]  time 0.02s 
Epoch 0/600 75/293 loss: 1.5932 acc [ 0.429  0.062  0.038]  time 0.02s 
Epoch 0/600 76/293 loss: 1.5909 acc [ 0.426  0.061  0.038]  time 0.02s 
Epoch 0/600 77/293 loss: 1.5883 acc [ 0.426  0.060  0.037]  time 0.78s 
Epoch 0/600 78/293 loss: 1.5872 acc [ 0.426  0.060  0.037]  time 0.09s 
Epoch 0/600 79/293 loss: 1.5857 acc [ 0.432  0.060  0.037]  time 0.02s 
Epoch 0/600 80/293 loss: 1.5837 acc [ 0.435  0.059  0.036]  time 0.39s 
Epoch 0/600 81/293 loss: 1.5827 acc [ 0.435  0.059  0.036]  time 0.36s 
Epoch 0/600 82/293 loss: 1.5799 acc [ 0.437  0.058  0.036]  time 1.36s 
Epoch 0/600 83/293 loss: 1.5790 acc [ 0.437  0.058  0.036]  time 0.02s 
Epoch 0/600 84/293 loss: 1.5769 acc [ 0.435  0.058  0.036]  time 0.02s 
Epoch 0/600 85/293 loss: 1.5746 acc [ 0.435  0.058  0.036]  time 0.02s 
Epoch 0/600 86/293 loss: 1.5723 acc [ 0.437  0.058  0.036]  time 2.70s 
Epoch 0/600 87/293 loss: 1.5694 acc [ 0.438  0.058  0.036]  time 0.02s 
Epoch 0/600 88/293 loss: 1.5677 acc [ 0.435  0.057  0.035]  time 0.02s 
Epoch 0/600 89/293 loss: 1.5718 acc [ 0.438  0.057  0.035]  time 0.02s 
Epoch 0/600 90/293 loss: 1.5684 acc [ 0.443  0.056  0.035]  time 4.33s 
Epoch 0/600 91/293 loss: 1.5698 acc [ 0.444  0.057  0.036]  time 0.02s 
Epoch 0/600 92/293 loss: 1.5721 acc [ 0.449  0.059  0.036]  time 0.52s 
Epoch 0/600 93/293 loss: 1.5695 acc [ 0.450  0.059  0.036]  time 0.02s 
Epoch 0/600 94/293 loss: 1.5687 acc [ 0.450  0.059  0.036]  time 0.38s 
Epoch 0/600 95/293 loss: 1.5680 acc [ 0.450  0.059  0.036]  time 0.02s 
Epoch 0/600 96/293 loss: 1.5673 acc [ 0.450  0.059  0.036]  time 4.99s 
Epoch 0/600 97/293 loss: 1.5666 acc [ 0.450  0.059  0.036]  time 0.02s 
Epoch 0/600 98/293 loss: 1.5667 acc [ 0.450  0.061  0.038]  time 0.02s 
Epoch 0/600 99/293 loss: 1.5671 acc [ 0.449  0.062  0.039]  time 0.02s 
Epoch 0/600 100/293 loss: 1.5657 acc [ 0.448  0.063  0.041]  time 1.27s 
Epoch 0/600 101/293 loss: 1.5650 acc [ 0.448  0.063  0.041]  time 0.02s 
Epoch 0/600 102/293 loss: 1.5644 acc [ 0.448  0.063  0.041]  time 0.02s 
Epoch 0/600 103/293 loss: 1.5622 acc [ 0.448  0.062  0.040]  time 0.02s 
Epoch 0/600 104/293 loss: 1.5617 acc [ 0.449  0.062  0.039]  time 1.17s 
Epoch 0/600 105/293 loss: 1.5671 acc [ 0.454  0.063  0.040]  time 3.25s 
Epoch 0/600 106/293 loss: 1.5647 acc [ 0.456  0.062  0.040]  time 0.02s 
Epoch 0/600 107/293 loss: 1.5623 acc [ 0.461  0.062  0.042]  time 0.02s 
Epoch 0/600 108/293 loss: 1.5617 acc [ 0.461  0.062  0.042]  time 0.02s 
Epoch 0/600 109/293 loss: 1.5612 acc [ 0.456  0.061  0.042]  time 1.54s 
Epoch 0/600 110/293 loss: 1.5594 acc [ 0.456  0.061  0.041]  time 0.02s 
Epoch 0/600 111/293 loss: 1.5571 acc [ 0.457  0.060  0.041]  time 0.02s 
Epoch 0/600 112/293 loss: 1.5636 acc [ 0.460  0.061  0.042]  time 0.02s 
Epoch 0/600 113/293 loss: 1.5638 acc [ 0.465  0.061  0.042]  time 6.89s 
Epoch 0/600 114/293 loss: 1.5632 acc [ 0.465  0.061  0.042]  time 0.02s 
Epoch 0/600 115/293 loss: 1.5622 acc [ 0.463  0.060  0.042]  time 0.02s 
Epoch 0/600 116/293 loss: 1.5616 acc [ 0.458  0.060  0.042]  time 0.02s 
Epoch 0/600 117/293 loss: 1.5608 acc [ 0.456  0.060  0.042]  time 1.49s 
Epoch 0/600 118/293 loss: 1.5603 acc [ 0.456  0.060  0.042]  time 0.03s 
Epoch 0/600 119/293 loss: 1.5594 acc [ 0.453  0.060  0.042]  time 0.02s 
Epoch 0/600 120/293 loss: 1.5582 acc [ 0.454  0.059  0.041]  time 0.02s 
Epoch 0/600 121/293 loss: 1.5577 acc [ 0.456  0.061  0.043]  time 2.45s 
Epoch 0/600 122/293 loss: 1.5562 acc [ 0.459  0.061  0.043]  time 0.02s 
Epoch 0/600 123/293 loss: 1.5557 acc [ 0.459  0.061  0.043]  time 1.00s 
Epoch 0/600 124/293 loss: 1.5553 acc [ 0.454  0.061  0.043]  time 0.03s 
Epoch 0/600 125/293 loss: 1.5537 acc [ 0.455  0.061  0.043]  time 0.03s 
Epoch 0/600 126/293 loss: 1.5515 acc [ 0.459  0.061  0.044]  time 3.50s 
Epoch 0/600 127/293 loss: 1.5511 acc [ 0.459  0.061  0.044]  time 2.39s 
Epoch 0/600 128/293 loss: 1.5503 acc [ 0.459  0.060  0.044]  time 0.03s 
Epoch 0/600 129/293 loss: 1.5499 acc [ 0.459  0.060  0.044]  time 0.03s 
Epoch 0/600 130/293 loss: 1.5483 acc [ 0.464  0.061  0.045]  time 1.69s 
Epoch 0/600 131/293 loss: 1.5479 acc [ 0.464  0.061  0.045]  time 0.54s 
Epoch 0/600 132/293 loss: 1.5476 acc [ 0.464  0.061  0.045]  time 0.03s 
Epoch 0/600 133/293 loss: 1.5472 acc [ 0.464  0.061  0.045]  time 0.03s 
Epoch 0/600 134/293 loss: 1.5456 acc [ 0.464  0.060  0.046]  time 0.87s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 1
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: false
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: false
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 1
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1705GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 1 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/293 loss: 1.6340 acc [ 0.118  0.000  0.000]  time 2.44s 
Epoch 0/600 1/293 loss: 1.5670 acc [ 0.118  0.000  0.000]  time 1.70s 
Epoch 0/600 2/293 loss: 1.7630 acc [ 0.098  0.114  0.050]  time 6.31s 
Epoch 0/600 3/293 loss: 1.7386 acc [ 0.104  0.138  0.031]  time 1.53s 
Epoch 0/600 4/293 loss: 1.7048 acc [ 0.097  0.138  0.031]  time 1.19s 
Epoch 0/600 5/293 loss: 1.6706 acc [ 0.097  0.138  0.031]  time 2.39s 
Epoch 0/600 6/293 loss: 1.6540 acc [ 0.099  0.095  0.024]  time 1.03s 
Epoch 0/600 7/293 loss: 1.6347 acc [ 0.083  0.071  0.018]  time 1.89s 
Epoch 0/600 8/293 loss: 1.6220 acc [ 0.089  0.057  0.018]  time 1.75s 
Epoch 0/600 9/293 loss: 1.6221 acc [ 0.108  0.052  0.024]  time 8.05s 
Epoch 0/600 10/293 loss: 1.9632 acc [ 0.138  0.123  0.055]  time 6.01s 
Epoch 0/600 11/293 loss: 1.9246 acc [ 0.138  0.123  0.055]  time 2.58s 
Epoch 0/600 12/293 loss: 1.8923 acc [ 0.134  0.111  0.048]  time 1.43s 
Epoch 0/600 13/293 loss: 1.8643 acc [ 0.134  0.111  0.048]  time 2.15s 
Epoch 0/600 14/293 loss: 1.8408 acc [ 0.131  0.100  0.047]  time 0.71s 
Epoch 0/600 15/293 loss: 1.8254 acc [ 0.129  0.095  0.045]  time 1.61s 
Epoch 0/600 16/293 loss: 1.8063 acc [ 0.129  0.095  0.045]  time 3.42s 
Epoch 0/600 17/293 loss: 1.8107 acc [ 0.161  0.095  0.045]  time 11.35s 
Epoch 0/600 18/293 loss: 1.7919 acc [ 0.177  0.095  0.045]  time 2.00s 
Epoch 0/600 19/293 loss: 1.7759 acc [ 0.185  0.095  0.045]  time 2.68s 
Epoch 0/600 20/293 loss: 1.7607 acc [ 0.183  0.086  0.042]  time 1.13s 
Epoch 0/600 21/293 loss: 1.7552 acc [ 0.190  0.082  0.045]  time 0.81s 
Epoch 0/600 22/293 loss: 1.7441 acc [ 0.190  0.082  0.045]  time 1.57s 
Epoch 0/600 23/293 loss: 1.7809 acc [ 0.202  0.094  0.050]  time 2.57s 
Epoch 0/600 24/293 loss: 1.7696 acc [ 0.202  0.094  0.050]  time 1.55s 
Epoch 0/600 25/293 loss: 1.7573 acc [ 0.204  0.088  0.050]  time 0.74s 
Epoch 0/600 26/293 loss: 1.7454 acc [ 0.204  0.088  0.050]  time 0.67s 
Epoch 0/600 27/293 loss: 1.7378 acc [ 0.205  0.083  0.047]  time 1.97s 
Epoch 0/600 28/293 loss: 1.7321 acc [ 0.227  0.083  0.047]  time 13.73s 
Epoch 0/600 29/293 loss: 1.7251 acc [ 0.227  0.079  0.044]  time 2.87s 
Epoch 0/600 30/293 loss: 1.7357 acc [ 0.243  0.078  0.048]  time 1.73s 
Epoch 0/600 31/293 loss: 1.7284 acc [ 0.243  0.078  0.048]  time 1.54s 
Epoch 0/600 32/293 loss: 1.7500 acc [ 0.255  0.077  0.060]  time 16.08s 
Epoch 0/600 33/293 loss: 1.7410 acc [ 0.254  0.073  0.058]  time 1.77s 
Epoch 0/600 34/293 loss: 1.7327 acc [ 0.251  0.073  0.058]  time 1.09s 
Epoch 0/600 35/293 loss: 1.7248 acc [ 0.255  0.069  0.056]  time 1.47s 
Epoch 0/600 36/293 loss: 1.7187 acc [ 0.255  0.069  0.056]  time 2.50s 
Epoch 0/600 37/293 loss: 1.7138 acc [ 0.258  0.069  0.054]  time 1.64s 
Epoch 0/600 38/293 loss: 1.7086 acc [ 0.259  0.066  0.051]  time 1.48s 
Epoch 0/600 39/293 loss: 1.7332 acc [ 0.267  0.072  0.050]  time 2.33s 
Epoch 0/600 40/293 loss: 1.7263 acc [ 0.270  0.069  0.050]  time 0.90s 
Epoch 0/600 41/293 loss: 1.7264 acc [ 0.272  0.071  0.050]  time 2.00s 
Epoch 0/600 42/293 loss: 1.7215 acc [ 0.277  0.069  0.048]  time 2.98s 
Epoch 0/600 43/293 loss: 1.7145 acc [ 0.278  0.068  0.047]  time 0.91s 
Epoch 0/600 44/293 loss: 1.7067 acc [ 0.279  0.068  0.047]  time 1.15s 
Epoch 0/600 45/293 loss: 1.7028 acc [ 0.292  0.068  0.047]  time 9.49s 
Epoch 0/600 46/293 loss: 1.7273 acc [ 0.294  0.077  0.047]  time 4.57s 
Epoch 0/600 47/293 loss: 1.7217 acc [ 0.293  0.075  0.046]  time 1.73s 
Epoch 0/600 48/293 loss: 1.7171 acc [ 0.303  0.075  0.046]  time 4.19s 
Epoch 0/600 49/293 loss: 1.7128 acc [ 0.303  0.075  0.046]  time 0.95s 
Epoch 0/600 50/293 loss: 1.7272 acc [ 0.308  0.080  0.047]  time 1.42s 
Epoch 0/600 51/293 loss: 1.7229 acc [ 0.308  0.080  0.047]  time 1.06s 
Epoch 0/600 52/293 loss: 1.7156 acc [ 0.313  0.080  0.047]  time 3.21s 
Epoch 0/600 53/293 loss: 1.7159 acc [ 0.324  0.079  0.049]  time 6.66s 
Epoch 0/600 54/293 loss: 1.7125 acc [ 0.333  0.079  0.049]  time 4.85s 
Epoch 0/600 55/293 loss: 1.7087 acc [ 0.333  0.079  0.049]  time 1.36s 
Epoch 0/600 56/293 loss: 1.7051 acc [ 0.333  0.079  0.049]  time 3.96s 
Epoch 0/600 57/293 loss: 1.7083 acc [ 0.336  0.079  0.052]  time 3.26s 
Epoch 0/600 58/293 loss: 1.7040 acc [ 0.342  0.079  0.052]  time 1.23s 
Epoch 0/600 59/293 loss: 1.6979 acc [ 0.344  0.079  0.052]  time 1.22s 
Epoch 0/600 60/293 loss: 1.6947 acc [ 0.344  0.079  0.052]  time 6.96s 
Epoch 0/600 61/293 loss: 1.6898 acc [ 0.344  0.077  0.051]  time 1.36s 
Epoch 0/600 62/293 loss: 1.6841 acc [ 0.347  0.077  0.051]  time 2.80s 
Epoch 0/600 63/293 loss: 1.6799 acc [ 0.349  0.077  0.051]  time 1.04s 
Epoch 0/600 64/293 loss: 1.6772 acc [ 0.349  0.077  0.051]  time 1.89s 
Epoch 0/600 65/293 loss: 1.6732 acc [ 0.351  0.075  0.049]  time 1.82s 
Epoch 0/600 66/293 loss: 1.6683 acc [ 0.358  0.075  0.049]  time 5.18s 
Epoch 0/600 67/293 loss: 1.6658 acc [ 0.358  0.075  0.049]  time 1.20s 
Epoch 0/600 68/293 loss: 1.6607 acc [ 0.359  0.073  0.049]  time 1.81s 
Epoch 0/600 69/293 loss: 1.6558 acc [ 0.361  0.071  0.049]  time 0.89s 
Epoch 0/600 70/293 loss: 1.6622 acc [ 0.360  0.070  0.053]  time 0.86s 
Epoch 0/600 71/293 loss: 1.6600 acc [ 0.360  0.070  0.053]  time 1.43s 
Epoch 0/600 72/293 loss: 1.6569 acc [ 0.367  0.070  0.053]  time 4.52s 
Epoch 0/600 73/293 loss: 1.6534 acc [ 0.366  0.068  0.051]  time 1.57s 
Epoch 0/600 74/293 loss: 1.6498 acc [ 0.367  0.068  0.051]  time 1.02s 
Epoch 0/600 75/293 loss: 1.6462 acc [ 0.366  0.066  0.051]  time 1.58s 
Epoch 0/600 76/293 loss: 1.6668 acc [ 0.372  0.069  0.050]  time 1.56s 
Epoch 0/600 77/293 loss: 1.6647 acc [ 0.372  0.069  0.050]  time 2.71s 
Epoch 0/600 78/293 loss: 1.6626 acc [ 0.372  0.069  0.050]  time 9.76s 
Epoch 0/600 79/293 loss: 1.6636 acc [ 0.371  0.070  0.048]  time 1.53s 
Epoch 0/600 80/293 loss: 1.6607 acc [ 0.370  0.068  0.047]  time 1.24s 
Epoch 0/600 81/293 loss: 1.6559 acc [ 0.373  0.067  0.047]  time 2.51s 
Epoch 0/600 82/293 loss: 1.6516 acc [ 0.373  0.067  0.047]  time 0.90s 
Epoch 0/600 83/293 loss: 1.6584 acc [ 0.379  0.068  0.050]  time 2.84s 
Epoch 0/600 84/293 loss: 1.6603 acc [ 0.380  0.068  0.053]  time 0.94s 
Epoch 0/600 85/293 loss: 1.6584 acc [ 0.382  0.067  0.054]  time 2.17s 
Epoch 0/600 86/293 loss: 1.6566 acc [ 0.382  0.067  0.054]  time 3.83s 
Epoch 0/600 87/293 loss: 1.6570 acc [ 0.389  0.066  0.053]  time 9.74s 
Epoch 0/600 88/293 loss: 1.6540 acc [ 0.391  0.066  0.052]  time 2.42s 
Epoch 0/600 89/293 loss: 1.6505 acc [ 0.389  0.064  0.052]  time 2.30s 
Epoch 0/600 90/293 loss: 1.6485 acc [ 0.393  0.063  0.051]  time 2.69s 
Epoch 0/600 91/293 loss: 1.6457 acc [ 0.391  0.062  0.050]  time 1.38s 
Epoch 0/600 92/293 loss: 1.6424 acc [ 0.391  0.062  0.050]  time 1.55s 
Epoch 0/600 93/293 loss: 1.6407 acc [ 0.387  0.062  0.050]  time 1.03s 
Epoch 0/600 94/293 loss: 1.6388 acc [ 0.385  0.061  0.049]  time 1.54s 
Epoch 0/600 95/293 loss: 1.6395 acc [ 0.385  0.063  0.048]  time 1.63s 
Epoch 0/600 96/293 loss: 1.6368 acc [ 0.385  0.062  0.048]  time 1.60s 
Epoch 0/600 97/293 loss: 1.6354 acc [ 0.385  0.062  0.048]  time 1.93s 
Epoch 0/600 98/293 loss: 1.6561 acc [ 0.390  0.064  0.049]  time 5.45s 
Epoch 0/600 99/293 loss: 1.6546 acc [ 0.390  0.064  0.049]  time 1.80s 
Epoch 0/600 100/293 loss: 1.6558 acc [ 0.390  0.064  0.050]  time 0.88s 
Epoch 0/600 101/293 loss: 1.6536 acc [ 0.389  0.063  0.050]  time 1.50s 
Epoch 0/600 102/293 loss: 1.6521 acc [ 0.389  0.063  0.050]  time 1.37s 
Epoch 0/600 103/293 loss: 1.6570 acc [ 0.392  0.064  0.052]  time 2.09s 
Epoch 0/600 104/293 loss: 1.6545 acc [ 0.390  0.064  0.052]  time 0.79s 
Epoch 0/600 105/293 loss: 1.6527 acc [ 0.390  0.063  0.052]  time 1.54s 
Epoch 0/600 106/293 loss: 1.6513 acc [ 0.390  0.063  0.052]  time 1.65s 
Epoch 0/600 107/293 loss: 1.6499 acc [ 0.390  0.063  0.052]  time 6.49s 
Epoch 0/600 108/293 loss: 1.6502 acc [ 0.393  0.063  0.052]  time 3.20s 
Epoch 0/600 109/293 loss: 1.6486 acc [ 0.395  0.063  0.052]  time 0.73s 
Epoch 0/600 110/293 loss: 1.6473 acc [ 0.395  0.063  0.052]  time 1.55s 
Epoch 0/600 111/293 loss: 1.6458 acc [ 0.390  0.062  0.052]  time 1.84s 
Epoch 0/600 112/293 loss: 1.6446 acc [ 0.390  0.062  0.052]  time 1.63s 
Epoch 0/600 113/293 loss: 1.6433 acc [ 0.390  0.062  0.052]  time 4.43s 
Epoch 0/600 114/293 loss: 1.6427 acc [ 0.388  0.062  0.052]  time 1.55s 
Epoch 0/600 115/293 loss: 1.6411 acc [ 0.388  0.062  0.052]  time 1.56s 
Epoch 0/600 116/293 loss: 1.6390 acc [ 0.393  0.062  0.051]  time 7.79s 
Epoch 0/600 117/293 loss: 1.6375 acc [ 0.392  0.062  0.051]  time 1.32s 
Epoch 0/600 118/293 loss: 1.6357 acc [ 0.390  0.061  0.051]  time 1.68s 
Epoch 0/600 119/293 loss: 1.6370 acc [ 0.396  0.062  0.052]  time 9.94s 
Epoch 0/600 120/293 loss: 1.6349 acc [ 0.395  0.062  0.052]  time 1.35s 
Epoch 0/600 121/293 loss: 1.6326 acc [ 0.397  0.062  0.052]  time 3.97s 
Epoch 0/600 122/293 loss: 1.6322 acc [ 0.396  0.062  0.055]  time 3.65s 
Epoch 0/600 123/293 loss: 1.6338 acc [ 0.398  0.064  0.055]  time 1.46s 
Epoch 0/600 124/293 loss: 1.6330 acc [ 0.399  0.064  0.055]  time 1.05s 
Epoch 0/600 125/293 loss: 1.6320 acc [ 0.399  0.064  0.055]  time 6.86s 
Epoch 0/600 126/293 loss: 1.6309 acc [ 0.399  0.064  0.055]  time 6.83s 
Epoch 0/600 127/293 loss: 1.6291 acc [ 0.400  0.063  0.055]  time 3.88s 
Epoch 0/600 128/293 loss: 1.6268 acc [ 0.400  0.063  0.055]  time 0.84s 
Epoch 0/600 129/293 loss: 1.6259 acc [ 0.400  0.063  0.055]  time 1.48s 
Epoch 0/600 130/293 loss: 1.6240 acc [ 0.401  0.063  0.055]  time 1.39s 
Epoch 0/600 131/293 loss: 1.6223 acc [ 0.401  0.063  0.055]  time 1.50s 
Epoch 0/600 132/293 loss: 1.6206 acc [ 0.403  0.063  0.054]  time 3.35s 
Epoch 0/600 133/293 loss: 1.6184 acc [ 0.403  0.062  0.054]  time 0.89s 
Epoch 0/600 134/293 loss: 1.6235 acc [ 0.404  0.064  0.054]  time 1.47s 
Epoch 0/600 135/293 loss: 1.6225 acc [ 0.404  0.064  0.054]  time 1.40s 
Epoch 0/600 136/293 loss: 1.6205 acc [ 0.404  0.064  0.054]  time 1.08s 
Epoch 0/600 137/293 loss: 1.6196 acc [ 0.404  0.064  0.054]  time 9.96s 
Epoch 0/600 138/293 loss: 1.6227 acc [ 0.408  0.065  0.055]  time 7.92s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 1
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 1
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1734GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 1 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/293 loss: 1.7207 acc [ 0.478  0.121  0.091]  time 2.34s 
Epoch 0/600 1/293 loss: 1.6870 acc [ 0.450  0.132  0.095]  time 1.99s 
Epoch 0/600 2/293 loss: 1.6288 acc [ 0.398  0.088  0.095]  time 1.93s 
Epoch 0/600 3/293 loss: 1.6040 acc [ 0.394  0.069  0.067]  time 1.00s 
Epoch 0/600 4/293 loss: 1.5806 acc [ 0.354  0.062  0.067]  time 1.74s 
Epoch 0/600 5/293 loss: 1.7234 acc [ 0.394  0.137  0.144]  time 8.63s 
Epoch 0/600 6/293 loss: 1.6958 acc [ 0.395  0.120  0.119]  time 2.61s 
Epoch 0/600 7/293 loss: 1.6713 acc [ 0.395  0.120  0.119]  time 1.44s 
Epoch 0/600 8/293 loss: 1.6523 acc [ 0.345  0.105  0.099]  time 0.99s 
Epoch 0/600 9/293 loss: 1.6370 acc [ 0.345  0.105  0.099]  time 1.02s 
Epoch 0/600 10/293 loss: 1.6246 acc [ 0.345  0.105  0.099]  time 1.45s 
Epoch 0/600 11/293 loss: 1.6114 acc [ 0.327  0.105  0.099]  time 2.22s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 1
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 4
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1730GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 1 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/293 loss: 2.5106 acc [ 0.508  0.537  0.535]  time 12.79s 
Epoch 0/600 1/293 loss: 2.0457 acc [ 0.401  0.274  0.273]  time 0.03s 
Epoch 0/600 2/293 loss: 1.9447 acc [ 0.366  0.276  0.284]  time 0.02s 
Epoch 0/600 3/293 loss: 1.8511 acc [ 0.343  0.226  0.240]  time 0.02s 
Epoch 0/600 4/293 loss: 1.7915 acc [ 0.326  0.181  0.192]  time 0.63s 
Epoch 0/600 5/293 loss: 1.7429 acc [ 0.326  0.181  0.192]  time 0.02s 
Epoch 0/600 6/293 loss: 1.7735 acc [ 0.386  0.168  0.180]  time 0.02s 
Epoch 0/600 7/293 loss: 1.7928 acc [ 0.428  0.145  0.180]  time 0.03s 
Epoch 0/600 8/293 loss: 1.7752 acc [ 0.424  0.146  0.181]  time 2.65s 
Epoch 0/600 9/293 loss: 1.7549 acc [ 0.418  0.146  0.181]  time 0.02s 
Epoch 0/600 10/293 loss: 1.7317 acc [ 0.418  0.146  0.181]  time 0.02s 
Epoch 0/600 11/293 loss: 1.7361 acc [ 0.429  0.154  0.189]  time 0.02s 
Epoch 0/600 12/293 loss: 1.7179 acc [ 0.429  0.154  0.189]  time 1.25s 
Epoch 0/600 13/293 loss: 1.7112 acc [ 0.440  0.140  0.170]  time 2.84s 
Epoch 0/600 14/293 loss: 1.6971 acc [ 0.440  0.140  0.170]  time 0.02s 
Epoch 0/600 15/293 loss: 1.6855 acc [ 0.433  0.131  0.158]  time 0.02s 
Epoch 0/600 16/293 loss: 1.6736 acc [ 0.416  0.124  0.158]  time 0.02s 
Epoch 0/600 17/293 loss: 1.6641 acc [ 0.413  0.124  0.158]  time 2.36s 
Epoch 0/600 18/293 loss: 1.6580 acc [ 0.415  0.118  0.150]  time 0.68s 
Epoch 0/600 19/293 loss: 1.6487 acc [ 0.418  0.118  0.150]  time 0.02s 
Epoch 0/600 20/293 loss: 1.6416 acc [ 0.418  0.118  0.150]  time 0.02s 
Epoch 0/600 21/293 loss: 1.6847 acc [ 0.430  0.144  0.179]  time 4.64s 
Epoch 0/600 22/293 loss: 1.6740 acc [ 0.424  0.135  0.167]  time 0.02s 
Epoch 0/600 23/293 loss: 1.6658 acc [ 0.415  0.128  0.157]  time 0.02s 
Epoch 0/600 24/293 loss: 1.6592 acc [ 0.415  0.128  0.157]  time 0.02s 
Epoch 0/600 25/293 loss: 1.6531 acc [ 0.415  0.128  0.157]  time 0.79s 
Epoch 0/600 26/293 loss: 1.8048 acc [ 0.442  0.167  0.184]  time 3.57s 
Epoch 0/600 27/293 loss: 1.7939 acc [ 0.442  0.167  0.184]  time 2.55s 
Epoch 0/600 28/293 loss: 1.7811 acc [ 0.431  0.167  0.184]  time 0.02s 
Epoch 0/600 29/293 loss: 1.7695 acc [ 0.422  0.159  0.184]  time 0.02s 
Epoch 0/600 30/293 loss: 1.7718 acc [ 0.420  0.164  0.187]  time 0.02s 
Epoch 0/600 31/293 loss: 1.7633 acc [ 0.420  0.164  0.187]  time 13.95s 
Epoch 0/600 32/293 loss: 1.7716 acc [ 0.425  0.168  0.199]  time 0.02s 
Epoch 0/600 33/293 loss: 1.7628 acc [ 0.419  0.161  0.190]  time 0.02s 
Epoch 0/600 34/293 loss: 1.7594 acc [ 0.411  0.162  0.192]  time 0.03s 
Epoch 0/600 35/293 loss: 1.7513 acc [ 0.406  0.156  0.187]  time 0.86s 
Epoch 0/600 36/293 loss: 1.7577 acc [ 0.411  0.163  0.194]  time 0.02s 
Epoch 0/600 37/293 loss: 1.7573 acc [ 0.409  0.165  0.201]  time 0.02s 
Epoch 0/600 38/293 loss: 1.7507 acc [ 0.409  0.165  0.201]  time 0.02s 
Epoch 0/600 39/293 loss: 1.7478 acc [ 0.410  0.168  0.204]  time 1.02s 
Epoch 0/600 40/293 loss: 1.7432 acc [ 0.417  0.162  0.204]  time 5.38s 
Epoch 0/600 41/293 loss: 1.7375 acc [ 0.418  0.159  0.200]  time 0.03s 
Epoch 0/600 42/293 loss: 1.7302 acc [ 0.415  0.154  0.192]  time 0.02s 
Epoch 0/600 43/293 loss: 1.7250 acc [ 0.415  0.154  0.192]  time 0.02s 
Epoch 0/600 44/293 loss: 1.7200 acc [ 0.415  0.154  0.192]  time 1.37s 
Epoch 0/600 45/293 loss: 1.7139 acc [ 0.416  0.151  0.188]  time 0.03s 
Epoch 0/600 46/293 loss: 1.7111 acc [ 0.423  0.149  0.182]  time 2.81s 
Epoch 0/600 47/293 loss: 1.7037 acc [ 0.423  0.149  0.182]  time 0.03s 
Epoch 0/600 48/293 loss: 1.6995 acc [ 0.412  0.145  0.176]  time 0.03s 
Epoch 0/600 49/293 loss: 1.7005 acc [ 0.410  0.146  0.183]  time 0.03s 
Epoch 0/600 50/293 loss: 1.6954 acc [ 0.403  0.146  0.183]  time 11.32s 
Epoch 0/600 51/293 loss: 1.6915 acc [ 0.404  0.144  0.183]  time 0.02s 
Epoch 0/600 52/293 loss: 1.6864 acc [ 0.401  0.144  0.183]  time 0.03s 
Epoch 0/600 53/293 loss: 1.6816 acc [ 0.404  0.141  0.178]  time 0.03s 
Epoch 0/600 54/293 loss: 1.6767 acc [ 0.399  0.141  0.178]  time 0.83s 
Epoch 0/600 55/293 loss: 1.6736 acc [ 0.399  0.141  0.178]  time 0.02s 
Epoch 0/600 56/293 loss: 1.6705 acc [ 0.399  0.141  0.178]  time 0.02s 
Epoch 0/600 57/293 loss: 1.6676 acc [ 0.390  0.141  0.178]  time 0.02s 
Epoch 0/600 58/293 loss: 1.6624 acc [ 0.389  0.141  0.178]  time 1.09s 
Epoch 0/600 59/293 loss: 1.6584 acc [ 0.386  0.137  0.178]  time 0.02s 
Epoch 0/600 60/293 loss: 1.6551 acc [ 0.382  0.133  0.173]  time 0.02s 
Epoch 0/600 61/293 loss: 1.6526 acc [ 0.382  0.133  0.173]  time 0.02s 
Epoch 0/600 62/293 loss: 1.6502 acc [ 0.382  0.133  0.173]  time 1.02s 
Epoch 0/600 63/293 loss: 1.6478 acc [ 0.382  0.133  0.173]  time 0.02s 
Epoch 0/600 64/293 loss: 1.6455 acc [ 0.382  0.133  0.173]  time 0.68s 
Epoch 0/600 65/293 loss: 1.6433 acc [ 0.374  0.130  0.167]  time 0.57s 
Epoch 0/600 66/293 loss: 1.6500 acc [ 0.379  0.136  0.176]  time 1.58s 
Epoch 0/600 67/293 loss: 1.6476 acc [ 0.385  0.134  0.172]  time 6.95s 
Epoch 0/600 68/293 loss: 1.6434 acc [ 0.383  0.134  0.172]  time 0.02s 
Epoch 0/600 69/293 loss: 1.6400 acc [ 0.379  0.131  0.168]  time 0.02s 
Epoch 0/600 70/293 loss: 1.6364 acc [ 0.378  0.128  0.163]  time 0.02s 
Epoch 0/600 71/293 loss: 1.6334 acc [ 0.383  0.126  0.163]  time 1.70s 
Epoch 0/600 72/293 loss: 1.6316 acc [ 0.383  0.126  0.163]  time 0.02s 
Epoch 0/600 73/293 loss: 1.6298 acc [ 0.383  0.126  0.163]  time 0.02s 
Epoch 0/600 74/293 loss: 1.6279 acc [ 0.379  0.126  0.163]  time 0.02s 
Epoch 0/600 75/293 loss: 1.6262 acc [ 0.379  0.126  0.163]  time 2.33s 
Epoch 0/600 76/293 loss: 1.6240 acc [ 0.378  0.125  0.162]  time 0.02s 
Epoch 0/600 77/293 loss: 1.6225 acc [ 0.378  0.125  0.162]  time 0.02s 
Epoch 0/600 78/293 loss: 1.6209 acc [ 0.378  0.125  0.162]  time 0.02s 
Epoch 0/600 79/293 loss: 1.6194 acc [ 0.378  0.125  0.162]  time 1.81s 
Epoch 0/600 80/293 loss: 1.6237 acc [ 0.380  0.129  0.164]  time 0.02s 
Epoch 0/600 81/293 loss: 1.6208 acc [ 0.379  0.127  0.161]  time 0.02s 
Epoch 0/600 82/293 loss: 1.6177 acc [ 0.378  0.124  0.157]  time 0.02s 
Epoch 0/600 83/293 loss: 1.6153 acc [ 0.385  0.124  0.157]  time 10.96s 
Epoch 0/600 84/293 loss: 1.6119 acc [ 0.385  0.122  0.154]  time 0.02s 
Epoch 0/600 85/293 loss: 1.6086 acc [ 0.388  0.122  0.154]  time 0.02s 
Epoch 0/600 86/293 loss: 1.6073 acc [ 0.388  0.122  0.154]  time 0.02s 
Epoch 0/600 87/293 loss: 1.6293 acc [ 0.394  0.127  0.160]  time 16.69s 
Epoch 0/600 88/293 loss: 1.6279 acc [ 0.388  0.127  0.160]  time 0.02s 
Epoch 0/600 89/293 loss: 1.6239 acc [ 0.394  0.127  0.160]  time 0.02s 
Epoch 0/600 90/293 loss: 1.6225 acc [ 0.394  0.127  0.160]  time 0.02s 
Epoch 0/600 91/293 loss: 1.6199 acc [ 0.396  0.125  0.157]  time 1.95s 
Epoch 0/600 92/293 loss: 1.6169 acc [ 0.399  0.124  0.156]  time 0.02s 
Epoch 0/600 93/293 loss: 1.6157 acc [ 0.399  0.124  0.156]  time 0.02s 
Epoch 0/600 94/293 loss: 1.6145 acc [ 0.399  0.124  0.156]  time 0.03s 
Epoch 0/600 95/293 loss: 1.6122 acc [ 0.406  0.123  0.154]  time 7.77s 
Epoch 0/600 96/293 loss: 1.6100 acc [ 0.405  0.122  0.154]  time 0.02s 
Epoch 0/600 97/293 loss: 1.6083 acc [ 0.403  0.121  0.151]  time 0.02s 
Epoch 0/600 98/293 loss: 1.6072 acc [ 0.403  0.121  0.151]  time 0.03s 
Epoch 0/600 99/293 loss: 1.6061 acc [ 0.403  0.121  0.151]  time 0.96s 
Epoch 0/600 100/293 loss: 1.6051 acc [ 0.403  0.121  0.151]  time 0.02s 
Epoch 0/600 101/293 loss: 1.6026 acc [ 0.402  0.121  0.151]  time 0.02s 
Epoch 0/600 102/293 loss: 1.6002 acc [ 0.405  0.121  0.151]  time 0.02s 
Epoch 0/600 103/293 loss: 1.5990 acc [ 0.406  0.119  0.150]  time 1.21s 
Epoch 0/600 104/293 loss: 1.5965 acc [ 0.406  0.119  0.150]  time 0.02s 
Epoch 0/600 105/293 loss: 1.5951 acc [ 0.403  0.119  0.148]  time 0.02s 
Epoch 0/600 106/293 loss: 1.5943 acc [ 0.403  0.119  0.148]  time 2.88s 
Epoch 0/600 107/293 loss: 1.5943 acc [ 0.407  0.120  0.150]  time 1.26s 
Epoch 0/600 108/293 loss: 1.6043 acc [ 0.410  0.126  0.153]  time 0.02s 
Epoch 0/600 109/293 loss: 1.6015 acc [ 0.410  0.124  0.153]  time 0.03s 
Epoch 0/600 110/293 loss: 1.6006 acc [ 0.410  0.124  0.153]  time 0.02s 
Epoch 0/600 111/293 loss: 1.5997 acc [ 0.410  0.124  0.153]  time 1.46s 
Epoch 0/600 112/293 loss: 1.5988 acc [ 0.410  0.124  0.153]  time 0.02s 
Epoch 0/600 113/293 loss: 1.5979 acc [ 0.410  0.124  0.153]  time 0.02s 
Epoch 0/600 114/293 loss: 1.5962 acc [ 0.409  0.123  0.151]  time 0.19s 
Epoch 0/600 115/293 loss: 1.5943 acc [ 0.407  0.121  0.151]  time 0.63s 
Epoch 0/600 116/293 loss: 1.5934 acc [ 0.406  0.120  0.151]  time 0.02s 
Epoch 0/600 117/293 loss: 1.5926 acc [ 0.406  0.120  0.151]  time 6.43s 
Epoch 0/600 118/293 loss: 1.5914 acc [ 0.406  0.120  0.153]  time 0.02s 
Epoch 0/600 119/293 loss: 1.5907 acc [ 0.407  0.121  0.153]  time 0.02s 
Epoch 0/600 120/293 loss: 1.5901 acc [ 0.407  0.122  0.153]  time 0.02s 
Epoch 0/600 121/293 loss: 1.5894 acc [ 0.407  0.122  0.153]  time 0.64s 
Epoch 0/600 122/293 loss: 1.5887 acc [ 0.407  0.122  0.153]  time 0.02s 
Epoch 0/600 123/293 loss: 1.5877 acc [ 0.407  0.121  0.152]  time 0.02s 
Epoch 0/600 124/293 loss: 1.5863 acc [ 0.409  0.121  0.153]  time 0.02s 
Epoch 0/600 125/293 loss: 1.5856 acc [ 0.409  0.121  0.153]  time 0.82s 
Epoch 0/600 126/293 loss: 1.5850 acc [ 0.409  0.121  0.153]  time 1.29s 
Epoch 0/600 127/293 loss: 1.5840 acc [ 0.408  0.121  0.153]  time 0.02s 
Epoch 0/600 128/293 loss: 1.5834 acc [ 0.408  0.121  0.153]  time 0.02s 
Epoch 0/600 129/293 loss: 1.5846 acc [ 0.409  0.125  0.156]  time 0.47s 
Epoch 0/600 130/293 loss: 1.5852 acc [ 0.410  0.127  0.161]  time 0.32s 
Epoch 0/600 131/293 loss: 1.5845 acc [ 0.410  0.127  0.161]  time 0.57s 
Epoch 0/600 132/293 loss: 1.5839 acc [ 0.410  0.127  0.161]  time 7.33s 
Epoch 0/600 133/293 loss: 1.5819 acc [ 0.414  0.127  0.161]  time 0.02s 
Epoch 0/600 134/293 loss: 1.5844 acc [ 0.416  0.131  0.164]  time 0.02s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 1
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: 2
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 4
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Using user specified cache_rate=2 to cache data in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 1 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/293 loss: 1.9790 acc [ 0.432  0.268  0.072]  time 6.52s 
Epoch 0/600 1/293 loss: 1.7395 acc [ 0.432  0.268  0.072]  time 0.03s 
Epoch 0/600 2/293 loss: 1.7055 acc [ 0.299  0.192  0.083]  time 0.02s 
Epoch 0/600 3/293 loss: 1.6549 acc [ 0.239  0.133  0.058]  time 0.02s 
Epoch 0/600 4/293 loss: 1.6239 acc [ 0.239  0.133  0.058]  time 2.28s 
Epoch 0/600 5/293 loss: 1.7021 acc [ 0.314  0.114  0.059]  time 5.20s 
Epoch 0/600 6/293 loss: 1.6733 acc [ 0.252  0.114  0.059]  time 0.03s 
Epoch 0/600 7/293 loss: 1.7413 acc [ 0.285  0.157  0.069]  time 0.02s 
Epoch 0/600 8/293 loss: 1.7202 acc [ 0.301  0.134  0.069]  time 0.03s 
Epoch 0/600 9/293 loss: 1.7025 acc [ 0.290  0.134  0.069]  time 1.67s 
Epoch 0/600 10/293 loss: 1.6991 acc [ 0.322  0.117  0.059]  time 0.03s 
Epoch 0/600 11/293 loss: 1.6825 acc [ 0.322  0.117  0.059]  time 0.02s 
Epoch 0/600 12/293 loss: 1.7934 acc [ 0.350  0.156  0.072]  time 2.08s 
Epoch 0/600 13/293 loss: 1.7734 acc [ 0.359  0.156  0.072]  time 0.65s 
Epoch 0/600 14/293 loss: 1.7523 acc [ 0.339  0.156  0.072]  time 0.03s 
Epoch 0/600 15/293 loss: 1.7332 acc [ 0.347  0.139  0.063]  time 0.03s 
Epoch 0/600 16/293 loss: 1.7194 acc [ 0.347  0.139  0.063]  time 1.06s 
Epoch 0/600 17/293 loss: 1.7739 acc [ 0.367  0.148  0.064]  time 0.03s 
Epoch 0/600 18/293 loss: 1.7595 acc [ 0.363  0.137  0.064]  time 0.02s 
Epoch 0/600 19/293 loss: 1.7465 acc [ 0.363  0.137  0.064]  time 0.02s 
Epoch 0/600 20/293 loss: 1.8172 acc [ 0.385  0.160  0.079]  time 4.42s 
Epoch 0/600 21/293 loss: 1.8003 acc [ 0.387  0.160  0.079]  time 0.02s 
Epoch 0/600 22/293 loss: 1.7899 acc [ 0.392  0.151  0.073]  time 0.53s 
Epoch 0/600 23/293 loss: 1.7778 acc [ 0.392  0.151  0.073]  time 0.02s 
Epoch 0/600 24/293 loss: 1.7655 acc [ 0.382  0.142  0.068]  time 1.25s 
Epoch 0/600 25/293 loss: 1.7542 acc [ 0.372  0.142  0.068]  time 0.02s 
Epoch 0/600 26/293 loss: 1.7536 acc [ 0.367  0.153  0.070]  time 0.22s 
Epoch 0/600 27/293 loss: 1.7450 acc [ 0.367  0.149  0.066]  time 0.02s 
Epoch 0/600 28/293 loss: 1.7365 acc [ 0.367  0.149  0.066]  time 1.53s 
Epoch 0/600 29/293 loss: 1.7292 acc [ 0.364  0.142  0.063]  time 0.02s 
Epoch 0/600 30/293 loss: 1.7194 acc [ 0.369  0.135  0.060]  time 0.02s 
Epoch 0/600 31/293 loss: 1.7413 acc [ 0.381  0.145  0.062]  time 0.02s 
Epoch 0/600 32/293 loss: 1.7340 acc [ 0.381  0.145  0.062]  time 1.50s 
Epoch 0/600 33/293 loss: 1.7284 acc [ 0.391  0.138  0.062]  time 7.73s 
Epoch 0/600 34/293 loss: 1.7206 acc [ 0.385  0.132  0.059]  time 0.02s 
Epoch 0/600 35/293 loss: 1.7258 acc [ 0.389  0.134  0.058]  time 0.02s 
Epoch 0/600 36/293 loss: 1.7184 acc [ 0.400  0.129  0.057]  time 0.02s 
Epoch 0/600 37/293 loss: 1.7114 acc [ 0.393  0.124  0.054]  time 1.45s 
Epoch 0/600 38/293 loss: 1.7060 acc [ 0.393  0.124  0.054]  time 0.52s 
Epoch 0/600 39/293 loss: 1.7008 acc [ 0.393  0.124  0.054]  time 0.02s 
Epoch 0/600 40/293 loss: 1.6939 acc [ 0.397  0.119  0.054]  time 0.02s 
Epoch 0/600 41/293 loss: 1.6884 acc [ 0.394  0.119  0.054]  time 0.67s 
Epoch 0/600 42/293 loss: 1.6840 acc [ 0.394  0.119  0.054]  time 6.06s 
Epoch 0/600 43/293 loss: 1.7008 acc [ 0.401  0.124  0.055]  time 0.04s 
Epoch 0/600 44/293 loss: 1.7002 acc [ 0.411  0.123  0.054]  time 4.27s 
Epoch 0/600 45/293 loss: 1.6919 acc [ 0.419  0.123  0.054]  time 0.03s 
Epoch 0/600 46/293 loss: 1.6878 acc [ 0.419  0.123  0.054]  time 0.02s 
Epoch 0/600 47/293 loss: 1.6847 acc [ 0.415  0.120  0.052]  time 0.02s 
Epoch 0/600 48/293 loss: 1.6810 acc [ 0.415  0.120  0.052]  time 1.53s 
Epoch 0/600 49/293 loss: 1.6752 acc [ 0.415  0.120  0.052]  time 0.03s 
Epoch 0/600 50/293 loss: 1.6693 acc [ 0.414  0.120  0.052]  time 0.02s 
Epoch 0/600 51/293 loss: 1.6661 acc [ 0.414  0.120  0.052]  time 0.02s 
Epoch 0/600 52/293 loss: 1.6619 acc [ 0.411  0.116  0.052]  time 3.99s 
Epoch 0/600 53/293 loss: 1.6589 acc [ 0.411  0.116  0.052]  time 0.03s 
Epoch 0/600 54/293 loss: 1.6542 acc [ 0.408  0.112  0.050]  time 0.03s 
Epoch 0/600 55/293 loss: 1.6515 acc [ 0.408  0.112  0.050]  time 0.03s 
Epoch 0/600 56/293 loss: 1.6500 acc [ 0.406  0.114  0.049]  time 1.56s 
Epoch 0/600 57/293 loss: 1.6477 acc [ 0.412  0.113  0.049]  time 0.03s 
Epoch 0/600 58/293 loss: 1.6452 acc [ 0.412  0.113  0.049]  time 2.45s 
Epoch 0/600 59/293 loss: 1.6439 acc [ 0.418  0.112  0.048]  time 1.29s 
Epoch 0/600 60/293 loss: 1.6411 acc [ 0.412  0.111  0.047]  time 0.02s 
Epoch 0/600 61/293 loss: 1.6379 acc [ 0.412  0.109  0.046]  time 0.03s 
Epoch 0/600 62/293 loss: 1.6347 acc [ 0.409  0.106  0.046]  time 0.02s 
Epoch 0/600 63/293 loss: 1.6326 acc [ 0.409  0.106  0.046]  time 3.34s 
Epoch 0/600 64/293 loss: 1.6477 acc [ 0.415  0.109  0.045]  time 0.02s 
Epoch 0/600 65/293 loss: 1.6425 acc [ 0.418  0.109  0.045]  time 0.02s 
Epoch 0/600 66/293 loss: 1.6404 acc [ 0.418  0.109  0.045]  time 0.02s 
Epoch 0/600 67/293 loss: 1.6362 acc [ 0.424  0.107  0.044]  time 14.23s 
Epoch 0/600 68/293 loss: 1.6334 acc [ 0.421  0.107  0.044]  time 0.02s 
Epoch 0/600 69/293 loss: 1.6325 acc [ 0.430  0.107  0.044]  time 0.02s 
Epoch 0/600 70/293 loss: 1.6310 acc [ 0.432  0.105  0.043]  time 0.02s 
Epoch 0/600 71/293 loss: 1.6289 acc [ 0.425  0.105  0.043]  time 2.57s 
Epoch 0/600 72/293 loss: 1.6249 acc [ 0.428  0.103  0.043]  time 0.02s 
Epoch 0/600 73/293 loss: 1.6231 acc [ 0.420  0.103  0.043]  time 0.02s 
Epoch 0/600 74/293 loss: 1.6201 acc [ 0.419  0.101  0.042]  time 0.02s 
Epoch 0/600 75/293 loss: 1.6186 acc [ 0.418  0.100  0.041]  time 1.37s 
Epoch 0/600 76/293 loss: 1.6143 acc [ 0.424  0.100  0.041]  time 3.18s 
Epoch 0/600 77/293 loss: 1.6104 acc [ 0.424  0.100  0.041]  time 0.02s 
Epoch 0/600 78/293 loss: 1.6090 acc [ 0.424  0.100  0.041]  time 0.02s 
Epoch 0/600 79/293 loss: 1.6054 acc [ 0.424  0.098  0.040]  time 0.02s 
Epoch 0/600 80/293 loss: 1.6032 acc [ 0.423  0.097  0.039]  time 0.90s 
Epoch 0/600 81/293 loss: 1.5987 acc [ 0.430  0.097  0.039]  time 5.19s 
Epoch 0/600 82/293 loss: 1.5975 acc [ 0.430  0.097  0.039]  time 0.02s 
Epoch 0/600 83/293 loss: 1.5946 acc [ 0.433  0.095  0.039]  time 0.02s 
Epoch 0/600 84/293 loss: 1.5935 acc [ 0.436  0.094  0.038]  time 0.02s 
Epoch 0/600 85/293 loss: 1.5906 acc [ 0.437  0.093  0.037]  time 2.52s 
Epoch 0/600 86/293 loss: 1.5896 acc [ 0.437  0.093  0.037]  time 0.02s 
Epoch 0/600 87/293 loss: 1.5896 acc [ 0.443  0.092  0.036]  time 1.81s 
Epoch 0/600 88/293 loss: 1.5872 acc [ 0.443  0.090  0.035]  time 0.03s 
Epoch 0/600 89/293 loss: 1.5862 acc [ 0.443  0.090  0.035]  time 0.03s 
Epoch 0/600 90/293 loss: 1.5951 acc [ 0.448  0.091  0.035]  time 0.03s 
Epoch 0/600 91/293 loss: 1.6170 acc [ 0.455  0.092  0.034]  time 5.66s 
Epoch 0/600 92/293 loss: 1.6158 acc [ 0.455  0.092  0.034]  time 0.02s 
Epoch 0/600 93/293 loss: 1.6129 acc [ 0.453  0.090  0.034]  time 0.03s 
Epoch 0/600 94/293 loss: 1.6103 acc [ 0.451  0.089  0.034]  time 0.03s 
Epoch 0/600 95/293 loss: 1.6076 acc [ 0.450  0.087  0.033]  time 1.87s 
Epoch 0/600 96/293 loss: 1.6051 acc [ 0.454  0.087  0.033]  time 6.87s 
Epoch 0/600 97/293 loss: 1.6023 acc [ 0.453  0.085  0.032]  time 0.02s 
Epoch 0/600 98/293 loss: 1.6012 acc [ 0.453  0.085  0.032]  time 0.02s 
Epoch 0/600 99/293 loss: 1.6002 acc [ 0.453  0.085  0.032]  time 0.02s 
Epoch 0/600 100/293 loss: 1.5987 acc [ 0.451  0.086  0.032]  time 2.74s 
Epoch 0/600 101/293 loss: 1.5966 acc [ 0.451  0.086  0.032]  time 0.02s 
Epoch 0/600 102/293 loss: 1.5944 acc [ 0.451  0.087  0.032]  time 0.02s 
Epoch 0/600 103/293 loss: 1.6114 acc [ 0.457  0.088  0.032]  time 0.02s 
Epoch 0/600 104/293 loss: 1.6090 acc [ 0.456  0.086  0.032]  time 1.65s 
Epoch 0/600 105/293 loss: 1.6074 acc [ 0.454  0.085  0.031]  time 0.03s 
Epoch 0/600 106/293 loss: 1.6057 acc [ 0.452  0.084  0.031]  time 0.02s 
Epoch 0/600 107/293 loss: 1.6056 acc [ 0.457  0.084  0.031]  time 1.12s 
Epoch 0/600 108/293 loss: 1.6046 acc [ 0.457  0.084  0.031]  time 0.84s 
Epoch 0/600 109/293 loss: 1.6037 acc [ 0.457  0.084  0.031]  time 2.73s 
Epoch 0/600 110/293 loss: 1.6016 acc [ 0.455  0.083  0.030]  time 0.03s 
Epoch 0/600 111/293 loss: 1.6006 acc [ 0.455  0.083  0.030]  time 0.02s 
Epoch 0/600 112/293 loss: 1.6004 acc [ 0.460  0.084  0.030]  time 5.68s 
Epoch 0/600 113/293 loss: 1.5996 acc [ 0.454  0.084  0.030]  time 0.02s 
Epoch 0/600 114/293 loss: 1.5974 acc [ 0.458  0.084  0.031]  time 0.02s 
Epoch 0/600 115/293 loss: 1.5955 acc [ 0.457  0.083  0.031]  time 0.02s 
Epoch 0/600 116/293 loss: 1.5946 acc [ 0.457  0.083  0.031]  time 1.23s 
Epoch 0/600 117/293 loss: 1.5923 acc [ 0.459  0.083  0.031]  time 0.03s 
Epoch 0/600 118/293 loss: 1.5916 acc [ 0.459  0.083  0.031]  time 0.02s 
Epoch 0/600 119/293 loss: 1.5890 acc [ 0.462  0.083  0.031]  time 1.33s 
Epoch 0/600 120/293 loss: 1.5883 acc [ 0.462  0.083  0.031]  time 6.01s 
Epoch 0/600 121/293 loss: 1.5863 acc [ 0.462  0.082  0.031]  time 0.02s 
Epoch 0/600 122/293 loss: 1.5844 acc [ 0.463  0.084  0.032]  time 0.41s 
Epoch 0/600 123/293 loss: 1.5837 acc [ 0.463  0.084  0.032]  time 0.03s 
Epoch 0/600 124/293 loss: 1.5830 acc [ 0.463  0.084  0.032]  time 5.43s 
Epoch 0/600 125/293 loss: 1.5824 acc [ 0.463  0.084  0.032]  time 0.02s 
Epoch 0/600 126/293 loss: 1.5801 acc [ 0.463  0.083  0.032]  time 0.02s 
Epoch 0/600 127/293 loss: 1.5795 acc [ 0.463  0.083  0.032]  time 0.02s 
Epoch 0/600 128/293 loss: 1.5789 acc [ 0.463  0.083  0.032]  time 1.14s 
Epoch 0/600 129/293 loss: 1.5772 acc [ 0.461  0.083  0.032]  time 0.02s 
Epoch 0/600 130/293 loss: 1.5751 acc [ 0.464  0.082  0.031]  time 0.02s 
Epoch 0/600 131/293 loss: 1.5779 acc [ 0.466  0.085  0.032]  time 0.02s 
Epoch 0/600 132/293 loss: 1.5773 acc [ 0.466  0.085  0.032]  time 7.35s 
Epoch 0/600 133/293 loss: 1.5748 acc [ 0.468  0.085  0.032]  time 0.02s 
Epoch 0/600 134/293 loss: 1.5735 acc [ 0.466  0.085  0.032]  time 0.02s 
Epoch 0/600 135/293 loss: 1.5712 acc [ 0.466  0.085  0.033]  time 0.02s 
Epoch 0/600 136/293 loss: 1.5707 acc [ 0.466  0.085  0.033]  time 2.33s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 1
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 0
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1714GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 1 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/293 loss: 1.6669 acc [ 0.066  0.040  0.021]  time 10.49s 
Epoch 0/600 1/293 loss: 1.6354 acc [ 0.089  0.020  0.011]  time 6.67s 
Epoch 0/600 2/293 loss: 1.6451 acc [ 0.111  0.039  0.054]  time 9.91s 
Epoch 0/600 3/293 loss: 1.8282 acc [ 0.165  0.086  0.121]  time 8.71s 
Epoch 0/600 4/293 loss: 1.7626 acc [ 0.165  0.086  0.121]  time 7.59s 
Epoch 0/600 5/293 loss: 1.7710 acc [ 0.210  0.086  0.121]  time 14.41s 
Epoch 0/600 6/293 loss: 1.7480 acc [ 0.204  0.073  0.099]  time 6.59s 
Epoch 0/600 7/293 loss: 1.7681 acc [ 0.226  0.073  0.099]  time 13.29s 
Epoch 0/600 8/293 loss: 1.7855 acc [ 0.247  0.074  0.095]  time 12.62s 
Epoch 0/600 9/293 loss: 1.7569 acc [ 0.247  0.074  0.095]  time 12.79s 
Epoch 0/600 10/293 loss: 1.7336 acc [ 0.247  0.074  0.095]  time 15.11s 
Epoch 0/600 11/293 loss: 1.8162 acc [ 0.262  0.104  0.132]  time 8.10s 
Epoch 0/600 12/293 loss: 1.7913 acc [ 0.247  0.095  0.118]  time 10.00s 
Epoch 0/600 13/293 loss: 1.7691 acc [ 0.237  0.086  0.108]  time 6.48s 
Epoch 0/600 14/293 loss: 1.7512 acc [ 0.237  0.086  0.108]  time 8.62s 
Epoch 0/600 15/293 loss: 1.7362 acc [ 0.233  0.086  0.102]  time 10.29s 
Epoch 0/600 16/293 loss: 1.7221 acc [ 0.216  0.086  0.102]  time 6.71s 
Epoch 0/600 17/293 loss: 1.7352 acc [ 0.226  0.100  0.116]  time 8.29s 
Epoch 0/600 18/293 loss: 1.7229 acc [ 0.226  0.100  0.116]  time 9.20s 
Epoch 0/600 19/293 loss: 1.7117 acc [ 0.226  0.100  0.116]  time 7.60s 
Epoch 0/600 20/293 loss: 1.7083 acc [ 0.236  0.096  0.108]  time 8.81s 
Epoch 0/600 21/293 loss: 1.7034 acc [ 0.231  0.093  0.108]  time 12.80s 
Epoch 0/600 22/293 loss: 1.6962 acc [ 0.226  0.089  0.103]  time 7.90s 
Epoch 0/600 23/293 loss: 1.6881 acc [ 0.226  0.089  0.103]  time 8.70s 
Epoch 0/600 24/293 loss: 1.6811 acc [ 0.222  0.086  0.099]  time 7.49s 
Epoch 0/600 25/293 loss: 1.6746 acc [ 0.221  0.084  0.097]  time 7.62s 
Epoch 0/600 26/293 loss: 1.6689 acc [ 0.226  0.084  0.097]  time 8.38s 
Epoch 0/600 27/293 loss: 1.6764 acc [ 0.230  0.086  0.096]  time 6.81s 
Epoch 0/600 28/293 loss: 1.6855 acc [ 0.233  0.087  0.095]  time 6.90s 
Epoch 0/600 29/293 loss: 1.6894 acc [ 0.251  0.084  0.095]  time 13.10s 
Epoch 0/600 30/293 loss: 1.6833 acc [ 0.251  0.084  0.095]  time 7.40s 
Epoch 0/600 31/293 loss: 1.6775 acc [ 0.251  0.082  0.092]  time 9.19s 
Epoch 0/600 32/293 loss: 1.6721 acc [ 0.251  0.082  0.092]  time 9.20s 
Epoch 0/600 33/293 loss: 1.6648 acc [ 0.255  0.079  0.087]  time 7.21s 
Epoch 0/600 34/293 loss: 1.6610 acc [ 0.259  0.077  0.087]  time 7.59s 
Epoch 0/600 35/293 loss: 1.6566 acc [ 0.266  0.074  0.084]  time 8.70s 
Epoch 0/600 36/293 loss: 1.7604 acc [ 0.285  0.079  0.092]  time 12.80s 
Epoch 0/600 37/293 loss: 1.7546 acc [ 0.296  0.078  0.089]  time 14.91s 
Epoch 0/600 38/293 loss: 1.8191 acc [ 0.312  0.081  0.091]  time 10.20s 
Epoch 0/600 39/293 loss: 1.8111 acc [ 0.312  0.081  0.091]  time 10.49s 
Epoch 0/600 40/293 loss: 1.8035 acc [ 0.312  0.081  0.091]  time 7.90s 
Epoch 0/600 41/293 loss: 1.7963 acc [ 0.312  0.081  0.091]  time 8.59s 
Epoch 0/600 42/293 loss: 1.8024 acc [ 0.315  0.081  0.092]  time 16.91s 
Epoch 0/600 43/293 loss: 1.7944 acc [ 0.315  0.081  0.092]  time 8.39s 
Epoch 0/600 44/293 loss: 1.8223 acc [ 0.326  0.083  0.096]  time 10.39s 
Epoch 0/600 45/293 loss: 1.8162 acc [ 0.319  0.081  0.093]  time 6.90s 
Epoch 0/600 46/293 loss: 1.8116 acc [ 0.322  0.079  0.091]  time 10.92s 
Epoch 0/600 47/293 loss: 1.8039 acc [ 0.315  0.076  0.088]  time 8.70s 
Epoch 0/600 48/293 loss: 1.7969 acc [ 0.312  0.076  0.085]  time 7.10s 
Epoch 0/600 49/293 loss: 1.7915 acc [ 0.312  0.074  0.083]  time 9.11s 
Epoch 0/600 50/293 loss: 1.7862 acc [ 0.314  0.074  0.083]  time 10.49s 
Epoch 0/600 51/293 loss: 1.7816 acc [ 0.320  0.072  0.083]  time 9.30s 
Epoch 0/600 52/293 loss: 1.7852 acc [ 0.332  0.071  0.081]  time 15.11s 
Epoch 0/600 53/293 loss: 1.7823 acc [ 0.338  0.071  0.080]  time 11.70s 
Epoch 0/600 54/293 loss: 1.7761 acc [ 0.334  0.069  0.079]  time 6.68s 
Epoch 0/600 55/293 loss: 1.7711 acc [ 0.330  0.068  0.077]  time 7.50s 
Epoch 0/600 56/293 loss: 1.7662 acc [ 0.326  0.069  0.077]  time 8.91s 
Epoch 0/600 57/293 loss: 1.7617 acc [ 0.326  0.069  0.077]  time 11.09s 
Epoch 0/600 58/293 loss: 1.7592 acc [ 0.325  0.068  0.076]  time 8.12s 
Epoch 0/600 59/293 loss: 1.7713 acc [ 0.330  0.068  0.081]  time 9.10s 
Epoch 0/600 60/293 loss: 1.7710 acc [ 0.336  0.069  0.080]  time 8.80s 
Epoch 0/600 61/293 loss: 1.7667 acc [ 0.336  0.069  0.080]  time 7.79s 
Epoch 0/600 62/293 loss: 1.7622 acc [ 0.340  0.068  0.079]  time 13.41s 
Epoch 0/600 63/293 loss: 1.7585 acc [ 0.336  0.068  0.077]  time 12.30s 
Epoch 0/600 64/293 loss: 1.7537 acc [ 0.335  0.068  0.077]  time 7.10s 
Epoch 0/600 65/293 loss: 1.7528 acc [ 0.333  0.067  0.077]  time 7.50s 
Epoch 0/600 66/293 loss: 1.7491 acc [ 0.333  0.067  0.077]  time 7.80s 
Epoch 0/600 67/293 loss: 1.7454 acc [ 0.333  0.067  0.077]  time 8.10s 
Epoch 0/600 68/293 loss: 1.7405 acc [ 0.334  0.066  0.076]  time 8.28s 
Epoch 0/600 69/293 loss: 1.7369 acc [ 0.339  0.066  0.076]  time 10.10s 
Epoch 0/600 70/293 loss: 1.7336 acc [ 0.339  0.066  0.076]  time 7.72s 
Epoch 0/600 71/293 loss: 1.7304 acc [ 0.339  0.066  0.076]  time 6.91s 
Epoch 0/600 72/293 loss: 1.7268 acc [ 0.337  0.066  0.075]  time 7.40s 
Epoch 0/600 73/293 loss: 1.7228 acc [ 0.339  0.066  0.075]  time 9.99s 
Epoch 0/600 74/293 loss: 1.7186 acc [ 0.340  0.064  0.073]  time 9.50s 
Epoch 0/600 75/293 loss: 1.7140 acc [ 0.340  0.063  0.072]  time 9.72s 
Epoch 0/600 76/293 loss: 1.7112 acc [ 0.340  0.063  0.072]  time 8.67s 
Epoch 0/600 77/293 loss: 1.7069 acc [ 0.339  0.062  0.072]  time 8.40s 
Epoch 0/600 78/293 loss: 1.7033 acc [ 0.337  0.061  0.070]  time 8.11s 
Epoch 0/600 79/293 loss: 1.7008 acc [ 0.337  0.061  0.070]  time 9.80s 
Epoch 0/600 80/293 loss: 1.6975 acc [ 0.344  0.060  0.070]  time 10.90s 
Epoch 0/600 81/293 loss: 1.6951 acc [ 0.344  0.060  0.070]  time 18.90s 
Epoch 0/600 82/293 loss: 1.6911 acc [ 0.344  0.060  0.070]  time 7.51s 
Epoch 0/600 83/293 loss: 1.6888 acc [ 0.344  0.060  0.070]  time 7.69s 
Epoch 0/600 84/293 loss: 1.6850 acc [ 0.344  0.060  0.070]  time 6.60s 
Epoch 0/600 85/293 loss: 1.6813 acc [ 0.346  0.059  0.069]  time 7.50s 
Epoch 0/600 86/293 loss: 1.6781 acc [ 0.343  0.059  0.069]  time 7.69s 
Epoch 0/600 87/293 loss: 1.6761 acc [ 0.338  0.059  0.069]  time 7.41s 
Epoch 0/600 88/293 loss: 1.6735 acc [ 0.338  0.059  0.068]  time 7.59s 
Epoch 0/600 89/293 loss: 1.6722 acc [ 0.340  0.059  0.067]  time 8.40s 
Epoch 0/600 90/293 loss: 1.6701 acc [ 0.339  0.059  0.066]  time 7.51s 
Epoch 0/600 91/293 loss: 1.6893 acc [ 0.344  0.060  0.066]  time 12.31s 
Epoch 0/600 92/293 loss: 1.6873 acc [ 0.345  0.059  0.065]  time 7.67s 
Epoch 0/600 93/293 loss: 1.6864 acc [ 0.342  0.059  0.064]  time 6.94s 
Epoch 0/600 94/293 loss: 1.6836 acc [ 0.340  0.058  0.063]  time 9.28s 
Epoch 0/600 95/293 loss: 1.6801 acc [ 0.339  0.057  0.062]  time 6.98s 
Epoch 0/600 96/293 loss: 1.6841 acc [ 0.340  0.057  0.062]  time 8.22s 
Epoch 0/600 97/293 loss: 1.6806 acc [ 0.338  0.057  0.062]  time 7.50s 
Epoch 0/600 98/293 loss: 1.6793 acc [ 0.342  0.056  0.061]  time 8.59s 
Epoch 0/600 99/293 loss: 1.6776 acc [ 0.342  0.056  0.061]  time 7.90s 
Epoch 0/600 100/293 loss: 1.6747 acc [ 0.341  0.056  0.060]  time 7.50s 
Epoch 0/600 101/293 loss: 1.6730 acc [ 0.341  0.056  0.060]  time 6.89s 
Epoch 0/600 102/293 loss: 1.6705 acc [ 0.340  0.055  0.059]  time 8.81s 
Epoch 0/600 103/293 loss: 1.6717 acc [ 0.341  0.056  0.059]  time 8.79s 
Epoch 0/600 104/293 loss: 1.6721 acc [ 0.339  0.056  0.059]  time 18.50s 
Epoch 0/600 105/293 loss: 1.6705 acc [ 0.339  0.056  0.059]  time 7.22s 
Epoch 0/600 106/293 loss: 1.6689 acc [ 0.339  0.056  0.059]  time 9.98s 
Epoch 0/600 107/293 loss: 1.6673 acc [ 0.341  0.055  0.059]  time 6.90s 
Epoch 0/600 108/293 loss: 1.6656 acc [ 0.341  0.054  0.059]  time 9.80s 
Epoch 0/600 109/293 loss: 1.6633 acc [ 0.340  0.054  0.058]  time 7.60s 
Epoch 0/600 110/293 loss: 1.6610 acc [ 0.338  0.054  0.058]  time 9.11s 
Epoch 0/600 111/293 loss: 1.6597 acc [ 0.337  0.053  0.057]  time 7.70s 
Epoch 0/600 112/293 loss: 1.6568 acc [ 0.337  0.052  0.056]  time 7.49s 
Epoch 0/600 113/293 loss: 1.6590 acc [ 0.338  0.052  0.056]  time 8.41s 
Epoch 0/600 114/293 loss: 1.6576 acc [ 0.338  0.052  0.056]  time 7.98s 
Epoch 0/600 115/293 loss: 1.6566 acc [ 0.340  0.052  0.056]  time 7.72s 
Epoch 0/600 116/293 loss: 1.6574 acc [ 0.342  0.053  0.056]  time 9.61s 
Epoch 0/600 117/293 loss: 1.6589 acc [ 0.343  0.053  0.056]  time 10.19s 
Epoch 0/600 118/293 loss: 1.6574 acc [ 0.341  0.053  0.056]  time 7.60s 
Epoch 0/600 119/293 loss: 1.6548 acc [ 0.341  0.053  0.056]  time 8.31s 
Epoch 0/600 120/293 loss: 1.6531 acc [ 0.340  0.053  0.056]  time 7.88s 
Epoch 0/600 121/293 loss: 1.6518 acc [ 0.340  0.053  0.056]  time 6.62s 
Epoch 0/600 122/293 loss: 1.6497 acc [ 0.341  0.053  0.056]  time 9.00s 
Epoch 0/600 123/293 loss: 1.6484 acc [ 0.338  0.053  0.056]  time 8.28s 
Epoch 0/600 124/293 loss: 1.6472 acc [ 0.338  0.053  0.056]  time 15.91s 
Epoch 0/600 125/293 loss: 1.6458 acc [ 0.337  0.052  0.055]  time 10.60s 
Epoch 0/600 126/293 loss: 1.6450 acc [ 0.338  0.053  0.055]  time 8.62s 
Epoch 0/600 127/293 loss: 1.6427 acc [ 0.340  0.052  0.055]  time 6.89s 
Epoch 0/600 128/293 loss: 1.6416 acc [ 0.340  0.052  0.055]  time 9.20s 
Epoch 0/600 129/293 loss: 1.6405 acc [ 0.340  0.052  0.055]  time 15.60s 
Epoch 0/600 130/293 loss: 1.6394 acc [ 0.340  0.052  0.055]  time 8.60s 
Epoch 0/600 131/293 loss: 1.6407 acc [ 0.342  0.052  0.055]  time 12.30s 
Epoch 0/600 132/293 loss: 1.6509 acc [ 0.345  0.053  0.056]  time 7.40s 
Epoch 0/600 133/293 loss: 1.6498 acc [ 0.345  0.053  0.056]  time 9.18s 
Epoch 0/600 134/293 loss: 1.6478 acc [ 0.345  0.053  0.055]  time 7.81s 
Epoch 0/600 135/293 loss: 1.6467 acc [ 0.345  0.053  0.055]  time 7.30s 
Epoch 0/600 136/293 loss: 1.6456 acc [ 0.345  0.053  0.055]  time 15.20s 
Epoch 0/600 137/293 loss: 1.6446 acc [ 0.345  0.053  0.055]  time 15.70s 
Epoch 0/600 138/293 loss: 1.6444 acc [ 0.343  0.052  0.054]  time 8.40s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 1
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 8
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1712GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 1 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/293 loss: 1.8424 acc [ 0.308  0.386  0.383]  time 3.00s 
Epoch 0/600 1/293 loss: 1.8023 acc [ 0.466  0.271  0.270]  time 4.64s 
Epoch 0/600 2/293 loss: 1.7427 acc [ 0.440  0.239  0.237]  time 0.02s 
Epoch 0/600 3/293 loss: 1.6853 acc [ 0.377  0.239  0.237]  time 0.02s 
Epoch 0/600 4/293 loss: 1.6560 acc [ 0.378  0.180  0.179]  time 0.02s 
Epoch 0/600 5/293 loss: 1.6300 acc [ 0.378  0.180  0.179]  time 0.02s 
Epoch 0/600 6/293 loss: 1.8486 acc [ 0.432  0.263  0.267]  time 0.24s 
Epoch 0/600 7/293 loss: 1.8108 acc [ 0.423  0.234  0.237]  time 0.02s 
Epoch 0/600 8/293 loss: 1.7762 acc [ 0.423  0.234  0.237]  time 0.02s 
Epoch 0/600 9/293 loss: 1.7486 acc [ 0.423  0.234  0.237]  time 3.20s 
Epoch 0/600 10/293 loss: 1.7315 acc [ 0.422  0.211  0.214]  time 0.02s 
Epoch 0/600 11/293 loss: 1.7122 acc [ 0.422  0.211  0.214]  time 0.02s 
Epoch 0/600 12/293 loss: 1.6974 acc [ 0.415  0.186  0.214]  time 0.02s 
Epoch 0/600 13/293 loss: 1.6830 acc [ 0.402  0.176  0.199]  time 0.02s 
Epoch 0/600 14/293 loss: 1.7046 acc [ 0.423  0.197  0.218]  time 0.02s 
Epoch 0/600 15/293 loss: 1.6919 acc [ 0.423  0.197  0.218]  time 0.02s 
Epoch 0/600 16/293 loss: 1.6806 acc [ 0.423  0.197  0.218]  time 0.02s 
Epoch 0/600 17/293 loss: 1.6705 acc [ 0.423  0.197  0.218]  time 10.51s 
Epoch 0/600 18/293 loss: 1.6608 acc [ 0.408  0.179  0.197]  time 0.02s 
Epoch 0/600 19/293 loss: 1.6530 acc [ 0.397  0.173  0.187]  time 0.02s 
Epoch 0/600 20/293 loss: 1.6457 acc [ 0.397  0.173  0.187]  time 0.02s 
Epoch 0/600 21/293 loss: 1.6384 acc [ 0.389  0.168  0.181]  time 0.02s 
Epoch 0/600 22/293 loss: 1.6314 acc [ 0.383  0.168  0.181]  time 0.02s 
Epoch 0/600 23/293 loss: 1.6259 acc [ 0.383  0.168  0.181]  time 0.02s 
Epoch 0/600 24/293 loss: 1.6201 acc [ 0.375  0.156  0.167]  time 0.02s 
Epoch 0/600 25/293 loss: 1.6155 acc [ 0.375  0.156  0.167]  time 1.60s 
Epoch 0/600 26/293 loss: 1.6250 acc [ 0.379  0.165  0.169]  time 0.02s 
Epoch 0/600 27/293 loss: 1.6200 acc [ 0.374  0.165  0.169]  time 0.02s 
Epoch 0/600 28/293 loss: 1.6258 acc [ 0.373  0.178  0.183]  time 0.02s 
Epoch 0/600 29/293 loss: 1.6216 acc [ 0.373  0.178  0.183]  time 0.02s 
Epoch 0/600 30/293 loss: 1.6202 acc [ 0.370  0.185  0.173]  time 0.02s 
Epoch 0/600 31/293 loss: 1.6160 acc [ 0.365  0.182  0.172]  time 0.02s 
Epoch 0/600 32/293 loss: 1.6104 acc [ 0.363  0.174  0.165]  time 0.03s 
Epoch 0/600 33/293 loss: 1.6053 acc [ 0.357  0.174  0.165]  time 2.05s 
Epoch 0/600 34/293 loss: 1.6064 acc [ 0.371  0.174  0.165]  time 0.29s 
Epoch 0/600 35/293 loss: 1.6054 acc [ 0.376  0.176  0.157]  time 0.02s 
Epoch 0/600 36/293 loss: 1.6023 acc [ 0.372  0.179  0.161]  time 0.02s 
Epoch 0/600 37/293 loss: 1.6019 acc [ 0.379  0.179  0.161]  time 0.02s 
Epoch 0/600 38/293 loss: 1.5993 acc [ 0.379  0.179  0.161]  time 0.02s 
Epoch 0/600 39/293 loss: 1.5977 acc [ 0.386  0.171  0.161]  time 0.60s 
Epoch 0/600 40/293 loss: 1.6055 acc [ 0.398  0.175  0.161]  time 1.38s 
Epoch 0/600 41/293 loss: 1.6000 acc [ 0.397  0.175  0.161]  time 2.22s 
Epoch 0/600 42/293 loss: 1.5976 acc [ 0.397  0.175  0.161]  time 6.50s 
Epoch 0/600 43/293 loss: 1.5943 acc [ 0.394  0.172  0.156]  time 0.02s 
Epoch 0/600 44/293 loss: 1.5907 acc [ 0.390  0.167  0.150]  time 0.02s 
Epoch 0/600 45/293 loss: 1.5888 acc [ 0.390  0.167  0.150]  time 0.03s 
Epoch 0/600 46/293 loss: 1.5848 acc [ 0.395  0.163  0.145]  time 0.02s 
Epoch 0/600 47/293 loss: 1.5830 acc [ 0.395  0.163  0.145]  time 0.02s 
Epoch 0/600 48/293 loss: 1.5813 acc [ 0.395  0.163  0.145]  time 0.02s 
Epoch 0/600 49/293 loss: 1.5797 acc [ 0.395  0.163  0.145]  time 0.02s 
Epoch 0/600 50/293 loss: 1.5783 acc [ 0.391  0.160  0.141]  time 2.35s 
Epoch 0/600 51/293 loss: 1.5768 acc [ 0.391  0.160  0.141]  time 0.02s 
Epoch 0/600 52/293 loss: 1.5760 acc [ 0.385  0.157  0.137]  time 0.02s 
Epoch 0/600 53/293 loss: 1.5975 acc [ 0.394  0.160  0.135]  time 0.02s 
Epoch 0/600 54/293 loss: 1.5950 acc [ 0.386  0.155  0.135]  time 0.02s 
Epoch 0/600 55/293 loss: 1.6014 acc [ 0.394  0.160  0.143]  time 0.02s 
Epoch 0/600 56/293 loss: 1.5996 acc [ 0.384  0.155  0.138]  time 0.02s 
Epoch 0/600 57/293 loss: 1.5992 acc [ 0.393  0.154  0.136]  time 4.82s 
Epoch 0/600 58/293 loss: 1.5975 acc [ 0.393  0.154  0.136]  time 1.71s 
Epoch 0/600 59/293 loss: 1.5945 acc [ 0.392  0.150  0.136]  time 0.02s 
Epoch 0/600 60/293 loss: 1.5918 acc [ 0.387  0.145  0.131]  time 0.02s 
Epoch 0/600 61/293 loss: 1.5903 acc [ 0.387  0.145  0.131]  time 0.02s 
Epoch 0/600 62/293 loss: 1.5901 acc [ 0.384  0.148  0.132]  time 0.02s 
Epoch 0/600 63/293 loss: 1.6007 acc [ 0.388  0.149  0.131]  time 0.02s 
Epoch 0/600 64/293 loss: 1.5976 acc [ 0.388  0.149  0.131]  time 0.02s 
Epoch 0/600 65/293 loss: 1.6001 acc [ 0.392  0.153  0.134]  time 2.76s 
Epoch 0/600 66/293 loss: 1.5984 acc [ 0.385  0.153  0.134]  time 0.76s 
Epoch 0/600 67/293 loss: 1.5970 acc [ 0.385  0.153  0.134]  time 0.02s 
Epoch 0/600 68/293 loss: 1.5944 acc [ 0.381  0.149  0.130]  time 0.02s 
Epoch 0/600 69/293 loss: 1.5916 acc [ 0.382  0.149  0.130]  time 0.02s 
Epoch 0/600 70/293 loss: 1.6035 acc [ 0.387  0.154  0.135]  time 0.02s 
Epoch 0/600 71/293 loss: 1.6009 acc [ 0.383  0.154  0.135]  time 0.02s 
Epoch 0/600 72/293 loss: 1.5985 acc [ 0.380  0.151  0.132]  time 0.03s 
Epoch 0/600 73/293 loss: 1.5962 acc [ 0.384  0.151  0.132]  time 9.19s 
Epoch 0/600 74/293 loss: 1.5949 acc [ 0.384  0.151  0.132]  time 0.02s 
Epoch 0/600 75/293 loss: 1.5937 acc [ 0.384  0.151  0.132]  time 0.02s 
Epoch 0/600 76/293 loss: 1.5925 acc [ 0.384  0.151  0.132]  time 0.02s 
Epoch 0/600 77/293 loss: 1.5913 acc [ 0.384  0.151  0.132]  time 0.02s 
Epoch 0/600 78/293 loss: 1.5920 acc [ 0.387  0.152  0.131]  time 0.02s 
Epoch 0/600 79/293 loss: 1.5902 acc [ 0.390  0.152  0.130]  time 0.02s 
Epoch 0/600 80/293 loss: 1.5878 acc [ 0.387  0.149  0.127]  time 0.03s 
Epoch 0/600 81/293 loss: 1.5867 acc [ 0.387  0.149  0.127]  time 2.91s 
Epoch 0/600 82/293 loss: 1.5855 acc [ 0.381  0.145  0.124]  time 0.03s 
Epoch 0/600 83/293 loss: 1.5826 acc [ 0.383  0.145  0.124]  time 0.03s 
Epoch 0/600 84/293 loss: 1.5803 acc [ 0.384  0.145  0.124]  time 0.02s 
Epoch 0/600 85/293 loss: 1.6103 acc [ 0.392  0.147  0.125]  time 0.03s 
Epoch 0/600 86/293 loss: 1.6091 acc [ 0.392  0.147  0.125]  time 0.03s 
Epoch 0/600 87/293 loss: 1.6065 acc [ 0.392  0.147  0.125]  time 0.03s 
Epoch 0/600 88/293 loss: 1.6063 acc [ 0.398  0.146  0.123]  time 0.03s 
Epoch 0/600 89/293 loss: 1.6044 acc [ 0.396  0.146  0.122]  time 2.45s 
Epoch 0/600 90/293 loss: 1.6022 acc [ 0.394  0.144  0.121]  time 0.02s 
Epoch 0/600 91/293 loss: 1.6009 acc [ 0.389  0.144  0.121]  time 0.02s 
Epoch 0/600 92/293 loss: 1.5992 acc [ 0.394  0.144  0.121]  time 0.02s 
Epoch 0/600 93/293 loss: 1.6052 acc [ 0.397  0.147  0.122]  time 0.02s 
Epoch 0/600 94/293 loss: 1.6041 acc [ 0.397  0.147  0.122]  time 0.02s 
Epoch 0/600 95/293 loss: 1.6030 acc [ 0.397  0.147  0.122]  time 0.02s 
Epoch 0/600 96/293 loss: 1.6019 acc [ 0.397  0.147  0.122]  time 0.02s 
Epoch 0/600 97/293 loss: 1.6009 acc [ 0.397  0.147  0.122]  time 0.96s 
Epoch 0/600 98/293 loss: 1.5987 acc [ 0.399  0.147  0.122]  time 0.03s 
Epoch 0/600 99/293 loss: 1.5965 acc [ 0.398  0.145  0.120]  time 0.03s 
Epoch 0/600 100/293 loss: 1.5959 acc [ 0.401  0.150  0.125]  time 0.03s 
Epoch 0/600 101/293 loss: 1.5939 acc [ 0.398  0.148  0.122]  time 0.02s 
Epoch 0/600 102/293 loss: 1.5945 acc [ 0.399  0.151  0.125]  time 0.03s 
Epoch 0/600 103/293 loss: 1.5936 acc [ 0.399  0.151  0.125]  time 0.02s 
Epoch 0/600 104/293 loss: 1.5953 acc [ 0.406  0.154  0.125]  time 9.10s 
Epoch 0/600 105/293 loss: 1.5940 acc [ 0.404  0.152  0.124]  time 0.02s 
Epoch 0/600 106/293 loss: 1.5931 acc [ 0.404  0.152  0.124]  time 0.02s 
Epoch 0/600 107/293 loss: 1.5907 acc [ 0.403  0.151  0.123]  time 0.02s 
Epoch 0/600 108/293 loss: 1.5890 acc [ 0.400  0.148  0.121]  time 0.02s 
Epoch 0/600 109/293 loss: 1.5873 acc [ 0.398  0.148  0.121]  time 0.02s 
Epoch 0/600 110/293 loss: 1.5848 acc [ 0.400  0.146  0.119]  time 0.02s 
Epoch 0/600 111/293 loss: 1.5839 acc [ 0.396  0.146  0.119]  time 0.02s 
Epoch 0/600 112/293 loss: 1.5832 acc [ 0.396  0.146  0.119]  time 4.24s 
Epoch 0/600 113/293 loss: 1.5928 acc [ 0.401  0.151  0.123]  time 0.02s 
Epoch 0/600 114/293 loss: 1.5920 acc [ 0.401  0.151  0.123]  time 0.02s 
Epoch 0/600 115/293 loss: 1.5934 acc [ 0.405  0.153  0.125]  time 0.02s 
Epoch 0/600 116/293 loss: 1.5913 acc [ 0.405  0.151  0.123]  time 0.02s 
Epoch 0/600 117/293 loss: 1.5896 acc [ 0.403  0.150  0.123]  time 0.02s 
Epoch 0/600 118/293 loss: 1.5879 acc [ 0.403  0.147  0.123]  time 0.02s 
Epoch 0/600 119/293 loss: 1.5857 acc [ 0.404  0.146  0.121]  time 0.02s 
Epoch 0/600 120/293 loss: 1.5830 acc [ 0.409  0.146  0.121]  time 5.34s 
Epoch 0/600 121/293 loss: 1.5822 acc [ 0.409  0.147  0.121]  time 0.02s 
Epoch 0/600 122/293 loss: 1.5810 acc [ 0.411  0.147  0.121]  time 0.02s 
Epoch 0/600 123/293 loss: 1.5803 acc [ 0.411  0.147  0.121]  time 0.02s 
Epoch 0/600 124/293 loss: 1.5783 acc [ 0.411  0.145  0.119]  time 0.02s 
Epoch 0/600 125/293 loss: 1.5767 acc [ 0.409  0.145  0.119]  time 0.02s 
Epoch 0/600 126/293 loss: 1.5761 acc [ 0.409  0.145  0.119]  time 2.28s 
Epoch 0/600 127/293 loss: 1.5740 acc [ 0.412  0.143  0.117]  time 0.03s 
Epoch 0/600 128/293 loss: 1.5734 acc [ 0.412  0.143  0.117]  time 0.32s 
Epoch 0/600 129/293 loss: 1.5717 acc [ 0.413  0.145  0.119]  time 0.02s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 1
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 8
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1712GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 1 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/293 loss: 1.5182 acc [ 0.209  0.010  0.002]  time 3.08s 
Epoch 0/600 1/293 loss: 1.7157 acc [ 0.332  0.029  0.134]  time 0.04s 
Epoch 0/600 2/293 loss: 1.6422 acc [ 0.240  0.020  0.109]  time 0.03s 
Epoch 0/600 3/293 loss: 1.6108 acc [ 0.238  0.037  0.102]  time 0.02s 
Epoch 0/600 4/293 loss: 1.6070 acc [ 0.293  0.033  0.084]  time 4.82s 
Epoch 0/600 5/293 loss: 1.5863 acc [ 0.279  0.028  0.084]  time 0.02s 
Epoch 0/600 6/293 loss: 1.5835 acc [ 0.319  0.028  0.084]  time 0.02s 
Epoch 0/600 7/293 loss: 1.5731 acc [ 0.319  0.028  0.084]  time 3.15s 
Epoch 0/600 8/293 loss: 1.5590 acc [ 0.319  0.025  0.084]  time 0.02s 
Epoch 0/600 9/293 loss: 1.5524 acc [ 0.307  0.028  0.083]  time 0.02s 
Epoch 0/600 10/293 loss: 1.5787 acc [ 0.349  0.033  0.086]  time 0.02s 
Epoch 0/600 11/293 loss: 1.5761 acc [ 0.368  0.031  0.081]  time 1.68s 
Epoch 0/600 12/293 loss: 1.5651 acc [ 0.363  0.030  0.072]  time 0.03s 
Epoch 0/600 13/293 loss: 1.5569 acc [ 0.369  0.031  0.068]  time 0.02s 
Epoch 0/600 14/293 loss: 1.6522 acc [ 0.392  0.040  0.090]  time 0.02s 
Epoch 0/600 15/293 loss: 1.6420 acc [ 0.392  0.037  0.083]  time 0.02s 
Epoch 0/600 16/293 loss: 1.6336 acc [ 0.392  0.037  0.083]  time 0.02s 
Epoch 0/600 17/293 loss: 1.6246 acc [ 0.391  0.037  0.083]  time 0.02s 
Epoch 0/600 18/293 loss: 1.6130 acc [ 0.409  0.035  0.083]  time 3.89s 
Epoch 0/600 19/293 loss: 1.6074 acc [ 0.409  0.035  0.083]  time 7.99s 
Epoch 0/600 20/293 loss: 1.6196 acc [ 0.414  0.034  0.096]  time 0.02s 
Epoch 0/600 21/293 loss: 1.6120 acc [ 0.406  0.032  0.096]  time 0.02s 
Epoch 0/600 22/293 loss: 1.6071 acc [ 0.406  0.032  0.096]  time 0.02s 
Epoch 0/600 23/293 loss: 1.7607 acc [ 0.427  0.034  0.122]  time 0.02s 
Epoch 0/600 24/293 loss: 1.7480 acc [ 0.418  0.035  0.114]  time 0.02s 
Epoch 0/600 25/293 loss: 1.7385 acc [ 0.418  0.035  0.114]  time 0.02s 
Epoch 0/600 26/293 loss: 1.7256 acc [ 0.421  0.035  0.114]  time 0.02s 
Epoch 0/600 27/293 loss: 1.7275 acc [ 0.433  0.040  0.118]  time 6.53s 
Epoch 0/600 28/293 loss: 1.7197 acc [ 0.433  0.040  0.118]  time 0.02s 
Epoch 0/600 29/293 loss: 1.7124 acc [ 0.433  0.040  0.118]  time 0.02s 
Epoch 0/600 30/293 loss: 1.7055 acc [ 0.433  0.040  0.118]  time 0.02s 
Epoch 0/600 31/293 loss: 1.6967 acc [ 0.427  0.040  0.118]  time 0.02s 
Epoch 0/600 32/293 loss: 1.6961 acc [ 0.426  0.040  0.121]  time 0.02s 
Epoch 0/600 33/293 loss: 1.6901 acc [ 0.411  0.040  0.116]  time 0.02s 
Epoch 0/600 34/293 loss: 1.6818 acc [ 0.409  0.040  0.116]  time 0.02s 
Epoch 0/600 35/293 loss: 1.6982 acc [ 0.410  0.039  0.122]  time 1.40s 
Epoch 0/600 36/293 loss: 1.6928 acc [ 0.410  0.039  0.122]  time 0.02s 
Epoch 0/600 37/293 loss: 1.6877 acc [ 0.412  0.039  0.118]  time 0.02s 
Epoch 0/600 38/293 loss: 1.6828 acc [ 0.398  0.039  0.118]  time 0.02s 
Epoch 0/600 39/293 loss: 1.6787 acc [ 0.392  0.041  0.119]  time 0.03s 
Epoch 0/600 40/293 loss: 1.6743 acc [ 0.392  0.041  0.119]  time 0.02s 
Epoch 0/600 41/293 loss: 1.6702 acc [ 0.392  0.041  0.119]  time 0.02s 
Epoch 0/600 42/293 loss: 1.6660 acc [ 0.393  0.040  0.118]  time 0.02s 
Epoch 0/600 43/293 loss: 1.6614 acc [ 0.383  0.040  0.118]  time 1.80s 
Epoch 0/600 44/293 loss: 1.6562 acc [ 0.387  0.039  0.114]  time 0.30s 
Epoch 0/600 45/293 loss: 1.6529 acc [ 0.387  0.039  0.114]  time 0.02s 
Epoch 0/600 46/293 loss: 1.6496 acc [ 0.387  0.039  0.114]  time 0.02s 
Epoch 0/600 47/293 loss: 1.6465 acc [ 0.387  0.039  0.114]  time 0.02s 
Epoch 0/600 48/293 loss: 1.6435 acc [ 0.387  0.039  0.114]  time 0.02s 
Epoch 0/600 49/293 loss: 1.6391 acc [ 0.390  0.038  0.111]  time 0.02s 
Epoch 0/600 50/293 loss: 1.6350 acc [ 0.385  0.038  0.107]  time 0.02s 
Epoch 0/600 51/293 loss: 1.6304 acc [ 0.391  0.038  0.107]  time 8.28s 
Epoch 0/600 52/293 loss: 1.6272 acc [ 0.386  0.039  0.105]  time 0.02s 
Epoch 0/600 53/293 loss: 1.6235 acc [ 0.380  0.039  0.105]  time 0.02s 
Epoch 0/600 54/293 loss: 1.6215 acc [ 0.375  0.043  0.104]  time 0.02s 
Epoch 0/600 55/293 loss: 1.6418 acc [ 0.381  0.042  0.112]  time 0.02s 
Epoch 0/600 56/293 loss: 1.6370 acc [ 0.379  0.041  0.108]  time 0.02s 
Epoch 0/600 57/293 loss: 1.6326 acc [ 0.379  0.040  0.108]  time 0.02s 
Epoch 0/600 58/293 loss: 1.6359 acc [ 0.390  0.040  0.108]  time 0.02s 
Epoch 0/600 59/293 loss: 1.6312 acc [ 0.399  0.041  0.105]  time 9.29s 
Epoch 0/600 60/293 loss: 1.6278 acc [ 0.396  0.041  0.105]  time 0.02s 
Epoch 0/600 61/293 loss: 1.6257 acc [ 0.396  0.041  0.105]  time 0.02s 
Epoch 0/600 62/293 loss: 1.6340 acc [ 0.394  0.040  0.109]  time 0.02s 
Epoch 0/600 63/293 loss: 1.6311 acc [ 0.394  0.041  0.108]  time 0.02s 
Epoch 0/600 64/293 loss: 1.6280 acc [ 0.392  0.041  0.108]  time 0.02s 
Epoch 0/600 65/293 loss: 1.6261 acc [ 0.385  0.041  0.108]  time 0.02s 
Epoch 0/600 66/293 loss: 1.6239 acc [ 0.385  0.041  0.108]  time 0.02s 
Epoch 0/600 67/293 loss: 1.6279 acc [ 0.385  0.044  0.106]  time 1.28s 
Epoch 0/600 68/293 loss: 1.6243 acc [ 0.382  0.044  0.106]  time 0.02s 
Epoch 0/600 69/293 loss: 1.6225 acc [ 0.382  0.044  0.106]  time 0.02s 
Epoch 0/600 70/293 loss: 1.6191 acc [ 0.388  0.043  0.104]  time 0.02s 
Epoch 0/600 71/293 loss: 1.6175 acc [ 0.388  0.043  0.104]  time 0.02s 
Epoch 0/600 72/293 loss: 1.6146 acc [ 0.386  0.043  0.101]  time 0.02s 
Epoch 0/600 73/293 loss: 1.6128 acc [ 0.391  0.043  0.101]  time 0.02s 
Epoch 0/600 74/293 loss: 1.6308 acc [ 0.398  0.047  0.103]  time 0.02s 
Epoch 0/600 75/293 loss: 1.6290 acc [ 0.398  0.047  0.103]  time 1.71s 
Epoch 0/600 76/293 loss: 1.6296 acc [ 0.406  0.049  0.102]  time 7.45s 
Epoch 0/600 77/293 loss: 1.6269 acc [ 0.403  0.048  0.100]  time 0.02s 
Epoch 0/600 78/293 loss: 1.6236 acc [ 0.404  0.048  0.100]  time 0.02s 
Epoch 0/600 79/293 loss: 1.6207 acc [ 0.412  0.047  0.100]  time 1.18s 
Epoch 0/600 80/293 loss: 1.6245 acc [ 0.417  0.049  0.099]  time 0.02s 
Epoch 0/600 81/293 loss: 1.6256 acc [ 0.414  0.048  0.098]  time 0.02s 
Epoch 0/600 82/293 loss: 1.6241 acc [ 0.414  0.048  0.098]  time 0.02s 
Epoch 0/600 83/293 loss: 1.6226 acc [ 0.414  0.048  0.098]  time 0.02s 
Epoch 0/600 84/293 loss: 1.6191 acc [ 0.418  0.048  0.096]  time 1.57s 
Epoch 0/600 85/293 loss: 1.6185 acc [ 0.415  0.052  0.096]  time 0.02s 
Epoch 0/600 86/293 loss: 1.6161 acc [ 0.421  0.052  0.094]  time 2.87s 
Epoch 0/600 87/293 loss: 1.6140 acc [ 0.421  0.052  0.093]  time 0.02s 
Epoch 0/600 88/293 loss: 1.6128 acc [ 0.421  0.052  0.093]  time 0.02s 
Epoch 0/600 89/293 loss: 1.6115 acc [ 0.421  0.052  0.093]  time 0.02s 
Epoch 0/600 90/293 loss: 1.6103 acc [ 0.422  0.053  0.094]  time 0.02s 
Epoch 0/600 91/293 loss: 1.6095 acc [ 0.421  0.054  0.095]  time 0.02s 
Epoch 0/600 92/293 loss: 1.6083 acc [ 0.421  0.054  0.095]  time 4.42s 
Epoch 0/600 93/293 loss: 1.6063 acc [ 0.420  0.054  0.095]  time 0.02s 
Epoch 0/600 94/293 loss: 1.6046 acc [ 0.416  0.054  0.095]  time 0.02s 
Epoch 0/600 95/293 loss: 1.6026 acc [ 0.415  0.054  0.095]  time 0.02s 
Epoch 0/600 96/293 loss: 1.6002 acc [ 0.415  0.053  0.093]  time 0.02s 
Epoch 0/600 97/293 loss: 1.6035 acc [ 0.421  0.054  0.095]  time 0.02s 
Epoch 0/600 98/293 loss: 1.6024 acc [ 0.421  0.054  0.095]  time 0.02s 
Epoch 0/600 99/293 loss: 1.6058 acc [ 0.424  0.057  0.094]  time 0.02s 
Epoch 0/600 100/293 loss: 1.6047 acc [ 0.424  0.057  0.094]  time 0.67s 
Epoch 0/600 101/293 loss: 1.6038 acc [ 0.420  0.056  0.094]  time 0.04s 
Epoch 0/600 102/293 loss: 1.6028 acc [ 0.420  0.056  0.094]  time 0.03s 
Epoch 0/600 103/293 loss: 1.6125 acc [ 0.425  0.057  0.094]  time 0.02s 
Epoch 0/600 104/293 loss: 1.6140 acc [ 0.427  0.058  0.094]  time 0.02s 
Epoch 0/600 105/293 loss: 1.6129 acc [ 0.427  0.058  0.094]  time 0.02s 
Epoch 0/600 106/293 loss: 1.6107 acc [ 0.425  0.059  0.093]  time 0.02s 
Epoch 0/600 107/293 loss: 1.6089 acc [ 0.424  0.058  0.093]  time 0.02s 
Epoch 0/600 108/293 loss: 1.6070 acc [ 0.422  0.059  0.092]  time 1.23s 
Epoch 0/600 109/293 loss: 1.6060 acc [ 0.422  0.059  0.092]  time 5.87s 
Epoch 0/600 110/293 loss: 1.6047 acc [ 0.424  0.060  0.093]  time 0.02s 
Epoch 0/600 111/293 loss: 1.6025 acc [ 0.425  0.059  0.092]  time 0.02s 
Epoch 0/600 112/293 loss: 1.6016 acc [ 0.425  0.059  0.092]  time 0.02s 
Epoch 0/600 113/293 loss: 1.5998 acc [ 0.428  0.059  0.092]  time 0.02s 
Epoch 0/600 114/293 loss: 1.5977 acc [ 0.426  0.060  0.091]  time 0.02s 
Epoch 0/600 115/293 loss: 1.5956 acc [ 0.425  0.060  0.090]  time 0.02s 
Epoch 0/600 116/293 loss: 1.5942 acc [ 0.426  0.059  0.090]  time 0.02s 
Epoch 0/600 117/293 loss: 1.5927 acc [ 0.423  0.058  0.089]  time 1.81s 
Epoch 0/600 118/293 loss: 1.5903 acc [ 0.426  0.058  0.088]  time 0.02s 
Epoch 0/600 119/293 loss: 1.5895 acc [ 0.426  0.058  0.088]  time 0.02s 
Epoch 0/600 120/293 loss: 1.5887 acc [ 0.421  0.058  0.088]  time 0.02s 
Epoch 0/600 121/293 loss: 1.5917 acc [ 0.419  0.057  0.087]  time 0.03s 
Epoch 0/600 122/293 loss: 1.5907 acc [ 0.423  0.056  0.086]  time 0.32s 
Epoch 0/600 123/293 loss: 1.5899 acc [ 0.419  0.056  0.086]  time 0.02s 
Epoch 0/600 124/293 loss: 1.5881 acc [ 0.418  0.056  0.085]  time 0.02s 
Epoch 0/600 125/293 loss: 1.5868 acc [ 0.416  0.055  0.083]  time 1.02s 
Epoch 0/600 126/293 loss: 1.5848 acc [ 0.417  0.055  0.083]  time 0.02s 
Epoch 0/600 127/293 loss: 1.5831 acc [ 0.416  0.056  0.084]  time 0.02s 
Epoch 0/600 128/293 loss: 1.5837 acc [ 0.420  0.056  0.083]  time 0.48s 
Epoch 0/600 129/293 loss: 1.5830 acc [ 0.420  0.056  0.083]  time 0.03s 
Epoch 0/600 130/293 loss: 1.5824 acc [ 0.420  0.056  0.083]  time 0.03s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 1
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 8
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1700GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 1 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/293 loss: 1.5000 acc [ 0.000  0.000  0.000]  time 12.47s 
Epoch 0/600 1/293 loss: 1.5000 acc [ 0.000  0.000  0.000]  time 0.03s 
Epoch 0/600 2/293 loss: 1.5772 acc [ 0.094  0.020  0.069]  time 0.05s 
Epoch 0/600 3/293 loss: 1.5919 acc [ 0.096  0.012  0.039]  time 0.02s 
Epoch 0/600 4/293 loss: 1.5735 acc [ 0.096  0.012  0.039]  time 0.02s 
Epoch 0/600 5/293 loss: 1.5737 acc [ 0.087  0.011  0.029]  time 0.02s 
Epoch 0/600 6/293 loss: 1.6065 acc [ 0.094  0.012  0.024]  time 0.02s 
Epoch 0/600 7/293 loss: 1.5932 acc [ 0.094  0.012  0.024]  time 0.03s 
Epoch 0/600 8/293 loss: 1.5952 acc [ 0.099  0.014  0.033]  time 0.02s 
Epoch 0/600 9/293 loss: 1.5857 acc [ 0.099  0.014  0.033]  time 0.02s 
Epoch 0/600 10/293 loss: 1.6513 acc [ 0.122  0.022  0.080]  time 1.36s 
Epoch 0/600 11/293 loss: 1.6675 acc [ 0.138  0.020  0.071]  time 0.02s 
Epoch 0/600 12/293 loss: 1.6548 acc [ 0.135  0.018  0.066]  time 0.02s 
Epoch 0/600 13/293 loss: 1.6504 acc [ 0.139  0.017  0.067]  time 0.02s 
Epoch 0/600 14/293 loss: 1.6403 acc [ 0.125  0.017  0.067]  time 0.03s 
Epoch 0/600 15/293 loss: 1.6356 acc [ 0.140  0.017  0.062]  time 0.02s 
Epoch 0/600 16/293 loss: 1.6327 acc [ 0.147  0.019  0.068]  time 0.92s 
Epoch 0/600 17/293 loss: 1.6239 acc [ 0.153  0.017  0.068]  time 0.04s 
Epoch 0/600 18/293 loss: 1.6198 acc [ 0.163  0.017  0.065]  time 0.13s 
Epoch 0/600 19/293 loss: 1.6142 acc [ 0.169  0.017  0.065]  time 0.03s 
Epoch 0/600 20/293 loss: 1.6586 acc [ 0.181  0.017  0.095]  time 1.10s 
Epoch 0/600 21/293 loss: 1.6514 acc [ 0.181  0.017  0.095]  time 0.02s 
Epoch 0/600 22/293 loss: 1.6464 acc [ 0.177  0.017  0.095]  time 0.02s 
Epoch 0/600 23/293 loss: 1.6403 acc [ 0.177  0.017  0.095]  time 0.87s 
Epoch 0/600 24/293 loss: 1.6330 acc [ 0.176  0.016  0.089]  time 0.02s 
Epoch 0/600 25/293 loss: 1.6268 acc [ 0.178  0.016  0.089]  time 0.02s 
Epoch 0/600 26/293 loss: 1.6577 acc [ 0.187  0.016  0.110]  time 0.07s 
Epoch 0/600 27/293 loss: 1.6509 acc [ 0.188  0.016  0.105]  time 0.03s 
Epoch 0/600 28/293 loss: 1.6457 acc [ 0.188  0.016  0.105]  time 3.85s 
Epoch 0/600 29/293 loss: 1.6572 acc [ 0.209  0.016  0.105]  time 2.21s 
Epoch 0/600 30/293 loss: 1.6642 acc [ 0.223  0.018  0.105]  time 2.25s 
Epoch 0/600 31/293 loss: 1.6590 acc [ 0.223  0.018  0.105]  time 0.02s 
Epoch 0/600 32/293 loss: 1.6518 acc [ 0.233  0.018  0.100]  time 0.03s 
Epoch 0/600 33/293 loss: 1.6463 acc [ 0.236  0.017  0.100]  time 0.03s 
Epoch 0/600 34/293 loss: 1.6421 acc [ 0.236  0.017  0.100]  time 0.02s 
Epoch 0/600 35/293 loss: 1.6368 acc [ 0.239  0.016  0.095]  time 0.02s 
Epoch 0/600 36/293 loss: 1.6395 acc [ 0.251  0.017  0.095]  time 2.34s 
Epoch 0/600 37/293 loss: 1.6378 acc [ 0.254  0.017  0.093]  time 0.02s 
Epoch 0/600 38/293 loss: 1.6881 acc [ 0.268  0.018  0.104]  time 4.76s 
Epoch 0/600 39/293 loss: 1.6834 acc [ 0.268  0.018  0.104]  time 0.02s 
Epoch 0/600 40/293 loss: 1.6793 acc [ 0.277  0.018  0.104]  time 0.02s 
Epoch 0/600 41/293 loss: 1.6719 acc [ 0.277  0.018  0.104]  time 0.02s 
Epoch 0/600 42/293 loss: 1.6673 acc [ 0.272  0.017  0.104]  time 0.02s 
Epoch 0/600 43/293 loss: 1.6615 acc [ 0.270  0.017  0.099]  time 0.02s 
Epoch 0/600 44/293 loss: 1.6569 acc [ 0.271  0.017  0.099]  time 0.02s 
Epoch 0/600 45/293 loss: 1.6532 acc [ 0.267  0.017  0.097]  time 0.02s 
Epoch 0/600 46/293 loss: 1.6498 acc [ 0.265  0.017  0.095]  time 1.09s 
Epoch 0/600 47/293 loss: 1.6463 acc [ 0.278  0.017  0.095]  time 2.62s 
Epoch 0/600 48/293 loss: 1.6433 acc [ 0.278  0.017  0.095]  time 0.02s 
Epoch 0/600 49/293 loss: 1.6378 acc [ 0.282  0.017  0.095]  time 0.03s 
Epoch 0/600 50/293 loss: 1.6366 acc [ 0.288  0.017  0.094]  time 0.02s 
Epoch 0/600 51/293 loss: 1.6350 acc [ 0.294  0.018  0.091]  time 0.02s 
Epoch 0/600 52/293 loss: 1.6558 acc [ 0.302  0.019  0.095]  time 0.02s 
Epoch 0/600 53/293 loss: 1.6506 acc [ 0.302  0.019  0.092]  time 0.02s 
Epoch 0/600 54/293 loss: 1.6479 acc [ 0.302  0.019  0.092]  time 0.02s 
Epoch 0/600 55/293 loss: 1.6449 acc [ 0.301  0.020  0.090]  time 1.42s 
Epoch 0/600 56/293 loss: 1.6415 acc [ 0.299  0.020  0.090]  time 0.03s 
Epoch 0/600 57/293 loss: 1.6391 acc [ 0.301  0.020  0.087]  time 0.03s 
Epoch 0/600 58/293 loss: 1.6355 acc [ 0.302  0.020  0.087]  time 0.02s 
Epoch 0/600 59/293 loss: 1.6311 acc [ 0.305  0.020  0.086]  time 0.03s 
Epoch 0/600 60/293 loss: 1.6284 acc [ 0.300  0.020  0.086]  time 0.02s 
Epoch 0/600 61/293 loss: 1.6233 acc [ 0.303  0.019  0.083]  time 0.02s 
Epoch 0/600 62/293 loss: 1.6213 acc [ 0.297  0.019  0.083]  time 0.02s 
Epoch 0/600 63/293 loss: 1.6194 acc [ 0.297  0.019  0.083]  time 3.53s 
Epoch 0/600 64/293 loss: 1.6160 acc [ 0.304  0.019  0.083]  time 0.02s 
Epoch 0/600 65/293 loss: 1.6143 acc [ 0.304  0.019  0.083]  time 0.02s 
Epoch 0/600 66/293 loss: 1.6116 acc [ 0.305  0.019  0.083]  time 0.02s 
Epoch 0/600 67/293 loss: 1.6088 acc [ 0.302  0.019  0.081]  time 0.02s 
Epoch 0/600 68/293 loss: 1.6150 acc [ 0.306  0.020  0.080]  time 0.02s 
Epoch 0/600 69/293 loss: 1.6116 acc [ 0.305  0.020  0.080]  time 0.02s 
Epoch 0/600 70/293 loss: 1.6100 acc [ 0.305  0.020  0.080]  time 0.02s 
Epoch 0/600 71/293 loss: 1.6085 acc [ 0.305  0.020  0.080]  time 1.18s 
Epoch 0/600 72/293 loss: 1.6063 acc [ 0.302  0.019  0.078]  time 0.02s 
Epoch 0/600 73/293 loss: 1.6036 acc [ 0.299  0.019  0.076]  time 0.02s 
Epoch 0/600 74/293 loss: 1.6012 acc [ 0.297  0.019  0.076]  time 0.02s 
Epoch 0/600 75/293 loss: 1.6001 acc [ 0.295  0.019  0.075]  time 0.02s 
Epoch 0/600 76/293 loss: 1.6240 acc [ 0.302  0.020  0.075]  time 2.85s 
Epoch 0/600 77/293 loss: 1.6204 acc [ 0.306  0.020  0.075]  time 0.02s 
Epoch 0/600 78/293 loss: 1.6180 acc [ 0.309  0.020  0.073]  time 0.02s 
Epoch 0/600 79/293 loss: 1.6205 acc [ 0.313  0.020  0.072]  time 0.02s 
Epoch 0/600 80/293 loss: 1.6190 acc [ 0.313  0.020  0.072]  time 0.02s 
Epoch 0/600 81/293 loss: 1.6162 acc [ 0.313  0.020  0.072]  time 0.02s 
Epoch 0/600 82/293 loss: 1.6148 acc [ 0.313  0.020  0.072]  time 0.02s 
Epoch 0/600 83/293 loss: 1.6151 acc [ 0.315  0.021  0.070]  time 0.97s 
Epoch 0/600 84/293 loss: 1.6140 acc [ 0.319  0.021  0.069]  time 0.27s 
Epoch 0/600 85/293 loss: 1.6126 acc [ 0.319  0.021  0.069]  time 0.02s 
Epoch 0/600 86/293 loss: 1.6113 acc [ 0.319  0.021  0.069]  time 0.02s 
Epoch 0/600 87/293 loss: 1.6184 acc [ 0.326  0.021  0.070]  time 6.86s 
Epoch 0/600 88/293 loss: 1.6350 acc [ 0.330  0.022  0.074]  time 0.02s 
Epoch 0/600 89/293 loss: 1.6319 acc [ 0.334  0.022  0.073]  time 1.24s 
Epoch 0/600 90/293 loss: 1.6305 acc [ 0.334  0.022  0.073]  time 0.02s 
Epoch 0/600 91/293 loss: 1.6283 acc [ 0.337  0.022  0.073]  time 0.02s 
Epoch 0/600 92/293 loss: 1.6257 acc [ 0.338  0.023  0.071]  time 0.02s 
Epoch 0/600 93/293 loss: 1.6346 acc [ 0.342  0.023  0.075]  time 0.02s 
Epoch 0/600 94/293 loss: 1.6322 acc [ 0.341  0.023  0.073]  time 0.02s 
Epoch 0/600 95/293 loss: 1.6293 acc [ 0.346  0.023  0.073]  time 2.32s 
Epoch 0/600 96/293 loss: 1.6271 acc [ 0.350  0.023  0.073]  time 0.02s 
Epoch 0/600 97/293 loss: 1.6256 acc [ 0.350  0.024  0.072]  time 1.31s 
Epoch 0/600 98/293 loss: 1.6297 acc [ 0.356  0.025  0.072]  time 0.02s 
Epoch 0/600 99/293 loss: 1.6280 acc [ 0.353  0.025  0.071]  time 0.02s 
Epoch 0/600 100/293 loss: 1.6255 acc [ 0.351  0.025  0.071]  time 0.02s 
Epoch 0/600 101/293 loss: 1.6243 acc [ 0.351  0.025  0.071]  time 0.65s 
Epoch 0/600 102/293 loss: 1.6226 acc [ 0.348  0.025  0.070]  time 0.02s 
Epoch 0/600 103/293 loss: 1.6226 acc [ 0.353  0.026  0.069]  time 9.02s 
Epoch 0/600 104/293 loss: 1.6327 acc [ 0.358  0.027  0.069]  time 0.02s 
Epoch 0/600 105/293 loss: 1.6315 acc [ 0.358  0.027  0.069]  time 0.02s 
Epoch 0/600 106/293 loss: 1.6298 acc [ 0.356  0.027  0.068]  time 0.02s 
Epoch 0/600 107/293 loss: 1.6394 acc [ 0.362  0.029  0.068]  time 0.02s 
Epoch 0/600 108/293 loss: 1.6381 acc [ 0.362  0.029  0.068]  time 0.02s 
Epoch 0/600 109/293 loss: 1.6369 acc [ 0.362  0.029  0.068]  time 0.02s 
Epoch 0/600 110/293 loss: 1.6344 acc [ 0.361  0.029  0.068]  time 0.02s 
Epoch 0/600 111/293 loss: 1.6380 acc [ 0.362  0.029  0.067]  time 0.84s 
Epoch 0/600 112/293 loss: 1.6360 acc [ 0.363  0.030  0.066]  time 0.02s 
Epoch 0/600 113/293 loss: 1.6360 acc [ 0.362  0.030  0.065]  time 0.02s 
Epoch 0/600 114/293 loss: 1.6375 acc [ 0.364  0.031  0.066]  time 0.03s 
Epoch 0/600 115/293 loss: 1.6354 acc [ 0.363  0.031  0.065]  time 0.03s 
Epoch 0/600 116/293 loss: 1.6342 acc [ 0.363  0.031  0.065]  time 0.02s 
Epoch 0/600 117/293 loss: 1.6340 acc [ 0.368  0.031  0.064]  time 0.02s 
Epoch 0/600 118/293 loss: 1.6318 acc [ 0.368  0.032  0.063]  time 0.02s 
Epoch 0/600 119/293 loss: 1.6307 acc [ 0.368  0.032  0.063]  time 2.92s 
Epoch 0/600 120/293 loss: 1.6296 acc [ 0.364  0.031  0.062]  time 0.02s 
Epoch 0/600 121/293 loss: 1.6276 acc [ 0.364  0.032  0.061]  time 0.02s 
Epoch 0/600 122/293 loss: 1.6262 acc [ 0.362  0.031  0.061]  time 0.02s 
Epoch 0/600 123/293 loss: 1.6252 acc [ 0.362  0.031  0.061]  time 0.02s 
Epoch 0/600 124/293 loss: 1.6232 acc [ 0.361  0.032  0.060]  time 0.02s 
Epoch 0/600 125/293 loss: 1.6214 acc [ 0.361  0.032  0.060]  time 0.02s 
Epoch 0/600 126/293 loss: 1.6194 acc [ 0.361  0.032  0.059]  time 0.02s 
Epoch 0/600 127/293 loss: 1.6173 acc [ 0.362  0.032  0.059]  time 1.20s 
Epoch 0/600 128/293 loss: 1.6151 acc [ 0.361  0.032  0.059]  time 0.02s 
Epoch 0/600 129/293 loss: 1.6136 acc [ 0.360  0.032  0.059]  time 0.02s 
Epoch 0/600 130/293 loss: 1.6155 acc [ 0.361  0.032  0.061]  time 0.02s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 1
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 8
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1713GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 1 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/293 loss: 1.5000 acc [ 0.000  0.000  0.000]  time 6.37s 
Epoch 0/600 1/293 loss: 1.5811 acc [ 0.125  0.000  0.000]  time 0.05s 
Epoch 0/600 2/293 loss: 1.6742 acc [ 0.135  0.159  0.161]  time 0.02s 
Epoch 0/600 3/293 loss: 1.6306 acc [ 0.135  0.159  0.161]  time 0.02s 
Epoch 0/600 4/293 loss: 1.6045 acc [ 0.135  0.159  0.161]  time 0.02s 
Epoch 0/600 5/293 loss: 1.6949 acc [ 0.175  0.129  0.138]  time 0.02s 
Epoch 0/600 6/293 loss: 1.7527 acc [ 0.199  0.099  0.106]  time 1.34s 
Epoch 0/600 7/293 loss: 1.8233 acc [ 0.201  0.186  0.190]  time 0.02s 
Epoch 0/600 8/293 loss: 1.7926 acc [ 0.189  0.186  0.190]  time 0.03s 
Epoch 0/600 9/293 loss: 1.7634 acc [ 0.189  0.186  0.190]  time 3.31s 
Epoch 0/600 10/293 loss: 1.7695 acc [ 0.196  0.200  0.205]  time 0.02s 
Epoch 0/600 11/293 loss: 1.7549 acc [ 0.193  0.200  0.205]  time 0.02s 
Epoch 0/600 12/293 loss: 1.7400 acc [ 0.191  0.181  0.186]  time 0.02s 
Epoch 0/600 13/293 loss: 1.7310 acc [ 0.189  0.173  0.181]  time 0.02s 
Epoch 0/600 14/293 loss: 1.7156 acc [ 0.189  0.173  0.181]  time 0.02s 
Epoch 0/600 15/293 loss: 1.7021 acc [ 0.189  0.173  0.181]  time 0.03s 
Epoch 0/600 16/293 loss: 1.6902 acc [ 0.189  0.173  0.181]  time 0.03s 
Epoch 0/600 17/293 loss: 1.6806 acc [ 0.196  0.153  0.160]  time 1.92s 
Epoch 0/600 18/293 loss: 1.6709 acc [ 0.194  0.140  0.145]  time 0.02s 
Epoch 0/600 19/293 loss: 1.6624 acc [ 0.194  0.140  0.145]  time 0.02s 
Epoch 0/600 20/293 loss: 1.6563 acc [ 0.197  0.131  0.135]  time 0.03s 
Epoch 0/600 21/293 loss: 1.6492 acc [ 0.197  0.131  0.135]  time 3.81s 
Epoch 0/600 22/293 loss: 1.6448 acc [ 0.203  0.131  0.135]  time 0.02s 
Epoch 0/600 23/293 loss: 1.6375 acc [ 0.211  0.131  0.135]  time 0.02s 
Epoch 0/600 24/293 loss: 1.6311 acc [ 0.213  0.131  0.135]  time 0.02s 
Epoch 0/600 25/293 loss: 1.6295 acc [ 0.229  0.119  0.135]  time 1.84s 
Epoch 0/600 26/293 loss: 1.7215 acc [ 0.254  0.160  0.193]  time 0.02s 
Epoch 0/600 27/293 loss: 1.7192 acc [ 0.267  0.160  0.193]  time 0.46s 
Epoch 0/600 28/293 loss: 1.7129 acc [ 0.264  0.153  0.183]  time 0.02s 
Epoch 0/600 29/293 loss: 1.7058 acc [ 0.264  0.153  0.183]  time 0.02s 
Epoch 0/600 30/293 loss: 1.6993 acc [ 0.254  0.153  0.183]  time 0.02s 
Epoch 0/600 31/293 loss: 1.6925 acc [ 0.250  0.153  0.183]  time 0.03s 
Epoch 0/600 32/293 loss: 1.6867 acc [ 0.250  0.153  0.183]  time 0.03s 
Epoch 0/600 33/293 loss: 1.6858 acc [ 0.257  0.152  0.176]  time 1.63s 
Epoch 0/600 34/293 loss: 1.6985 acc [ 0.269  0.163  0.185]  time 0.02s 
Epoch 0/600 35/293 loss: 1.6930 acc [ 0.269  0.163  0.185]  time 0.51s 
Epoch 0/600 36/293 loss: 1.7091 acc [ 0.282  0.177  0.203]  time 0.02s 
Epoch 0/600 37/293 loss: 1.7066 acc [ 0.285  0.173  0.198]  time 0.03s 
Epoch 0/600 38/293 loss: 1.7005 acc [ 0.282  0.165  0.187]  time 0.03s 
Epoch 0/600 39/293 loss: 1.6942 acc [ 0.284  0.156  0.177]  time 0.02s 
Epoch 0/600 40/293 loss: 1.6927 acc [ 0.289  0.156  0.176]  time 0.02s 
Epoch 0/600 41/293 loss: 1.7026 acc [ 0.295  0.163  0.189]  time 7.12s 
Epoch 0/600 42/293 loss: 1.6979 acc [ 0.295  0.163  0.189]  time 0.02s 
Epoch 0/600 43/293 loss: 1.6989 acc [ 0.307  0.161  0.185]  time 3.99s 
Epoch 0/600 44/293 loss: 1.6945 acc [ 0.307  0.161  0.185]  time 0.02s 
Epoch 0/600 45/293 loss: 1.6895 acc [ 0.305  0.161  0.185]  time 0.02s 
Epoch 0/600 46/293 loss: 1.6839 acc [ 0.303  0.155  0.177]  time 0.02s 
Epoch 0/600 47/293 loss: 1.6798 acc [ 0.309  0.152  0.170]  time 0.02s 
Epoch 0/600 48/293 loss: 1.6785 acc [ 0.317  0.149  0.167]  time 0.02s 
Epoch 0/600 49/293 loss: 1.6742 acc [ 0.322  0.146  0.167]  time 0.02s 
Epoch 0/600 50/293 loss: 1.6706 acc [ 0.322  0.143  0.163]  time 0.02s 
Epoch 0/600 51/293 loss: 1.6673 acc [ 0.322  0.143  0.163]  time 2.98s 
Epoch 0/600 52/293 loss: 1.6665 acc [ 0.326  0.139  0.163]  time 0.03s 
Epoch 0/600 53/293 loss: 1.6669 acc [ 0.333  0.138  0.159]  time 0.02s 
Epoch 0/600 54/293 loss: 1.6639 acc [ 0.333  0.138  0.159]  time 0.02s 
Epoch 0/600 55/293 loss: 1.6738 acc [ 0.338  0.143  0.164]  time 0.02s 
Epoch 0/600 56/293 loss: 1.6708 acc [ 0.338  0.143  0.164]  time 0.02s 
Epoch 0/600 57/293 loss: 1.6670 acc [ 0.336  0.143  0.164]  time 0.02s 
Epoch 0/600 58/293 loss: 1.6625 acc [ 0.335  0.138  0.164]  time 0.02s 
Epoch 0/600 59/293 loss: 1.6586 acc [ 0.333  0.134  0.164]  time 1.42s 
Epoch 0/600 60/293 loss: 1.6647 acc [ 0.335  0.139  0.170]  time 0.02s 
Epoch 0/600 61/293 loss: 1.6620 acc [ 0.335  0.139  0.170]  time 0.02s 
Epoch 0/600 62/293 loss: 1.6589 acc [ 0.330  0.139  0.170]  time 3.64s 
Epoch 0/600 63/293 loss: 1.6550 acc [ 0.331  0.139  0.170]  time 0.02s 
Epoch 0/600 64/293 loss: 1.6526 acc [ 0.331  0.139  0.170]  time 0.02s 
Epoch 0/600 65/293 loss: 1.6505 acc [ 0.326  0.137  0.166]  time 0.02s 
Epoch 0/600 66/293 loss: 1.6483 acc [ 0.326  0.137  0.166]  time 0.02s 
Epoch 0/600 67/293 loss: 1.6457 acc [ 0.328  0.135  0.162]  time 0.02s 
Epoch 0/600 68/293 loss: 1.6437 acc [ 0.327  0.135  0.161]  time 0.02s 
Epoch 0/600 69/293 loss: 1.6426 acc [ 0.328  0.134  0.161]  time 0.02s 
Epoch 0/600 70/293 loss: 1.6406 acc [ 0.328  0.134  0.161]  time 1.10s 
Epoch 0/600 71/293 loss: 1.6375 acc [ 0.329  0.134  0.161]  time 0.02s 
Epoch 0/600 72/293 loss: 1.6340 acc [ 0.331  0.134  0.161]  time 0.02s 
Epoch 0/600 73/293 loss: 1.6309 acc [ 0.331  0.134  0.161]  time 0.02s 
Epoch 0/600 74/293 loss: 1.6277 acc [ 0.331  0.131  0.157]  time 0.03s 
Epoch 0/600 75/293 loss: 1.6532 acc [ 0.337  0.142  0.176]  time 1.33s 
Epoch 0/600 76/293 loss: 1.6513 acc [ 0.337  0.142  0.176]  time 0.03s 
Epoch 0/600 77/293 loss: 1.6479 acc [ 0.335  0.138  0.171]  time 0.02s 
Epoch 0/600 78/293 loss: 1.6460 acc [ 0.335  0.138  0.171]  time 6.02s 
Epoch 0/600 79/293 loss: 1.6430 acc [ 0.335  0.136  0.167]  time 0.02s 
Epoch 0/600 80/293 loss: 1.6412 acc [ 0.335  0.136  0.167]  time 0.02s 
Epoch 0/600 81/293 loss: 1.6377 acc [ 0.338  0.133  0.163]  time 0.02s 
Epoch 0/600 82/293 loss: 1.6361 acc [ 0.338  0.133  0.163]  time 0.02s 
Epoch 0/600 83/293 loss: 1.6327 acc [ 0.343  0.130  0.163]  time 0.81s 
Epoch 0/600 84/293 loss: 1.6311 acc [ 0.338  0.128  0.160]  time 0.03s 
Epoch 0/600 85/293 loss: 1.6284 acc [ 0.339  0.127  0.157]  time 0.03s 
Epoch 0/600 86/293 loss: 1.6278 acc [ 0.340  0.126  0.156]  time 1.65s 
Epoch 0/600 87/293 loss: 1.6253 acc [ 0.338  0.126  0.156]  time 0.03s 
Epoch 0/600 88/293 loss: 1.6231 acc [ 0.338  0.124  0.152]  time 0.02s 
Epoch 0/600 89/293 loss: 1.6202 acc [ 0.339  0.124  0.152]  time 0.02s 
Epoch 0/600 90/293 loss: 1.6172 acc [ 0.338  0.124  0.152]  time 0.02s 
Epoch 0/600 91/293 loss: 1.6161 acc [ 0.339  0.123  0.149]  time 0.02s 
Epoch 0/600 92/293 loss: 1.6121 acc [ 0.343  0.121  0.146]  time 0.02s 
Epoch 0/600 93/293 loss: 1.6109 acc [ 0.343  0.121  0.146]  time 0.02s 
Epoch 0/600 94/293 loss: 1.6074 acc [ 0.344  0.121  0.146]  time 3.40s 
Epoch 0/600 95/293 loss: 1.6063 acc [ 0.344  0.121  0.146]  time 5.60s 
Epoch 0/600 96/293 loss: 1.6067 acc [ 0.344  0.122  0.147]  time 0.02s 
Epoch 0/600 97/293 loss: 1.6041 acc [ 0.343  0.120  0.147]  time 0.02s 
Epoch 0/600 98/293 loss: 1.6026 acc [ 0.344  0.119  0.144]  time 0.02s 
Epoch 0/600 99/293 loss: 1.6014 acc [ 0.341  0.118  0.143]  time 0.02s 
Epoch 0/600 100/293 loss: 1.5995 acc [ 0.340  0.116  0.140]  time 0.02s 
Epoch 0/600 101/293 loss: 1.6007 acc [ 0.339  0.117  0.139]  time 0.02s 
Epoch 0/600 102/293 loss: 1.5989 acc [ 0.342  0.115  0.136]  time 0.02s 
Epoch 0/600 103/293 loss: 1.5970 acc [ 0.340  0.113  0.134]  time 1.53s 
Epoch 0/600 104/293 loss: 1.5961 acc [ 0.340  0.113  0.134]  time 1.54s 
Epoch 0/600 105/293 loss: 1.5952 acc [ 0.340  0.113  0.134]  time 0.02s 
Epoch 0/600 106/293 loss: 1.6026 acc [ 0.343  0.118  0.138]  time 0.02s 
Epoch 0/600 107/293 loss: 1.6017 acc [ 0.343  0.118  0.138]  time 0.02s 
Epoch 0/600 108/293 loss: 1.6005 acc [ 0.345  0.118  0.138]  time 0.02s 
Epoch 0/600 109/293 loss: 1.5996 acc [ 0.345  0.118  0.138]  time 0.02s 
Epoch 0/600 110/293 loss: 1.5974 acc [ 0.346  0.117  0.136]  time 0.02s 
Epoch 0/600 111/293 loss: 1.5957 acc [ 0.346  0.116  0.135]  time 3.13s 
Epoch 0/600 112/293 loss: 1.5989 acc [ 0.350  0.115  0.133]  time 2.82s 
Epoch 0/600 113/293 loss: 1.5980 acc [ 0.350  0.115  0.133]  time 0.02s 
Epoch 0/600 114/293 loss: 1.5951 acc [ 0.352  0.113  0.131]  time 0.02s 
Epoch 0/600 115/293 loss: 1.5959 acc [ 0.358  0.112  0.129]  time 0.02s 
Epoch 0/600 116/293 loss: 1.5951 acc [ 0.358  0.112  0.129]  time 0.03s 
Epoch 0/600 117/293 loss: 1.5944 acc [ 0.362  0.111  0.128]  time 0.03s 
Epoch 0/600 118/293 loss: 1.5920 acc [ 0.362  0.111  0.128]  time 0.02s 
Epoch 0/600 119/293 loss: 1.5901 acc [ 0.363  0.111  0.128]  time 0.02s 
Epoch 0/600 120/293 loss: 1.5880 acc [ 0.363  0.111  0.128]  time 1.59s 
Epoch 0/600 121/293 loss: 1.5860 acc [ 0.362  0.109  0.125]  time 0.02s 
Epoch 0/600 122/293 loss: 1.5949 acc [ 0.365  0.113  0.128]  time 0.02s 
Epoch 0/600 123/293 loss: 1.5941 acc [ 0.365  0.113  0.128]  time 0.02s 
Epoch 0/600 124/293 loss: 1.5977 acc [ 0.370  0.114  0.130]  time 0.02s 
Epoch 0/600 125/293 loss: 1.6035 acc [ 0.374  0.118  0.134]  time 0.02s 
Epoch 0/600 126/293 loss: 1.6019 acc [ 0.373  0.117  0.133]  time 0.03s 
Epoch 0/600 127/293 loss: 1.6004 acc [ 0.372  0.116  0.132]  time 0.03s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 1
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 8
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1724GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 1 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/293 loss: 1.5000 acc [ 0.000  0.000  0.000]  time 10.03s 
Epoch 0/600 1/293 loss: 1.5053 acc [ 0.222  0.027  0.072]  time 0.05s 
Epoch 0/600 2/293 loss: 1.5827 acc [ 0.358  0.027  0.072]  time 0.42s 
Epoch 0/600 3/293 loss: 1.6140 acc [ 0.406  0.026  0.052]  time 0.02s 
Epoch 0/600 4/293 loss: 1.6094 acc [ 0.414  0.017  0.052]  time 0.02s 
Epoch 0/600 5/293 loss: 1.5934 acc [ 0.380  0.013  0.052]  time 0.02s 
Epoch 0/600 6/293 loss: 1.6172 acc [ 0.423  0.013  0.052]  time 0.02s 
Epoch 0/600 7/293 loss: 1.6958 acc [ 0.432  0.071  0.127]  time 0.02s 
Epoch 0/600 8/293 loss: 1.6740 acc [ 0.432  0.071  0.127]  time 0.16s 
Epoch 0/600 9/293 loss: 1.6566 acc [ 0.432  0.071  0.127]  time 6.11s 
Epoch 0/600 10/293 loss: 1.6436 acc [ 0.420  0.063  0.106]  time 0.02s 
Epoch 0/600 11/293 loss: 1.6331 acc [ 0.425  0.061  0.101]  time 0.02s 
Epoch 0/600 12/293 loss: 1.6229 acc [ 0.425  0.061  0.101]  time 0.02s 
Epoch 0/600 13/293 loss: 1.6157 acc [ 0.417  0.058  0.100]  time 0.02s 
Epoch 0/600 14/293 loss: 1.6229 acc [ 0.423  0.078  0.102]  time 0.02s 
Epoch 0/600 15/293 loss: 1.6184 acc [ 0.423  0.078  0.102]  time 0.03s 
Epoch 0/600 16/293 loss: 1.6115 acc [ 0.423  0.078  0.102]  time 3.96s 
Epoch 0/600 17/293 loss: 1.6008 acc [ 0.419  0.078  0.102]  time 0.02s 
Epoch 0/600 18/293 loss: 1.6165 acc [ 0.428  0.080  0.116]  time 0.02s 
Epoch 0/600 19/293 loss: 1.6107 acc [ 0.428  0.080  0.116]  time 0.02s 
Epoch 0/600 20/293 loss: 1.6054 acc [ 0.428  0.080  0.116]  time 0.02s 
Epoch 0/600 21/293 loss: 1.6006 acc [ 0.428  0.080  0.116]  time 0.02s 
Epoch 0/600 22/293 loss: 1.6174 acc [ 0.431  0.079  0.130]  time 0.02s 
Epoch 0/600 23/293 loss: 1.6149 acc [ 0.432  0.080  0.127]  time 0.02s 
Epoch 0/600 24/293 loss: 1.6087 acc [ 0.421  0.080  0.127]  time 0.70s 
Epoch 0/600 25/293 loss: 1.6041 acc [ 0.433  0.074  0.116]  time 5.35s 
Epoch 0/600 26/293 loss: 1.6002 acc [ 0.433  0.074  0.116]  time 0.02s 
Epoch 0/600 27/293 loss: 1.5944 acc [ 0.421  0.069  0.106]  time 0.02s 
Epoch 0/600 28/293 loss: 1.5876 acc [ 0.418  0.066  0.099]  time 0.03s 
Epoch 0/600 29/293 loss: 1.5830 acc [ 0.413  0.062  0.099]  time 0.02s 
Epoch 0/600 30/293 loss: 1.5803 acc [ 0.413  0.062  0.099]  time 0.02s 
Epoch 0/600 31/293 loss: 1.5778 acc [ 0.413  0.062  0.099]  time 0.02s 
Epoch 0/600 32/293 loss: 1.5736 acc [ 0.418  0.062  0.099]  time 0.02s 
Epoch 0/600 33/293 loss: 1.5735 acc [ 0.425  0.059  0.099]  time 10.24s 
Epoch 0/600 34/293 loss: 1.5714 acc [ 0.425  0.059  0.099]  time 0.02s 
Epoch 0/600 35/293 loss: 1.5687 acc [ 0.417  0.060  0.096]  time 0.02s 
Epoch 0/600 36/293 loss: 1.5657 acc [ 0.409  0.058  0.093]  time 0.02s 
Epoch 0/600 37/293 loss: 1.5640 acc [ 0.409  0.058  0.093]  time 0.02s 
Epoch 0/600 38/293 loss: 1.5623 acc [ 0.409  0.058  0.093]  time 0.02s 
Epoch 0/600 39/293 loss: 1.5608 acc [ 0.409  0.058  0.093]  time 0.02s 
Epoch 0/600 40/293 loss: 1.5614 acc [ 0.416  0.057  0.095]  time 0.02s 
Epoch 0/600 41/293 loss: 1.5600 acc [ 0.416  0.057  0.095]  time 1.19s 
Epoch 0/600 42/293 loss: 1.5570 acc [ 0.412  0.057  0.095]  time 0.03s 
Epoch 0/600 43/293 loss: 1.5527 acc [ 0.407  0.057  0.095]  time 0.02s 
Epoch 0/600 44/293 loss: 1.5505 acc [ 0.399  0.056  0.091]  time 0.02s 
Epoch 0/600 45/293 loss: 1.5483 acc [ 0.403  0.054  0.089]  time 0.02s 
Epoch 0/600 46/293 loss: 1.5473 acc [ 0.403  0.054  0.089]  time 0.02s 
Epoch 0/600 47/293 loss: 1.5469 acc [ 0.403  0.056  0.088]  time 0.03s 
Epoch 0/600 48/293 loss: 1.5459 acc [ 0.398  0.054  0.087]  time 0.02s 
Epoch 0/600 49/293 loss: 1.5430 acc [ 0.395  0.054  0.087]  time 1.34s 
Epoch 0/600 50/293 loss: 1.5422 acc [ 0.395  0.054  0.087]  time 0.03s 
Epoch 0/600 51/293 loss: 1.5510 acc [ 0.395  0.056  0.089]  time 0.06s 
Epoch 0/600 52/293 loss: 1.5500 acc [ 0.395  0.056  0.089]  time 5.87s 
Epoch 0/600 53/293 loss: 1.5533 acc [ 0.397  0.058  0.091]  time 0.02s 
Epoch 0/600 54/293 loss: 1.5523 acc [ 0.397  0.058  0.091]  time 0.02s 
Epoch 0/600 55/293 loss: 1.5548 acc [ 0.397  0.060  0.093]  time 0.02s 
Epoch 0/600 56/293 loss: 1.5539 acc [ 0.397  0.060  0.093]  time 0.03s 
Epoch 0/600 57/293 loss: 1.5516 acc [ 0.389  0.060  0.093]  time 0.03s 
Epoch 0/600 58/293 loss: 1.5478 acc [ 0.395  0.058  0.092]  time 0.03s 
Epoch 0/600 59/293 loss: 1.5445 acc [ 0.394  0.058  0.092]  time 0.03s 
Epoch 0/600 60/293 loss: 1.5438 acc [ 0.394  0.058  0.092]  time 1.46s 
Epoch 0/600 61/293 loss: 1.5432 acc [ 0.399  0.058  0.093]  time 0.03s 
Epoch 0/600 62/293 loss: 1.5425 acc [ 0.399  0.058  0.093]  time 0.02s 
Epoch 0/600 63/293 loss: 1.5408 acc [ 0.397  0.058  0.090]  time 0.03s 
Epoch 0/600 64/293 loss: 1.5376 acc [ 0.400  0.058  0.090]  time 8.62s 
Epoch 0/600 65/293 loss: 1.5349 acc [ 0.402  0.056  0.087]  time 0.02s 
Epoch 0/600 66/293 loss: 1.5344 acc [ 0.402  0.056  0.087]  time 0.02s 
Epoch 0/600 67/293 loss: 1.5334 acc [ 0.394  0.056  0.087]  time 0.02s 
Epoch 0/600 68/293 loss: 1.5317 acc [ 0.390  0.055  0.085]  time 0.02s 
Epoch 0/600 69/293 loss: 1.5313 acc [ 0.390  0.055  0.085]  time 0.94s 
Epoch 0/600 70/293 loss: 1.5294 acc [ 0.385  0.054  0.085]  time 0.02s 
Epoch 0/600 71/293 loss: 1.5290 acc [ 0.385  0.054  0.085]  time 0.20s 
Epoch 0/600 72/293 loss: 1.5286 acc [ 0.385  0.054  0.085]  time 1.29s 
Epoch 0/600 73/293 loss: 1.5269 acc [ 0.388  0.053  0.085]  time 0.02s 
Epoch 0/600 74/293 loss: 1.5266 acc [ 0.388  0.053  0.085]  time 0.02s 
Epoch 0/600 75/293 loss: 1.5250 acc [ 0.384  0.052  0.082]  time 0.02s 
Epoch 0/600 76/293 loss: 1.5243 acc [ 0.382  0.050  0.082]  time 0.02s 
Epoch 0/600 77/293 loss: 1.5240 acc [ 0.382  0.050  0.082]  time 0.02s 
Epoch 0/600 78/293 loss: 1.5237 acc [ 0.382  0.050  0.082]  time 0.02s 
Epoch 0/600 79/293 loss: 1.5220 acc [ 0.381  0.051  0.081]  time 1.89s 
Epoch 0/600 80/293 loss: 1.5196 acc [ 0.390  0.051  0.081]  time 5.94s 
Epoch 0/600 81/293 loss: 1.5194 acc [ 0.390  0.051  0.081]  time 0.02s 
Epoch 0/600 82/293 loss: 1.5190 acc [ 0.390  0.052  0.081]  time 0.02s 
Epoch 0/600 83/293 loss: 1.5170 acc [ 0.393  0.051  0.081]  time 0.02s 
Epoch 0/600 84/293 loss: 1.5168 acc [ 0.393  0.051  0.081]  time 0.02s 
Epoch 0/600 85/293 loss: 1.5177 acc [ 0.394  0.054  0.080]  time 0.02s 
Epoch 0/600 86/293 loss: 1.5154 acc [ 0.397  0.053  0.081]  time 0.02s 
Epoch 0/600 87/293 loss: 1.5170 acc [ 0.405  0.053  0.080]  time 0.02s 
Epoch 0/600 88/293 loss: 1.5216 acc [ 0.412  0.054  0.081]  time 8.33s 
Epoch 0/600 89/293 loss: 1.5214 acc [ 0.412  0.054  0.081]  time 0.02s 
Epoch 0/600 90/293 loss: 1.5232 acc [ 0.420  0.055  0.080]  time 0.02s 
Epoch 0/600 91/293 loss: 1.5230 acc [ 0.420  0.055  0.080]  time 0.02s 
Epoch 0/600 92/293 loss: 1.5225 acc [ 0.420  0.057  0.079]  time 0.02s 
Epoch 0/600 93/293 loss: 1.5222 acc [ 0.420  0.057  0.079]  time 0.02s 
Epoch 0/600 94/293 loss: 1.5246 acc [ 0.422  0.058  0.082]  time 0.02s 
Epoch 0/600 95/293 loss: 1.5226 acc [ 0.423  0.057  0.082]  time 0.03s 
Epoch 0/600 96/293 loss: 1.5253 acc [ 0.426  0.058  0.083]  time 6.46s 
Epoch 0/600 97/293 loss: 1.5250 acc [ 0.426  0.058  0.083]  time 0.03s 
Epoch 0/600 98/293 loss: 1.5263 acc [ 0.429  0.061  0.082]  time 0.03s 
Epoch 0/600 99/293 loss: 1.5260 acc [ 0.429  0.061  0.082]  time 0.02s 
Epoch 0/600 100/293 loss: 1.5254 acc [ 0.426  0.061  0.082]  time 0.03s 
Epoch 0/600 101/293 loss: 1.5282 acc [ 0.427  0.064  0.084]  time 0.02s 
Epoch 0/600 102/293 loss: 1.5284 acc [ 0.433  0.066  0.085]  time 0.02s 
Epoch 0/600 103/293 loss: 1.5281 acc [ 0.433  0.066  0.085]  time 0.02s 
Epoch 0/600 104/293 loss: 1.5415 acc [ 0.437  0.068  0.086]  time 1.38s 
Epoch 0/600 105/293 loss: 1.5407 acc [ 0.433  0.068  0.086]  time 0.02s 
Epoch 0/600 106/293 loss: 1.5402 acc [ 0.427  0.066  0.086]  time 0.02s 
Epoch 0/600 107/293 loss: 1.5395 acc [ 0.422  0.066  0.086]  time 0.03s 
Epoch 0/600 108/293 loss: 1.5372 acc [ 0.422  0.066  0.086]  time 0.02s 
Epoch 0/600 109/293 loss: 1.5356 acc [ 0.420  0.066  0.085]  time 0.02s 
Epoch 0/600 110/293 loss: 1.5336 acc [ 0.421  0.064  0.084]  time 0.02s 
Epoch 0/600 111/293 loss: 1.5333 acc [ 0.421  0.064  0.084]  time 0.02s 
Epoch 0/600 112/293 loss: 1.5331 acc [ 0.421  0.064  0.084]  time 0.67s 
Epoch 0/600 113/293 loss: 1.5317 acc [ 0.420  0.064  0.084]  time 0.02s 
Epoch 0/600 114/293 loss: 1.5314 acc [ 0.420  0.064  0.084]  time 0.02s 
Epoch 0/600 115/293 loss: 1.5296 acc [ 0.421  0.064  0.082]  time 0.02s 
Epoch 0/600 116/293 loss: 1.5282 acc [ 0.419  0.063  0.080]  time 0.02s 
Epoch 0/600 117/293 loss: 1.5264 acc [ 0.419  0.062  0.079]  time 0.02s 
Epoch 0/600 118/293 loss: 1.5243 acc [ 0.421  0.063  0.082]  time 5.20s 
Epoch 0/600 119/293 loss: 1.5232 acc [ 0.418  0.063  0.082]  time 0.03s 
Epoch 0/600 120/293 loss: 1.5215 acc [ 0.419  0.063  0.082]  time 0.03s 
Epoch 0/600 121/293 loss: 1.5213 acc [ 0.414  0.063  0.082]  time 0.03s 
Epoch 0/600 122/293 loss: 1.5211 acc [ 0.414  0.063  0.082]  time 5.50s 
Epoch 0/600 123/293 loss: 1.5209 acc [ 0.414  0.063  0.082]  time 0.02s 
Epoch 0/600 124/293 loss: 1.5196 acc [ 0.415  0.063  0.082]  time 0.02s 
Epoch 0/600 125/293 loss: 1.5183 acc [ 0.415  0.066  0.085]  time 0.02s 
Epoch 0/600 126/293 loss: 1.5169 acc [ 0.415  0.066  0.085]  time 0.02s 
Epoch 0/600 127/293 loss: 1.5150 acc [ 0.417  0.065  0.084]  time 0.02s 
Epoch 0/600 128/293 loss: 1.5130 acc [ 0.417  0.065  0.084]  time 0.02s 
Epoch 0/600 129/293 loss: 1.5165 acc [ 0.422  0.068  0.084]  time 0.02s 
Epoch 0/600 130/293 loss: 1.5158 acc [ 0.420  0.069  0.086]  time 2.19s 
Epoch 0/600 131/293 loss: 1.5147 acc [ 0.420  0.069  0.086]  time 0.02s 
Epoch 0/600 132/293 loss: 1.5124 acc [ 0.424  0.069  0.086]  time 0.02s 
Epoch 0/600 133/293 loss: 1.5132 acc [ 0.421  0.068  0.085]  time 0.03s 
Epoch 0/600 134/293 loss: 1.5129 acc [ 0.418  0.068  0.085]  time 0.03s 
Epoch 0/600 135/293 loss: 1.5114 acc [ 0.416  0.067  0.085]  time 0.03s 
Epoch 0/600 136/293 loss: 1.5102 acc [ 0.416  0.068  0.084]  time 0.03s 
Epoch 0/600 137/293 loss: 1.5102 acc [ 0.416  0.068  0.084]  time 0.03s 
Epoch 0/600 138/293 loss: 1.5155 acc [ 0.420  0.071  0.086]  time 1.72s 
Epoch 0/600 139/293 loss: 1.5158 acc [ 0.423  0.072  0.086]  time 0.02s 
Epoch 0/600 140/293 loss: 1.5149 acc [ 0.422  0.071  0.084]  time 0.03s 
Epoch 0/600 141/293 loss: 1.5148 acc [ 0.422  0.071  0.084]  time 0.02s 
Epoch 0/600 142/293 loss: 1.5137 acc [ 0.424  0.072  0.084]  time 0.02s 
Epoch 0/600 143/293 loss: 1.5136 acc [ 0.424  0.072  0.084]  time 0.02s 
Epoch 0/600 144/293 loss: 1.5133 acc [ 0.424  0.073  0.083]  time 0.02s 
Epoch 0/600 145/293 loss: 1.5122 acc [ 0.422  0.073  0.086]  time 0.03s 
Epoch 0/600 146/293 loss: 1.5105 acc [ 0.422  0.076  0.089]  time 4.72s 
Epoch 0/600 147/293 loss: 1.5104 acc [ 0.422  0.076  0.089]  time 0.03s 
Epoch 0/600 148/293 loss: 1.5090 acc [ 0.425  0.077  0.090]  time 5.36s 
Epoch 0/600 149/293 loss: 1.5084 acc [ 0.424  0.077  0.090]  time 0.02s 
Epoch 0/600 150/293 loss: 1.5108 acc [ 0.426  0.079  0.091]  time 0.02s 
Epoch 0/600 151/293 loss: 1.5102 acc [ 0.425  0.079  0.091]  time 0.02s 
Epoch 0/600 152/293 loss: 1.5094 acc [ 0.428  0.078  0.091]  time 3.34s 
Epoch 0/600 153/293 loss: 1.5090 acc [ 0.427  0.078  0.090]  time 0.02s 
Epoch 0/600 154/293 loss: 1.5090 acc [ 0.427  0.078  0.090]  time 0.02s 
Epoch 0/600 155/293 loss: 1.5089 acc [ 0.427  0.078  0.090]  time 0.02s 
Epoch 0/600 156/293 loss: 1.5093 acc [ 0.426  0.077  0.090]  time 0.07s 
Epoch 0/600 157/293 loss: 1.5092 acc [ 0.426  0.077  0.090]  time 0.02s 
Epoch 0/600 158/293 loss: 1.5091 acc [ 0.426  0.076  0.090]  time 0.03s 
Epoch 0/600 159/293 loss: 1.5090 acc [ 0.426  0.076  0.090]  time 0.02s 
Epoch 0/600 160/293 loss: 1.5078 acc [ 0.425  0.076  0.089]  time 0.77s 
Epoch 0/600 161/293 loss: 1.5078 acc [ 0.425  0.076  0.089]  time 0.02s 
Epoch 0/600 162/293 loss: 1.5077 acc [ 0.425  0.076  0.089]  time 0.02s 
Epoch 0/600 163/293 loss: 1.5077 acc [ 0.425  0.076  0.089]  time 0.02s 
Epoch 0/600 164/293 loss: 1.5064 acc [ 0.429  0.075  0.089]  time 0.06s 
Epoch 0/600 165/293 loss: 1.5066 acc [ 0.430  0.080  0.092]  time 0.02s 
Epoch 0/600 166/293 loss: 1.5057 acc [ 0.428  0.079  0.092]  time 0.02s 
Epoch 0/600 167/293 loss: 1.5062 acc [ 0.431  0.080  0.092]  time 7.46s 
Epoch 0/600 168/293 loss: 1.5053 acc [ 0.430  0.080  0.092]  time 0.02s 
Epoch 0/600 169/293 loss: 1.5050 acc [ 0.434  0.079  0.092]  time 0.02s 
Epoch 0/600 170/293 loss: 1.5037 acc [ 0.434  0.079  0.092]  time 0.02s 
Epoch 0/600 171/293 loss: 1.5037 acc [ 0.434  0.079  0.092]  time 0.02s 
Epoch 0/600 172/293 loss: 1.5023 acc [ 0.433  0.079  0.092]  time 0.02s 
Epoch 0/600 173/293 loss: 1.5204 acc [ 0.437  0.081  0.093]  time 0.02s 
Epoch 0/600 174/293 loss: 1.5203 acc [ 0.437  0.081  0.093]  time 0.02s 
Epoch 0/600 175/293 loss: 1.5231 acc [ 0.440  0.086  0.100]  time 1.91s 
Epoch 0/600 176/293 loss: 1.5229 acc [ 0.440  0.086  0.100]  time 0.02s 
Epoch 0/600 177/293 loss: 1.5217 acc [ 0.439  0.087  0.101]  time 0.02s 
Epoch 0/600 178/293 loss: 1.5201 acc [ 0.443  0.086  0.101]  time 0.02s 
Epoch 0/600 179/293 loss: 1.5200 acc [ 0.443  0.086  0.101]  time 0.03s 
Epoch 0/600 180/293 loss: 1.5194 acc [ 0.443  0.090  0.105]  time 0.02s 
Epoch 0/600 181/293 loss: 1.5193 acc [ 0.445  0.090  0.104]  time 0.17s 
Epoch 0/600 182/293 loss: 1.5184 acc [ 0.443  0.090  0.103]  time 0.02s 
Epoch 0/600 183/293 loss: 1.5189 acc [ 0.447  0.093  0.106]  time 2.91s 
Epoch 0/600 184/293 loss: 1.5188 acc [ 0.447  0.093  0.106]  time 0.02s 
Epoch 0/600 185/293 loss: 1.5177 acc [ 0.446  0.096  0.107]  time 0.02s 
Epoch 0/600 186/293 loss: 1.5176 acc [ 0.446  0.096  0.107]  time 0.02s 
Epoch 0/600 187/293 loss: 1.5175 acc [ 0.446  0.096  0.107]  time 0.02s 
Epoch 0/600 188/293 loss: 1.5174 acc [ 0.446  0.096  0.107]  time 0.02s 
Epoch 0/600 189/293 loss: 1.5172 acc [ 0.443  0.096  0.107]  time 0.02s 
Epoch 0/600 190/293 loss: 1.5163 acc [ 0.442  0.095  0.106]  time 0.02s 
Epoch 0/600 191/293 loss: 1.5154 acc [ 0.446  0.097  0.108]  time 9.70s 
Epoch 0/600 192/293 loss: 1.5144 acc [ 0.445  0.098  0.109]  time 0.02s 
Epoch 0/600 193/293 loss: 1.5130 acc [ 0.445  0.098  0.109]  time 0.02s 
Epoch 0/600 194/293 loss: 1.5129 acc [ 0.445  0.098  0.109]  time 0.02s 
Epoch 0/600 195/293 loss: 1.5120 acc [ 0.444  0.097  0.108]  time 0.02s 
Epoch 0/600 196/293 loss: 1.5119 acc [ 0.444  0.097  0.108]  time 0.02s 
Epoch 0/600 197/293 loss: 1.5119 acc [ 0.444  0.097  0.108]  time 0.02s 
Epoch 0/600 198/293 loss: 1.5150 acc [ 0.445  0.098  0.107]  time 0.02s 
Epoch 0/600 199/293 loss: 1.5149 acc [ 0.445  0.098  0.107]  time 2.89s 
Epoch 0/600 200/293 loss: 1.5148 acc [ 0.441  0.098  0.107]  time 0.02s 
Epoch 0/600 201/293 loss: 1.5150 acc [ 0.444  0.098  0.107]  time 0.02s 
Epoch 0/600 202/293 loss: 1.5141 acc [ 0.444  0.097  0.107]  time 0.02s 
Epoch 0/600 203/293 loss: 1.5130 acc [ 0.445  0.097  0.107]  time 0.02s 
Epoch 0/600 204/293 loss: 1.5122 acc [ 0.445  0.097  0.107]  time 0.02s 
Epoch 0/600 205/293 loss: 1.5122 acc [ 0.445  0.097  0.107]  time 0.02s 
Epoch 0/600 206/293 loss: 1.5113 acc [ 0.444  0.096  0.106]  time 0.02s 
Epoch 0/600 207/293 loss: 1.5131 acc [ 0.446  0.102  0.113]  time 0.87s 
Epoch 0/600 208/293 loss: 1.5131 acc [ 0.446  0.102  0.113]  time 0.03s 
Epoch 0/600 209/293 loss: 1.5130 acc [ 0.446  0.102  0.113]  time 0.02s 
Epoch 0/600 210/293 loss: 1.5121 acc [ 0.447  0.103  0.114]  time 7.64s 
Epoch 0/600 211/293 loss: 1.5111 acc [ 0.448  0.104  0.114]  time 0.02s 
Epoch 0/600 212/293 loss: 1.5189 acc [ 0.451  0.106  0.115]  time 0.28s 
Epoch 0/600 213/293 loss: 1.5188 acc [ 0.451  0.106  0.115]  time 0.02s 
Epoch 0/600 214/293 loss: 1.5187 acc [ 0.452  0.105  0.114]  time 0.02s 
Epoch 0/600 215/293 loss: 1.5176 acc [ 0.452  0.105  0.114]  time 0.02s 
Epoch 0/600 216/293 loss: 1.5172 acc [ 0.452  0.107  0.118]  time 0.02s 
Epoch 0/600 217/293 loss: 1.5171 acc [ 0.452  0.107  0.118]  time 0.02s 
Epoch 0/600 218/293 loss: 1.5170 acc [ 0.452  0.107  0.118]  time 5.28s 
Epoch 0/600 219/293 loss: 1.5197 acc [ 0.454  0.111  0.123]  time 0.02s 
Epoch 0/600 220/293 loss: 1.5188 acc [ 0.454  0.110  0.123]  time 0.02s 
Epoch 0/600 221/293 loss: 1.5188 acc [ 0.454  0.110  0.123]  time 0.02s 
Epoch 0/600 222/293 loss: 1.5187 acc [ 0.451  0.110  0.123]  time 0.02s 
Epoch 0/600 223/293 loss: 1.5176 acc [ 0.450  0.111  0.123]  time 0.02s 
Epoch 0/600 224/293 loss: 1.5178 acc [ 0.451  0.112  0.124]  time 0.02s 
Epoch 0/600 225/293 loss: 1.5167 acc [ 0.450  0.112  0.125]  time 0.02s 
Epoch 0/600 226/293 loss: 1.5151 acc [ 0.451  0.112  0.125]  time 1.53s 
Epoch 0/600 227/293 loss: 1.5137 acc [ 0.454  0.112  0.125]  time 0.03s 
Epoch 0/600 228/293 loss: 1.5186 acc [ 0.455  0.113  0.126]  time 0.03s 
Epoch 0/600 229/293 loss: 1.5185 acc [ 0.455  0.113  0.126]  time 0.02s 
Epoch 0/600 230/293 loss: 1.5172 acc [ 0.455  0.116  0.128]  time 0.03s 
Epoch 0/600 231/293 loss: 1.5193 acc [ 0.458  0.119  0.133]  time 4.14s 
Epoch 0/600 232/293 loss: 1.5192 acc [ 0.458  0.119  0.133]  time 0.02s 
Epoch 0/600 233/293 loss: 1.5220 acc [ 0.458  0.118  0.132]  time 0.02s 
Epoch 0/600 234/293 loss: 1.5219 acc [ 0.458  0.118  0.132]  time 0.02s 
Epoch 0/600 235/293 loss: 1.5215 acc [ 0.460  0.117  0.132]  time 0.25s 
Epoch 0/600 236/293 loss: 1.5253 acc [ 0.463  0.122  0.137]  time 0.13s 
Epoch 0/600 237/293 loss: 1.5243 acc [ 0.463  0.122  0.137]  time 0.03s 
Epoch 0/600 238/293 loss: 1.5242 acc [ 0.463  0.122  0.137]  time 0.02s 
Epoch 0/600 239/293 loss: 1.5230 acc [ 0.464  0.122  0.136]  time 0.36s 
Epoch 0/600 240/293 loss: 1.5223 acc [ 0.466  0.121  0.136]  time 1.80s 
Epoch 0/600 241/293 loss: 1.5210 acc [ 0.467  0.121  0.136]  time 0.02s 
Epoch 0/600 242/293 loss: 1.5201 acc [ 0.466  0.121  0.136]  time 0.02s 
Epoch 0/600 243/293 loss: 1.5200 acc [ 0.463  0.121  0.136]  time 0.02s 
Epoch 0/600 244/293 loss: 1.5192 acc [ 0.465  0.121  0.136]  time 4.59s 
Epoch 0/600 245/293 loss: 1.5184 acc [ 0.464  0.121  0.135]  time 0.02s 
Epoch 0/600 246/293 loss: 1.5175 acc [ 0.463  0.121  0.135]  time 0.02s 
Epoch 0/600 247/293 loss: 1.5168 acc [ 0.463  0.120  0.134]  time 0.03s 
Epoch 0/600 248/293 loss: 1.5169 acc [ 0.464  0.123  0.136]  time 4.88s 
Epoch 0/600 249/293 loss: 1.5167 acc [ 0.462  0.123  0.136]  time 0.02s 
Epoch 0/600 250/293 loss: 1.5166 acc [ 0.462  0.123  0.136]  time 0.02s 
Epoch 0/600 251/293 loss: 1.5165 acc [ 0.460  0.122  0.136]  time 0.03s 
Epoch 0/600 252/293 loss: 1.5151 acc [ 0.460  0.123  0.138]  time 0.03s 
Epoch 0/600 253/293 loss: 1.5142 acc [ 0.460  0.123  0.138]  time 0.03s 
Epoch 0/600 254/293 loss: 1.5142 acc [ 0.460  0.123  0.138]  time 0.03s 
Epoch 0/600 255/293 loss: 1.5131 acc [ 0.461  0.124  0.139]  time 0.02s 
Epoch 0/600 256/293 loss: 1.5130 acc [ 0.461  0.124  0.139]  time 1.27s 
Epoch 0/600 257/293 loss: 1.5130 acc [ 0.461  0.124  0.139]  time 0.02s 
Epoch 0/600 258/293 loss: 1.5129 acc [ 0.461  0.124  0.139]  time 0.03s 
Epoch 0/600 259/293 loss: 1.5121 acc [ 0.462  0.124  0.139]  time 0.03s 
Epoch 0/600 260/293 loss: 1.5306 acc [ 0.462  0.125  0.139]  time 3.12s 
Epoch 0/600 261/293 loss: 1.5349 acc [ 0.463  0.125  0.139]  time 0.03s 
Epoch 0/600 262/293 loss: 1.5341 acc [ 0.465  0.127  0.141]  time 0.03s 
Epoch 0/600 263/293 loss: 1.5368 acc [ 0.465  0.129  0.142]  time 0.03s 
Epoch 0/600 264/293 loss: 1.5358 acc [ 0.466  0.129  0.142]  time 0.03s 
Epoch 0/600 265/293 loss: 1.5357 acc [ 0.466  0.129  0.142]  time 0.03s 
Epoch 0/600 266/293 loss: 1.5350 acc [ 0.465  0.128  0.141]  time 0.03s 
Epoch 0/600 267/293 loss: 1.5346 acc [ 0.463  0.127  0.140]  time 0.03s 
Epoch 0/600 268/293 loss: 1.5333 acc [ 0.465  0.128  0.141]  time 3.28s 
Epoch 0/600 269/293 loss: 1.5331 acc [ 0.465  0.128  0.141]  time 0.02s 
Epoch 0/600 270/293 loss: 1.5317 acc [ 0.466  0.129  0.142]  time 0.03s 
Epoch 0/600 271/293 loss: 1.5316 acc [ 0.468  0.129  0.142]  time 1.76s 
Epoch 0/600 272/293 loss: 1.5314 acc [ 0.468  0.129  0.142]  time 0.32s 
Epoch 0/600 273/293 loss: 1.5313 acc [ 0.465  0.129  0.142]  time 0.02s 
Epoch 0/600 274/293 loss: 1.5389 acc [ 0.466  0.130  0.143]  time 0.02s 
Epoch 0/600 275/293 loss: 1.5380 acc [ 0.466  0.133  0.145]  time 0.02s 
Epoch 0/600 276/293 loss: 1.5375 acc [ 0.464  0.133  0.145]  time 0.30s 
Epoch 0/600 277/293 loss: 1.5373 acc [ 0.467  0.133  0.145]  time 5.57s 
Epoch 0/600 278/293 loss: 1.5364 acc [ 0.467  0.133  0.145]  time 0.02s 
Epoch 0/600 279/293 loss: 1.5364 acc [ 0.467  0.134  0.146]  time 0.02s 
Epoch 0/600 280/293 loss: 1.5352 acc [ 0.468  0.137  0.149]  time 0.02s 
Epoch 0/600 281/293 loss: 1.5343 acc [ 0.468  0.137  0.149]  time 0.02s 
Epoch 0/600 282/293 loss: 1.5332 acc [ 0.468  0.137  0.149]  time 0.02s 
Epoch 0/600 283/293 loss: 1.5324 acc [ 0.467  0.137  0.149]  time 0.02s 
Epoch 0/600 284/293 loss: 1.5314 acc [ 0.467  0.137  0.149]  time 0.02s 
Epoch 0/600 285/293 loss: 1.5313 acc [ 0.467  0.137  0.149]  time 2.37s 
Epoch 0/600 286/293 loss: 1.5312 acc [ 0.467  0.137  0.149]  time 0.02s 
Epoch 0/600 287/293 loss: 1.5302 acc [ 0.467  0.136  0.148]  time 0.02s 
Epoch 0/600 288/293 loss: 1.5295 acc [ 0.467  0.136  0.148]  time 0.02s 
Epoch 0/600 289/293 loss: 1.5294 acc [ 0.467  0.136  0.148]  time 0.02s 
Epoch 0/600 290/293 loss: 1.5293 acc [ 0.467  0.136  0.148]  time 0.02s 
Epoch 0/600 291/293 loss: 1.5285 acc [ 0.467  0.136  0.148]  time 0.02s 
Epoch 0/600 292/293 loss: 1.5274 acc [ 0.468  0.136  0.149]  time 4.47s 
Final training  0/599 loss: 1.5274 acc_avg: 0.2514 acc [ 0.468  0.136  0.149] time 196.25s  lr: 2.0000e-05
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 0, 'best_metric': -1}, save_time 0.09s
Estimated remaining training time for the current model fold 0 is 37.72 hr, running time 0.05 hr, est total time 37.78 hr 

Epoch 1/600 0/293 loss: 2.2550 acc [ 0.544  0.014  0.006]  time 1.87s 
Epoch 1/600 1/293 loss: 1.7613 acc [ 0.452  0.014  0.006]  time 0.05s 
Epoch 1/600 2/293 loss: 1.6869 acc [ 0.580  0.272  0.284]  time 0.92s 
Epoch 1/600 3/293 loss: 1.5847 acc [ 0.545  0.204  0.215]  time 0.61s 
Epoch 1/600 4/293 loss: 1.5397 acc [ 0.474  0.192  0.204]  time 0.02s 
Epoch 1/600 5/293 loss: 1.4944 acc [ 0.483  0.165  0.181]  time 0.66s 
Epoch 1/600 6/293 loss: 1.4652 acc [ 0.490  0.156  0.157]  time 0.03s 
Epoch 1/600 7/293 loss: 1.4696 acc [ 0.490  0.156  0.157]  time 0.03s 
Epoch 1/600 8/293 loss: 1.4340 acc [ 0.490  0.181  0.178]  time 0.03s 
Epoch 1/600 9/293 loss: 1.4406 acc [ 0.490  0.181  0.178]  time 0.03s 
Epoch 1/600 10/293 loss: 1.4460 acc [ 0.490  0.181  0.178]  time 0.03s 
Epoch 1/600 11/293 loss: 1.4249 acc [ 0.516  0.181  0.178]  time 1.09s 
Epoch 1/600 12/293 loss: 1.4075 acc [ 0.541  0.229  0.222]  time 0.04s 
Epoch 1/600 13/293 loss: 1.4384 acc [ 0.496  0.204  0.197]  time 0.03s 
Epoch 1/600 14/293 loss: 1.5202 acc [ 0.487  0.190  0.183]  time 0.03s 
Epoch 1/600 15/293 loss: 1.5015 acc [ 0.510  0.174  0.170]  time 0.03s 
Epoch 1/600 16/293 loss: 1.5014 acc [ 0.510  0.174  0.170]  time 0.03s 
Epoch 1/600 17/293 loss: 1.5105 acc [ 0.527  0.160  0.156]  time 1.48s 
Epoch 1/600 18/293 loss: 1.4991 acc [ 0.518  0.160  0.156]  time 0.03s 
Epoch 1/600 19/293 loss: 1.4991 acc [ 0.486  0.147  0.144]  time 0.05s 
Epoch 1/600 20/293 loss: 1.4904 acc [ 0.475  0.137  0.144]  time 0.14s 
Epoch 1/600 21/293 loss: 1.4909 acc [ 0.475  0.137  0.144]  time 0.08s 
Epoch 1/600 22/293 loss: 1.4913 acc [ 0.475  0.137  0.144]  time 0.09s 
Epoch 1/600 23/293 loss: 1.4916 acc [ 0.475  0.137  0.144]  time 0.02s 
Epoch 1/600 24/293 loss: 1.4853 acc [ 0.492  0.136  0.134]  time 0.04s 
Epoch 1/600 25/293 loss: 1.5061 acc [ 0.501  0.165  0.165]  time 0.04s 
Epoch 1/600 26/293 loss: 1.5002 acc [ 0.497  0.178  0.183]  time 0.02s 
Epoch 1/600 27/293 loss: 1.4898 acc [ 0.516  0.168  0.183]  time 0.02s 
Epoch 1/600 28/293 loss: 1.4902 acc [ 0.516  0.168  0.183]  time 0.03s 
Epoch 1/600 29/293 loss: 1.5273 acc [ 0.529  0.191  0.208]  time 0.03s 
Epoch 1/600 30/293 loss: 1.5197 acc [ 0.533  0.191  0.208]  time 0.03s 
Epoch 1/600 31/293 loss: 1.5185 acc [ 0.514  0.191  0.208]  time 0.04s 
Epoch 1/600 32/293 loss: 1.5065 acc [ 0.519  0.208  0.225]  time 0.03s 
Epoch 1/600 33/293 loss: 1.5022 acc [ 0.511  0.208  0.225]  time 0.39s 
Epoch 1/600 34/293 loss: 1.4959 acc [ 0.515  0.210  0.226]  time 0.02s 
Epoch 1/600 35/293 loss: 1.4897 acc [ 0.521  0.230  0.247]  time 0.22s 
Epoch 1/600 36/293 loss: 1.4900 acc [ 0.521  0.230  0.247]  time 0.02s 
Epoch 1/600 37/293 loss: 1.4913 acc [ 0.522  0.220  0.235]  time 0.11s 
Epoch 1/600 38/293 loss: 1.4896 acc [ 0.523  0.220  0.235]  time 0.30s 
Epoch 1/600 39/293 loss: 1.4899 acc [ 0.523  0.220  0.235]  time 3.00s 
Epoch 1/600 40/293 loss: 1.4773 acc [ 0.530  0.234  0.249]  time 0.03s 
Epoch 1/600 41/293 loss: 1.4742 acc [ 0.534  0.225  0.249]  time 0.03s 
Epoch 1/600 42/293 loss: 1.4748 acc [ 0.534  0.225  0.249]  time 0.02s 
Epoch 1/600 43/293 loss: 1.4659 acc [ 0.543  0.231  0.256]  time 0.02s 
Epoch 1/600 44/293 loss: 1.4611 acc [ 0.546  0.239  0.263]  time 0.03s 
Epoch 1/600 45/293 loss: 1.4533 acc [ 0.553  0.237  0.260]  time 0.03s 
Epoch 1/600 46/293 loss: 1.4502 acc [ 0.554  0.237  0.258]  time 0.79s 
Epoch 1/600 47/293 loss: 1.4595 acc [ 0.547  0.230  0.249]  time 0.02s 
Epoch 1/600 48/293 loss: 1.4623 acc [ 0.535  0.223  0.241]  time 0.02s 
Epoch 1/600 49/293 loss: 1.4576 acc [ 0.545  0.216  0.241]  time 0.03s 
Epoch 1/600 50/293 loss: 1.4497 acc [ 0.554  0.218  0.241]  time 0.03s 
Epoch 1/600 51/293 loss: 1.4506 acc [ 0.554  0.218  0.241]  time 0.03s 
Epoch 1/600 52/293 loss: 1.4602 acc [ 0.563  0.220  0.245]  time 0.02s 
Epoch 1/600 53/293 loss: 1.4609 acc [ 0.563  0.220  0.245]  time 0.03s 
Epoch 1/600 54/293 loss: 1.4616 acc [ 0.563  0.220  0.245]  time 2.28s 
Epoch 1/600 55/293 loss: 1.4657 acc [ 0.563  0.223  0.248]  time 0.31s 
Epoch 1/600 56/293 loss: 1.4619 acc [ 0.565  0.217  0.241]  time 0.03s 
Epoch 1/600 57/293 loss: 1.4582 acc [ 0.565  0.217  0.241]  time 0.02s 
Epoch 1/600 58/293 loss: 1.4589 acc [ 0.565  0.217  0.241]  time 0.02s 
Epoch 1/600 59/293 loss: 1.4596 acc [ 0.565  0.217  0.241]  time 0.03s 
Epoch 1/600 60/293 loss: 1.4603 acc [ 0.565  0.217  0.241]  time 0.02s 
Epoch 1/600 61/293 loss: 1.5319 acc [ 0.568  0.220  0.243]  time 0.02s 
Epoch 1/600 62/293 loss: 1.5441 acc [ 0.572  0.228  0.253]  time 1.24s 
Epoch 1/600 63/293 loss: 1.5388 acc [ 0.578  0.228  0.252]  time 0.03s 
Epoch 1/600 64/293 loss: 1.5382 acc [ 0.578  0.228  0.252]  time 0.02s 
Epoch 1/600 65/293 loss: 1.5376 acc [ 0.578  0.228  0.252]  time 0.04s 
Epoch 1/600 66/293 loss: 1.5357 acc [ 0.574  0.230  0.252]  time 0.03s 
Epoch 1/600 67/293 loss: 1.5381 acc [ 0.578  0.236  0.257]  time 0.02s 
Epoch 1/600 68/293 loss: 1.5559 acc [ 0.584  0.248  0.271]  time 0.05s 
Epoch 1/600 69/293 loss: 1.5544 acc [ 0.577  0.243  0.271]  time 0.02s 
Epoch 1/600 70/293 loss: 1.5515 acc [ 0.572  0.243  0.271]  time 0.18s 
Epoch 1/600 71/293 loss: 1.5508 acc [ 0.572  0.243  0.271]  time 0.02s 
Epoch 1/600 72/293 loss: 1.5467 acc [ 0.574  0.246  0.275]  time 0.02s 
Epoch 1/600 73/293 loss: 1.5460 acc [ 0.564  0.246  0.275]  time 0.02s 
Epoch 1/600 74/293 loss: 1.5426 acc [ 0.565  0.242  0.269]  time 0.03s 
Epoch 1/600 75/293 loss: 1.5420 acc [ 0.565  0.242  0.269]  time 0.02s 
Epoch 1/600 76/293 loss: 1.5386 acc [ 0.564  0.238  0.265]  time 0.02s 
Epoch 1/600 77/293 loss: 1.5348 acc [ 0.565  0.238  0.264]  time 1.41s 
Epoch 1/600 78/293 loss: 1.5346 acc [ 0.559  0.238  0.264]  time 0.03s 
Epoch 1/600 79/293 loss: 1.5315 acc [ 0.561  0.234  0.264]  time 0.03s 
Epoch 1/600 80/293 loss: 1.5307 acc [ 0.565  0.238  0.268]  time 0.02s 
Epoch 1/600 81/293 loss: 1.5297 acc [ 0.561  0.233  0.262]  time 0.02s 
Epoch 1/600 82/293 loss: 1.5326 acc [ 0.560  0.229  0.256]  time 1.35s 
Epoch 1/600 83/293 loss: 1.5286 acc [ 0.559  0.232  0.259]  time 0.02s 
Epoch 1/600 84/293 loss: 1.5324 acc [ 0.560  0.239  0.267]  time 0.02s 
Epoch 1/600 85/293 loss: 1.5320 acc [ 0.560  0.239  0.267]  time 0.02s 
Epoch 1/600 86/293 loss: 1.5292 acc [ 0.558  0.236  0.261]  time 0.02s 
Epoch 1/600 87/293 loss: 1.5266 acc [ 0.556  0.233  0.257]  time 0.03s 
Epoch 1/600 88/293 loss: 1.5239 acc [ 0.552  0.236  0.260]  time 0.02s 
Epoch 1/600 89/293 loss: 1.5210 acc [ 0.553  0.233  0.256]  time 0.02s 
Epoch 1/600 90/293 loss: 1.5208 acc [ 0.553  0.233  0.256]  time 1.17s 
Epoch 1/600 91/293 loss: 1.5206 acc [ 0.553  0.233  0.256]  time 0.02s 
Epoch 1/600 92/293 loss: 1.5171 acc [ 0.551  0.236  0.259]  time 0.02s 
Epoch 1/600 93/293 loss: 1.5148 acc [ 0.549  0.232  0.254]  time 0.02s 
Epoch 1/600 94/293 loss: 1.5116 acc [ 0.553  0.232  0.254]  time 0.03s 
Epoch 1/600 95/293 loss: 1.5596 acc [ 0.551  0.232  0.254]  time 0.02s 
Epoch 1/600 96/293 loss: 1.5605 acc [ 0.550  0.234  0.255]  time 0.02s 
Epoch 1/600 97/293 loss: 1.5596 acc [ 0.553  0.232  0.252]  time 0.02s 
Epoch 1/600 98/293 loss: 1.5590 acc [ 0.553  0.232  0.252]  time 0.16s 
Epoch 1/600 99/293 loss: 1.5557 acc [ 0.552  0.229  0.248]  time 0.02s 
Epoch 1/600 100/293 loss: 1.5551 acc [ 0.552  0.229  0.248]  time 0.02s 
Epoch 1/600 101/293 loss: 1.5520 acc [ 0.555  0.227  0.248]  time 0.03s 
Epoch 1/600 102/293 loss: 1.5515 acc [ 0.555  0.227  0.248]  time 0.02s 
Epoch 1/600 103/293 loss: 1.5510 acc [ 0.555  0.227  0.248]  time 1.60s 
Epoch 1/600 104/293 loss: 1.5484 acc [ 0.556  0.223  0.244]  time 0.02s 
Epoch 1/600 105/293 loss: 1.5479 acc [ 0.556  0.223  0.244]  time 0.05s 
Epoch 1/600 106/293 loss: 1.5475 acc [ 0.556  0.223  0.244]  time 0.02s 
Epoch 1/600 107/293 loss: 1.5470 acc [ 0.556  0.223  0.244]  time 0.02s 
Epoch 1/600 108/293 loss: 1.5457 acc [ 0.559  0.228  0.249]  time 0.02s 
Epoch 1/600 109/293 loss: 1.5453 acc [ 0.559  0.228  0.249]  time 0.85s 
Epoch 1/600 110/293 loss: 1.5447 acc [ 0.553  0.228  0.249]  time 0.02s 
Epoch 1/600 111/293 loss: 1.5445 acc [ 0.553  0.225  0.245]  time 0.02s 
Epoch 1/600 112/293 loss: 1.5441 acc [ 0.553  0.225  0.245]  time 0.03s 
Epoch 1/600 113/293 loss: 1.5462 acc [ 0.555  0.225  0.245]  time 2.05s 
Epoch 1/600 114/293 loss: 1.5458 acc [ 0.555  0.225  0.245]  time 0.06s 
Epoch 1/600 115/293 loss: 1.5433 acc [ 0.557  0.230  0.249]  time 0.02s 
Epoch 1/600 116/293 loss: 1.5503 acc [ 0.560  0.234  0.254]  time 0.02s 
Epoch 1/600 117/293 loss: 1.5499 acc [ 0.553  0.234  0.254]  time 0.02s 
Epoch 1/600 118/293 loss: 1.5494 acc [ 0.552  0.230  0.250]  time 0.02s 
Epoch 1/600 119/293 loss: 1.5490 acc [ 0.554  0.238  0.259]  time 0.02s 
Epoch 1/600 120/293 loss: 1.5468 acc [ 0.554  0.236  0.256]  time 0.02s 
Epoch 1/600 121/293 loss: 1.5445 acc [ 0.554  0.233  0.256]  time 1.98s 
Epoch 1/600 122/293 loss: 1.5421 acc [ 0.553  0.231  0.254]  time 0.02s 
Epoch 1/600 123/293 loss: 1.5393 acc [ 0.555  0.231  0.254]  time 0.02s 
Epoch 1/600 124/293 loss: 1.5377 acc [ 0.555  0.234  0.257]  time 0.03s 
Epoch 1/600 125/293 loss: 1.5361 acc [ 0.555  0.234  0.257]  time 0.04s 
Epoch 1/600 126/293 loss: 1.5358 acc [ 0.555  0.234  0.257]  time 0.02s 
Epoch 1/600 127/293 loss: 1.5334 acc [ 0.555  0.232  0.254]  time 0.03s 
Epoch 1/600 128/293 loss: 1.5313 acc [ 0.554  0.232  0.254]  time 0.04s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 4
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 4
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1711GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 4 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/74 loss: 1.5349 acc [ 0.207  0.071  0.106]  time 10.91s 
Epoch 0/600 1/74 loss: 1.5491 acc [ 0.248  0.087  0.132]  time 0.26s 
Epoch 0/600 2/74 loss: 1.5371 acc [ 0.275  0.087  0.124]  time 0.03s 
Epoch 0/600 3/74 loss: 1.5420 acc [ 0.274  0.083  0.117]  time 1.98s 
Epoch 0/600 4/74 loss: 1.5331 acc [ 0.302  0.080  0.117]  time 19.46s 
Epoch 0/600 5/74 loss: 1.5399 acc [ 0.270  0.078  0.111]  time 0.03s 
Epoch 0/600 6/74 loss: 1.5414 acc [ 0.278  0.079  0.108]  time 0.03s 
Epoch 0/600 7/74 loss: 1.5560 acc [ 0.268  0.099  0.133]  time 0.03s 
Epoch 0/600 8/74 loss: 1.5464 acc [ 0.274  0.100  0.135]  time 9.03s 
Epoch 0/600 9/74 loss: 1.5431 acc [ 0.268  0.093  0.131]  time 0.03s 
Epoch 0/600 10/74 loss: 1.5387 acc [ 0.290  0.091  0.128]  time 0.03s 
Epoch 0/600 11/74 loss: 1.5306 acc [ 0.285  0.091  0.128]  time 0.03s 
Epoch 0/600 12/74 loss: 1.5323 acc [ 0.292  0.098  0.136]  time 13.04s 
Epoch 0/600 13/74 loss: 1.5342 acc [ 0.307  0.093  0.125]  time 0.03s 
Epoch 0/600 14/74 loss: 1.5303 acc [ 0.318  0.095  0.125]  time 0.03s 
Epoch 0/600 15/74 loss: 1.5300 acc [ 0.316  0.097  0.125]  time 0.03s 
Epoch 0/600 16/74 loss: 1.5256 acc [ 0.324  0.096  0.121]  time 23.52s 
Epoch 0/600 17/74 loss: 1.5394 acc [ 0.340  0.104  0.124]  time 0.03s 
Epoch 0/600 18/74 loss: 1.5367 acc [ 0.350  0.101  0.119]  time 0.03s 
Epoch 0/600 19/74 loss: 1.5358 acc [ 0.342  0.102  0.117]  time 0.03s 
Epoch 0/600 20/74 loss: 1.5646 acc [ 0.352  0.111  0.132]  time 16.87s 
Epoch 0/600 21/74 loss: 1.5670 acc [ 0.353  0.113  0.130]  time 0.03s 
Epoch 0/600 22/74 loss: 1.5639 acc [ 0.351  0.116  0.132]  time 0.03s 
Epoch 0/600 23/74 loss: 1.5606 acc [ 0.353  0.115  0.129]  time 0.03s 
Epoch 0/600 24/74 loss: 1.5611 acc [ 0.357  0.115  0.126]  time 30.99s 
Epoch 0/600 25/74 loss: 1.5562 acc [ 0.353  0.114  0.124]  time 0.03s 
Epoch 0/600 26/74 loss: 1.5555 acc [ 0.356  0.115  0.124]  time 0.03s 
Epoch 0/600 27/74 loss: 1.5564 acc [ 0.364  0.118  0.125]  time 0.03s 
Epoch 0/600 28/74 loss: 1.5537 acc [ 0.362  0.118  0.125]  time 15.85s 
Epoch 0/600 29/74 loss: 1.5497 acc [ 0.357  0.115  0.122]  time 0.03s 
Epoch 0/600 30/74 loss: 1.5531 acc [ 0.360  0.116  0.121]  time 0.03s 
Epoch 0/600 31/74 loss: 1.5596 acc [ 0.362  0.118  0.122]  time 0.03s 
Epoch 0/600 32/74 loss: 1.5549 acc [ 0.363  0.117  0.121]  time 11.83s 
Epoch 0/600 33/74 loss: 1.5511 acc [ 0.362  0.115  0.117]  time 0.03s 
Epoch 0/600 34/74 loss: 1.5485 acc [ 0.356  0.111  0.113]  time 0.03s 
Epoch 0/600 35/74 loss: 1.5441 acc [ 0.359  0.110  0.111]  time 0.03s 
Epoch 0/600 36/74 loss: 1.5417 acc [ 0.361  0.110  0.111]  time 11.35s 
Epoch 0/600 37/74 loss: 1.5393 acc [ 0.364  0.109  0.110]  time 0.04s 
Epoch 0/600 38/74 loss: 1.5364 acc [ 0.362  0.108  0.110]  time 0.03s 
Epoch 0/600 39/74 loss: 1.5424 acc [ 0.366  0.108  0.110]  time 0.04s 
Epoch 0/600 40/74 loss: 1.5404 acc [ 0.371  0.109  0.109]  time 19.08s 
Epoch 0/600 41/74 loss: 1.5389 acc [ 0.367  0.110  0.109]  time 0.03s 
Epoch 0/600 42/74 loss: 1.5363 acc [ 0.369  0.109  0.107]  time 0.03s 
Epoch 0/600 43/74 loss: 1.5336 acc [ 0.374  0.109  0.107]  time 0.03s 
Epoch 0/600 44/74 loss: 1.5320 acc [ 0.370  0.108  0.105]  time 18.89s 
Epoch 0/600 45/74 loss: 1.5302 acc [ 0.374  0.107  0.103]  time 0.03s 
Epoch 0/600 46/74 loss: 1.5306 acc [ 0.378  0.107  0.103]  time 0.03s 
Epoch 0/600 47/74 loss: 1.5288 acc [ 0.376  0.107  0.102]  time 0.03s 
Epoch 0/600 48/74 loss: 1.5290 acc [ 0.374  0.107  0.101]  time 7.61s 
Epoch 0/600 49/74 loss: 1.5265 acc [ 0.374  0.106  0.100]  time 0.03s 
Epoch 0/600 50/74 loss: 1.5266 acc [ 0.377  0.107  0.100]  time 0.04s 
Epoch 0/600 51/74 loss: 1.5277 acc [ 0.380  0.106  0.101]  time 0.04s 
Epoch 0/600 52/74 loss: 1.5256 acc [ 0.382  0.106  0.101]  time 18.61s 
Epoch 0/600 53/74 loss: 1.5258 acc [ 0.387  0.107  0.100]  time 3.11s 
Epoch 0/600 54/74 loss: 1.5237 acc [ 0.389  0.106  0.099]  time 0.03s 
Epoch 0/600 55/74 loss: 1.5212 acc [ 0.387  0.104  0.096]  time 0.03s 
Epoch 0/600 56/74 loss: 1.5288 acc [ 0.389  0.103  0.095]  time 18.99s 
Epoch 0/600 57/74 loss: 1.5265 acc [ 0.388  0.102  0.094]  time 0.03s 
Epoch 0/600 58/74 loss: 1.5241 acc [ 0.389  0.100  0.092]  time 0.03s 
Epoch 0/600 59/74 loss: 1.5246 acc [ 0.391  0.101  0.091]  time 0.03s 
Epoch 0/600 60/74 loss: 1.5225 acc [ 0.393  0.099  0.089]  time 12.64s 
Epoch 0/600 61/74 loss: 1.5212 acc [ 0.392  0.099  0.088]  time 0.04s 
Epoch 0/600 62/74 loss: 1.5196 acc [ 0.389  0.098  0.086]  time 0.04s 
Epoch 0/600 63/74 loss: 1.5172 acc [ 0.389  0.097  0.085]  time 0.04s 
Epoch 0/600 64/74 loss: 1.5151 acc [ 0.391  0.097  0.084]  time 8.09s 
Epoch 0/600 65/74 loss: 1.5147 acc [ 0.393  0.097  0.083]  time 0.03s 
Epoch 0/600 66/74 loss: 1.5126 acc [ 0.392  0.097  0.083]  time 0.03s 
Epoch 0/600 67/74 loss: 1.5109 acc [ 0.395  0.098  0.082]  time 0.03s 
Epoch 0/600 68/74 loss: 1.5123 acc [ 0.398  0.099  0.082]  time 9.22s 
Epoch 0/600 69/74 loss: 1.5115 acc [ 0.402  0.098  0.081]  time 4.38s 
Epoch 0/600 70/74 loss: 1.5099 acc [ 0.403  0.098  0.080]  time 0.03s 
Epoch 0/600 71/74 loss: 1.5078 acc [ 0.404  0.098  0.080]  time 0.03s 
Epoch 0/600 72/74 loss: 1.5062 acc [ 0.404  0.098  0.080]  time 2.09s 
Epoch 0/600 73/74 loss: 1.5056 acc [ 0.405  0.098  0.080]  time 2.04s 
Final training  0/599 loss: 1.5056 acc_avg: 0.1941 acc [ 0.405  0.098  0.080] time 291.53s  lr: 2.0000e-05
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 0, 'best_metric': -1}, save_time 0.12s
Estimated remaining training time for the current model fold 0 is 56.04 hr, running time 0.08 hr, est total time 56.12 hr 

Epoch 1/600 0/74 loss: 1.4560 acc [ 0.606  0.106  0.055]  time 4.07s 
Epoch 1/600 1/74 loss: 1.3977 acc [ 0.514  0.122  0.051]  time 0.03s 
Epoch 1/600 2/74 loss: 1.3759 acc [ 0.505  0.098  0.041]  time 0.03s 
Epoch 1/600 3/74 loss: 1.3611 acc [ 0.553  0.094  0.036]  time 1.01s 
Epoch 1/600 4/74 loss: 1.4959 acc [ 0.596  0.087  0.028]  time 3.44s 
Epoch 1/600 5/74 loss: 1.4836 acc [ 0.581  0.087  0.028]  time 0.11s 
Epoch 1/600 6/74 loss: 1.4686 acc [ 0.547  0.094  0.028]  time 0.03s 
Epoch 1/600 7/74 loss: 1.4566 acc [ 0.550  0.085  0.027]  time 0.46s 
Epoch 1/600 8/74 loss: 1.4647 acc [ 0.549  0.090  0.027]  time 2.38s 
Epoch 1/600 9/74 loss: 1.4431 acc [ 0.542  0.084  0.027]  time 0.05s 
Epoch 1/600 10/74 loss: 1.4290 acc [ 0.541  0.090  0.029]  time 0.09s 
Epoch 1/600 11/74 loss: 1.4147 acc [ 0.526  0.097  0.036]  time 0.82s 
Epoch 1/600 12/74 loss: 1.4202 acc [ 0.519  0.115  0.041]  time 1.10s 
Epoch 1/600 13/74 loss: 1.4149 acc [ 0.517  0.126  0.048]  time 0.07s 
Epoch 1/600 14/74 loss: 1.4074 acc [ 0.506  0.119  0.047]  time 0.28s 
Epoch 1/600 15/74 loss: 1.4006 acc [ 0.513  0.120  0.050]  time 4.63s 
Epoch 1/600 16/74 loss: 1.3916 acc [ 0.518  0.117  0.052]  time 0.03s 
Epoch 1/600 17/74 loss: 1.3849 acc [ 0.512  0.120  0.058]  time 0.03s 
Epoch 1/600 18/74 loss: 1.3802 acc [ 0.508  0.119  0.058]  time 0.03s 
Epoch 1/600 19/74 loss: 1.3829 acc [ 0.503  0.116  0.057]  time 2.57s 
Epoch 1/600 20/74 loss: 1.3764 acc [ 0.507  0.119  0.065]  time 0.03s 
Epoch 1/600 21/74 loss: 1.3708 acc [ 0.502  0.120  0.069]  time 0.03s 
Epoch 1/600 22/74 loss: 1.3685 acc [ 0.501  0.119  0.072]  time 0.06s 
Epoch 1/600 23/74 loss: 1.3640 acc [ 0.498  0.121  0.079]  time 2.76s 
Epoch 1/600 24/74 loss: 1.3610 acc [ 0.504  0.125  0.085]  time 0.03s 
Epoch 1/600 25/74 loss: 1.3581 acc [ 0.510  0.136  0.099]  time 0.42s 
Epoch 1/600 26/74 loss: 1.3511 acc [ 0.512  0.135  0.103]  time 0.03s 
Epoch 1/600 27/74 loss: 1.3847 acc [ 0.517  0.149  0.121]  time 2.15s 
Epoch 1/600 28/74 loss: 1.3853 acc [ 0.518  0.147  0.123]  time 0.03s 
Epoch 1/600 29/74 loss: 1.3868 acc [ 0.511  0.145  0.122]  time 0.07s 
Epoch 1/600 30/74 loss: 1.3835 acc [ 0.512  0.145  0.122]  time 0.03s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 4
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 4
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1709GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 4 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/74 loss: 1.6679 acc [ 0.505  0.058  0.011]  time 26.71s 
Epoch 0/600 1/74 loss: 1.5942 acc [ 0.375  0.094  0.007]  time 0.12s 
Epoch 0/600 2/74 loss: 1.5633 acc [ 0.342  0.114  0.006]  time 0.03s 
Epoch 0/600 3/74 loss: 1.5557 acc [ 0.343  0.099  0.006]  time 0.03s 
Epoch 0/600 4/74 loss: 1.5442 acc [ 0.341  0.084  0.006]  time 7.23s 
Epoch 0/600 5/74 loss: 1.6865 acc [ 0.377  0.168  0.031]  time 0.03s 
Epoch 0/600 6/74 loss: 1.6556 acc [ 0.357  0.148  0.032]  time 0.03s 
Epoch 0/600 7/74 loss: 1.6518 acc [ 0.362  0.149  0.030]  time 0.03s 
Epoch 0/600 8/74 loss: 1.6479 acc [ 0.364  0.140  0.034]  time 14.77s 
Epoch 0/600 9/74 loss: 1.6325 acc [ 0.374  0.134  0.032]  time 0.03s 
Epoch 0/600 10/74 loss: 1.6168 acc [ 0.366  0.127  0.032]  time 0.04s 
Epoch 0/600 11/74 loss: 1.6040 acc [ 0.360  0.120  0.029]  time 0.04s 
Epoch 0/600 12/74 loss: 1.5954 acc [ 0.349  0.116  0.029]  time 5.19s 
Epoch 0/600 13/74 loss: 1.5886 acc [ 0.351  0.113  0.027]  time 0.04s 
Epoch 0/600 14/74 loss: 1.5788 acc [ 0.349  0.110  0.027]  time 0.04s 
Epoch 0/600 15/74 loss: 1.5760 acc [ 0.347  0.109  0.026]  time 0.04s 
Epoch 0/600 16/74 loss: 1.5671 acc [ 0.346  0.103  0.025]  time 18.91s 
Epoch 0/600 17/74 loss: 1.5596 acc [ 0.345  0.100  0.024]  time 0.03s 
Epoch 0/600 18/74 loss: 1.5535 acc [ 0.340  0.096  0.024]  time 0.04s 
Epoch 0/600 19/74 loss: 1.5475 acc [ 0.339  0.096  0.024]  time 0.03s 
Epoch 0/600 20/74 loss: 1.5421 acc [ 0.333  0.090  0.023]  time 12.72s 
Epoch 0/600 21/74 loss: 1.5353 acc [ 0.333  0.087  0.023]  time 0.04s 
Epoch 0/600 22/74 loss: 1.5422 acc [ 0.343  0.092  0.022]  time 0.03s 
Epoch 0/600 23/74 loss: 1.5420 acc [ 0.347  0.094  0.022]  time 0.03s 
Epoch 0/600 24/74 loss: 1.5362 acc [ 0.351  0.092  0.021]  time 29.56s 
Epoch 0/600 25/74 loss: 1.5333 acc [ 0.357  0.090  0.021]  time 0.03s 
Epoch 0/600 26/74 loss: 1.5348 acc [ 0.359  0.088  0.021]  time 0.03s 
Epoch 0/600 27/74 loss: 1.5298 acc [ 0.357  0.083  0.020]  time 0.03s 
Epoch 0/600 28/74 loss: 1.5278 acc [ 0.362  0.082  0.020]  time 13.96s 
Epoch 0/600 29/74 loss: 1.5304 acc [ 0.366  0.082  0.020]  time 0.03s 
Epoch 0/600 30/74 loss: 1.5280 acc [ 0.364  0.079  0.019]  time 0.03s 
Epoch 0/600 31/74 loss: 1.5266 acc [ 0.362  0.078  0.019]  time 0.03s 
Epoch 0/600 32/74 loss: 1.5284 acc [ 0.366  0.079  0.019]  time 10.36s 
Epoch 0/600 33/74 loss: 1.5276 acc [ 0.367  0.081  0.020]  time 0.04s 
Epoch 0/600 34/74 loss: 1.5297 acc [ 0.369  0.080  0.020]  time 0.03s 
Epoch 0/600 35/74 loss: 1.5294 acc [ 0.371  0.078  0.020]  time 0.03s 
Epoch 0/600 36/74 loss: 1.5444 acc [ 0.379  0.080  0.020]  time 15.19s 
Epoch 0/600 37/74 loss: 1.5409 acc [ 0.379  0.078  0.020]  time 0.04s 
Epoch 0/600 38/74 loss: 1.5495 acc [ 0.381  0.080  0.021]  time 2.29s 
Epoch 0/600 39/74 loss: 1.5468 acc [ 0.377  0.079  0.021]  time 0.03s 
Epoch 0/600 40/74 loss: 1.5442 acc [ 0.373  0.076  0.020]  time 4.25s 
Epoch 0/600 41/74 loss: 1.5407 acc [ 0.378  0.075  0.020]  time 10.04s 
Epoch 0/600 42/74 loss: 1.5370 acc [ 0.378  0.075  0.020]  time 0.03s 
Epoch 0/600 43/74 loss: 1.5362 acc [ 0.378  0.075  0.020]  time 0.03s 
Epoch 0/600 44/74 loss: 1.5337 acc [ 0.377  0.072  0.021]  time 0.03s 
Epoch 0/600 45/74 loss: 1.5326 acc [ 0.380  0.072  0.021]  time 25.37s 
Epoch 0/600 46/74 loss: 1.5303 acc [ 0.381  0.070  0.021]  time 0.03s 
Epoch 0/600 47/74 loss: 1.5272 acc [ 0.379  0.069  0.021]  time 0.03s 
Epoch 0/600 48/74 loss: 1.5258 acc [ 0.384  0.068  0.021]  time 1.70s 
Epoch 0/600 49/74 loss: 1.5232 acc [ 0.385  0.067  0.021]  time 8.05s 
Epoch 0/600 50/74 loss: 1.5201 acc [ 0.383  0.065  0.021]  time 0.03s 
Epoch 0/600 51/74 loss: 1.5199 acc [ 0.388  0.065  0.022]  time 0.03s 
Epoch 0/600 52/74 loss: 1.5166 acc [ 0.389  0.064  0.022]  time 0.04s 
Epoch 0/600 53/74 loss: 1.5139 acc [ 0.389  0.063  0.022]  time 10.27s 
Epoch 0/600 54/74 loss: 1.5137 acc [ 0.389  0.063  0.022]  time 0.04s 
Epoch 0/600 55/74 loss: 1.5114 acc [ 0.389  0.062  0.022]  time 0.03s 
Epoch 0/600 56/74 loss: 1.5201 acc [ 0.394  0.061  0.023]  time 1.23s 
Epoch 0/600 57/74 loss: 1.5191 acc [ 0.396  0.061  0.023]  time 8.50s 
Epoch 0/600 58/74 loss: 1.5180 acc [ 0.396  0.060  0.023]  time 0.04s 
Epoch 0/600 59/74 loss: 1.5166 acc [ 0.394  0.058  0.024]  time 0.04s 
Epoch 0/600 60/74 loss: 1.5164 acc [ 0.394  0.058  0.024]  time 2.53s 
Epoch 0/600 61/74 loss: 1.5155 acc [ 0.393  0.057  0.025]  time 6.26s 
Epoch 0/600 62/74 loss: 1.5129 acc [ 0.396  0.056  0.024]  time 0.04s 
Epoch 0/600 63/74 loss: 1.5131 acc [ 0.399  0.055  0.024]  time 0.03s 
Epoch 0/600 64/74 loss: 1.5109 acc [ 0.401  0.055  0.024]  time 4.12s 
Epoch 0/600 65/74 loss: 1.5116 acc [ 0.400  0.054  0.025]  time 5.07s 
Epoch 0/600 66/74 loss: 1.5139 acc [ 0.404  0.054  0.025]  time 0.04s 
Epoch 0/600 67/74 loss: 1.5117 acc [ 0.403  0.053  0.026]  time 6.92s 
Epoch 0/600 68/74 loss: 1.5104 acc [ 0.404  0.052  0.026]  time 4.57s 
Epoch 0/600 69/74 loss: 1.5086 acc [ 0.405  0.052  0.026]  time 5.28s 
Epoch 0/600 70/74 loss: 1.5067 acc [ 0.404  0.051  0.026]  time 0.04s 
Epoch 0/600 71/74 loss: 1.5055 acc [ 0.402  0.050  0.025]  time 0.04s 
Epoch 0/600 72/74 loss: 1.5034 acc [ 0.404  0.050  0.025]  time 5.99s 
Epoch 0/600 73/74 loss: 1.5079 acc [ 0.405  0.050  0.025]  time 0.37s 
Final training  0/599 loss: 1.5079 acc_avg: 0.1602 acc [ 0.405  0.050  0.025] time 269.16s  lr: 2.0000e-05
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 0, 'best_metric': -1}, save_time 0.12s
Estimated remaining training time for the current model fold 0 is 51.74 hr, running time 0.07 hr, est total time 51.81 hr 

Epoch 1/600 0/74 loss: 1.5178 acc [ 0.486  0.027  0.037]  time 4.67s 
Epoch 1/600 1/74 loss: 1.4871 acc [ 0.494  0.016  0.036]  time 0.73s 
Epoch 1/600 2/74 loss: 1.4496 acc [ 0.487  0.013  0.029]  time 0.03s 
Epoch 1/600 3/74 loss: 1.4396 acc [ 0.407  0.013  0.030]  time 0.04s 
Epoch 1/600 4/74 loss: 1.4339 acc [ 0.420  0.014  0.039]  time 1.95s 
Epoch 1/600 5/74 loss: 1.4326 acc [ 0.404  0.021  0.049]  time 1.33s 
Epoch 1/600 6/74 loss: 1.4273 acc [ 0.431  0.021  0.048]  time 1.91s 
Epoch 1/600 7/74 loss: 1.4241 acc [ 0.417  0.020  0.047]  time 0.03s 
Epoch 1/600 8/74 loss: 1.4128 acc [ 0.439  0.019  0.043]  time 1.09s 
Epoch 1/600 9/74 loss: 1.4076 acc [ 0.441  0.027  0.052]  time 0.18s 
Epoch 1/600 10/74 loss: 1.4035 acc [ 0.430  0.025  0.051]  time 0.99s 
Epoch 1/600 11/74 loss: 1.4056 acc [ 0.438  0.037  0.066]  time 0.03s 
Epoch 1/600 12/74 loss: 1.3972 acc [ 0.454  0.040  0.068]  time 1.28s 
Epoch 1/600 13/74 loss: 1.3865 acc [ 0.456  0.049  0.079]  time 0.28s 
Epoch 1/600 14/74 loss: 1.3900 acc [ 0.453  0.048  0.076]  time 0.80s 
Epoch 1/600 15/74 loss: 1.3802 acc [ 0.453  0.054  0.087]  time 0.13s 
Epoch 1/600 16/74 loss: 1.3718 acc [ 0.460  0.056  0.088]  time 2.42s 
Epoch 1/600 17/74 loss: 1.3621 acc [ 0.468  0.060  0.093]  time 0.71s 
Epoch 1/600 18/74 loss: 1.3601 acc [ 0.466  0.058  0.093]  time 0.31s 
Epoch 1/600 19/74 loss: 1.3547 acc [ 0.465  0.061  0.096]  time 0.05s 
Epoch 1/600 20/74 loss: 1.3557 acc [ 0.469  0.058  0.093]  time 2.39s 
Epoch 1/600 21/74 loss: 1.3461 acc [ 0.479  0.061  0.098]  time 0.13s 
Epoch 1/600 22/74 loss: 1.3439 acc [ 0.478  0.063  0.102]  time 0.49s 
Epoch 1/600 23/74 loss: 1.3638 acc [ 0.479  0.061  0.097]  time 0.03s 
Epoch 1/600 24/74 loss: 1.3769 acc [ 0.481  0.071  0.110]  time 1.98s 
Epoch 1/600 25/74 loss: 1.3767 acc [ 0.481  0.070  0.107]  time 0.03s 
Epoch 1/600 26/74 loss: 1.3743 acc [ 0.486  0.067  0.103]  time 2.54s 
Epoch 1/600 27/74 loss: 1.3747 acc [ 0.488  0.070  0.107]  time 0.04s 
Epoch 1/600 28/74 loss: 1.3706 acc [ 0.492  0.072  0.109]  time 1.21s 
Epoch 1/600 29/74 loss: 1.3683 acc [ 0.494  0.074  0.110]  time 0.04s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 4
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 4
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1714GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 4 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/74 loss: 1.5098 acc [ 0.388  0.087  0.126]  time 19.01s 
Epoch 0/600 1/74 loss: 1.6665 acc [ 0.482  0.144  0.163]  time 4.25s 
Epoch 0/600 2/74 loss: 1.6046 acc [ 0.413  0.136  0.158]  time 0.03s 
Epoch 0/600 3/74 loss: 1.5721 acc [ 0.402  0.121  0.142]  time 0.35s 
Epoch 0/600 4/74 loss: 1.5638 acc [ 0.396  0.109  0.134]  time 0.73s 
Epoch 0/600 5/74 loss: 1.5480 acc [ 0.371  0.105  0.133]  time 7.05s 
Epoch 0/600 6/74 loss: 1.5430 acc [ 0.379  0.097  0.122]  time 0.03s 
Epoch 0/600 7/74 loss: 1.5438 acc [ 0.375  0.103  0.134]  time 1.21s 
Epoch 0/600 8/74 loss: 1.5380 acc [ 0.362  0.104  0.133]  time 0.81s 
Epoch 0/600 9/74 loss: 1.5362 acc [ 0.378  0.100  0.128]  time 6.86s 
Epoch 0/600 10/74 loss: 1.5325 acc [ 0.368  0.097  0.125]  time 0.03s 
Epoch 0/600 11/74 loss: 1.5258 acc [ 0.353  0.093  0.125]  time 0.03s 
Epoch 0/600 12/74 loss: 1.5209 acc [ 0.347  0.093  0.126]  time 16.04s 
Epoch 0/600 13/74 loss: 1.5152 acc [ 0.350  0.093  0.127]  time 0.03s 
Epoch 0/600 14/74 loss: 1.5108 acc [ 0.358  0.093  0.127]  time 0.03s 
Epoch 0/600 15/74 loss: 1.5130 acc [ 0.362  0.096  0.127]  time 0.03s 
Epoch 0/600 16/74 loss: 1.5154 acc [ 0.363  0.098  0.128]  time 6.26s 
Epoch 0/600 17/74 loss: 1.5193 acc [ 0.377  0.100  0.134]  time 3.43s 
Epoch 0/600 18/74 loss: 1.5174 acc [ 0.378  0.100  0.136]  time 5.80s 
Epoch 0/600 19/74 loss: 1.5134 acc [ 0.369  0.096  0.135]  time 0.03s 
Epoch 0/600 20/74 loss: 1.5198 acc [ 0.371  0.100  0.138]  time 3.13s 
Epoch 0/600 21/74 loss: 1.5157 acc [ 0.375  0.096  0.135]  time 1.99s 
Epoch 0/600 22/74 loss: 1.5117 acc [ 0.377  0.095  0.132]  time 16.48s 
Epoch 0/600 23/74 loss: 1.5234 acc [ 0.386  0.103  0.135]  time 0.03s 
Epoch 0/600 24/74 loss: 1.5209 acc [ 0.383  0.103  0.136]  time 0.03s 
Epoch 0/600 25/74 loss: 1.5256 acc [ 0.386  0.102  0.135]  time 0.93s 
Epoch 0/600 26/74 loss: 1.5218 acc [ 0.380  0.101  0.134]  time 10.38s 
Epoch 0/600 27/74 loss: 1.5177 acc [ 0.376  0.098  0.130]  time 0.03s 
Epoch 0/600 28/74 loss: 1.5135 acc [ 0.378  0.097  0.128]  time 4.47s 
Epoch 0/600 29/74 loss: 1.5109 acc [ 0.374  0.098  0.129]  time 0.04s 
Epoch 0/600 30/74 loss: 1.5098 acc [ 0.378  0.099  0.132]  time 18.70s 
Epoch 0/600 31/74 loss: 1.5093 acc [ 0.378  0.100  0.131]  time 0.03s 
Epoch 0/600 32/74 loss: 1.5075 acc [ 0.377  0.099  0.131]  time 0.03s 
Epoch 0/600 33/74 loss: 1.5061 acc [ 0.371  0.099  0.128]  time 0.03s 
Epoch 0/600 34/74 loss: 1.5050 acc [ 0.369  0.101  0.127]  time 6.87s 
Epoch 0/600 35/74 loss: 1.5027 acc [ 0.365  0.098  0.122]  time 0.03s 
Epoch 0/600 36/74 loss: 1.4997 acc [ 0.367  0.097  0.119]  time 0.03s 
Epoch 0/600 37/74 loss: 1.4974 acc [ 0.367  0.097  0.119]  time 0.03s 
Epoch 0/600 38/74 loss: 1.5018 acc [ 0.373  0.098  0.119]  time 20.03s 
Epoch 0/600 39/74 loss: 1.4997 acc [ 0.372  0.097  0.118]  time 0.03s 
Epoch 0/600 40/74 loss: 1.4964 acc [ 0.372  0.095  0.117]  time 0.03s 
Epoch 0/600 41/74 loss: 1.4938 acc [ 0.372  0.093  0.114]  time 0.03s 
Epoch 0/600 42/74 loss: 1.4910 acc [ 0.372  0.092  0.114]  time 9.21s 
Epoch 0/600 43/74 loss: 1.4882 acc [ 0.379  0.090  0.111]  time 0.70s 
Epoch 0/600 44/74 loss: 1.4884 acc [ 0.387  0.089  0.112]  time 10.72s 
Epoch 0/600 45/74 loss: 1.4871 acc [ 0.387  0.089  0.111]  time 0.03s 
Epoch 0/600 46/74 loss: 1.4859 acc [ 0.384  0.089  0.111]  time 0.22s 
Epoch 0/600 47/74 loss: 1.4980 acc [ 0.388  0.088  0.110]  time 1.63s 
Epoch 0/600 48/74 loss: 1.4968 acc [ 0.385  0.087  0.109]  time 4.60s 
Epoch 0/600 49/74 loss: 1.4937 acc [ 0.385  0.086  0.107]  time 0.03s 
Epoch 0/600 50/74 loss: 1.4927 acc [ 0.385  0.086  0.106]  time 2.66s 
Epoch 0/600 51/74 loss: 1.4903 acc [ 0.384  0.085  0.106]  time 5.92s 
Epoch 0/600 52/74 loss: 1.4910 acc [ 0.384  0.084  0.107]  time 3.67s 
Epoch 0/600 53/74 loss: 1.4876 acc [ 0.385  0.084  0.105]  time 0.04s 
Epoch 0/600 54/74 loss: 1.4859 acc [ 0.383  0.084  0.104]  time 0.03s 
Epoch 0/600 55/74 loss: 1.4829 acc [ 0.386  0.084  0.104]  time 6.09s 
Epoch 0/600 56/74 loss: 1.4808 acc [ 0.385  0.084  0.104]  time 2.20s 
Epoch 0/600 57/74 loss: 1.4780 acc [ 0.385  0.083  0.103]  time 0.03s 
Epoch 0/600 58/74 loss: 1.4761 acc [ 0.386  0.083  0.102]  time 0.43s 
Epoch 0/600 59/74 loss: 1.4732 acc [ 0.385  0.082  0.101]  time 16.44s 
Epoch 0/600 60/74 loss: 1.4715 acc [ 0.383  0.081  0.101]  time 0.04s 
Epoch 0/600 61/74 loss: 1.4708 acc [ 0.390  0.083  0.101]  time 0.04s 
Epoch 0/600 62/74 loss: 1.4706 acc [ 0.391  0.083  0.102]  time 0.04s 
Epoch 0/600 63/74 loss: 1.4680 acc [ 0.392  0.082  0.101]  time 18.65s 
Epoch 0/600 64/74 loss: 1.4659 acc [ 0.392  0.082  0.100]  time 0.03s 
Epoch 0/600 65/74 loss: 1.4652 acc [ 0.392  0.082  0.102]  time 0.03s 
Epoch 0/600 66/74 loss: 1.4632 acc [ 0.396  0.081  0.101]  time 0.03s 
Epoch 0/600 67/74 loss: 1.4616 acc [ 0.400  0.082  0.101]  time 11.58s 
Epoch 0/600 68/74 loss: 1.4599 acc [ 0.404  0.082  0.101]  time 0.03s 
Epoch 0/600 69/74 loss: 1.4593 acc [ 0.408  0.082  0.102]  time 0.03s 
Epoch 0/600 70/74 loss: 1.4575 acc [ 0.409  0.082  0.102]  time 0.03s 
Epoch 0/600 71/74 loss: 1.4578 acc [ 0.410  0.081  0.102]  time 16.94s 
Epoch 0/600 72/74 loss: 1.4562 acc [ 0.411  0.081  0.101]  time 0.03s 
Epoch 0/600 73/74 loss: 1.4557 acc [ 0.411  0.081  0.101]  time 0.25s 
Final training  0/599 loss: 1.4557 acc_avg: 0.1977 acc [ 0.411  0.081  0.101] time 267.94s  lr: 2.0000e-05
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 0, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 51.50 hr, running time 0.07 hr, est total time 51.58 hr 

Epoch 1/600 0/74 loss: 1.3939 acc [ 0.320  0.000  0.000]  time 3.12s 
Epoch 1/600 1/74 loss: 1.4008 acc [ 0.432  0.005  0.003]  time 0.27s 
Epoch 1/600 2/74 loss: 1.3740 acc [ 0.531  0.053  0.059]  time 0.03s 
Epoch 1/600 3/74 loss: 1.3532 acc [ 0.595  0.069  0.070]  time 2.99s 
Epoch 1/600 4/74 loss: 1.4816 acc [ 0.636  0.090  0.101]  time 1.02s 
Epoch 1/600 5/74 loss: 1.4419 acc [ 0.644  0.124  0.128]  time 0.05s 
Epoch 1/600 6/74 loss: 1.4083 acc [ 0.596  0.144  0.149]  time 0.03s 
Epoch 1/600 7/74 loss: 1.3873 acc [ 0.618  0.188  0.191]  time 4.73s 
Epoch 1/600 8/74 loss: 1.3747 acc [ 0.605  0.204  0.203]  time 0.06s 
Epoch 1/600 9/74 loss: 1.3683 acc [ 0.590  0.198  0.196]  time 0.03s 
Epoch 1/600 10/74 loss: 1.3663 acc [ 0.569  0.183  0.179]  time 0.05s 
Epoch 1/600 11/74 loss: 1.3831 acc [ 0.558  0.174  0.170]  time 4.09s 
Epoch 1/600 12/74 loss: 1.3979 acc [ 0.537  0.160  0.155]  time 0.11s 
Epoch 1/600 13/74 loss: 1.3971 acc [ 0.523  0.149  0.143]  time 1.12s 
Epoch 1/600 14/74 loss: 1.3813 acc [ 0.532  0.156  0.149]  time 0.03s 
Epoch 1/600 15/74 loss: 1.4017 acc [ 0.528  0.156  0.147]  time 3.05s 
Epoch 1/600 16/74 loss: 1.3955 acc [ 0.523  0.151  0.147]  time 0.03s 
Epoch 1/600 17/74 loss: 1.4025 acc [ 0.533  0.167  0.162]  time 0.03s 
Epoch 1/600 18/74 loss: 1.4043 acc [ 0.538  0.167  0.162]  time 0.04s 
Epoch 1/600 19/74 loss: 1.3983 acc [ 0.521  0.154  0.149]  time 3.43s 
Epoch 1/600 20/74 loss: 1.3875 acc [ 0.524  0.157  0.152]  time 0.03s 
Epoch 1/600 21/74 loss: 1.3854 acc [ 0.525  0.157  0.152]  time 1.47s 
Epoch 1/600 22/74 loss: 1.3848 acc [ 0.520  0.160  0.156]  time 0.07s 
Epoch 1/600 23/74 loss: 1.3842 acc [ 0.511  0.160  0.156]  time 1.97s 
Epoch 1/600 24/74 loss: 1.3822 acc [ 0.511  0.152  0.151]  time 0.04s 
Epoch 1/600 25/74 loss: 1.3769 acc [ 0.515  0.154  0.154]  time 0.03s 
Epoch 1/600 26/74 loss: 1.3727 acc [ 0.515  0.153  0.150]  time 0.03s 
Epoch 1/600 27/74 loss: 1.3687 acc [ 0.520  0.148  0.148]  time 4.57s 
Epoch 1/600 28/74 loss: 1.3636 acc [ 0.522  0.141  0.142]  time 0.03s 
Epoch 1/600 29/74 loss: 1.3573 acc [ 0.526  0.146  0.147]  time 1.33s 
Epoch 1/600 30/74 loss: 1.3551 acc [ 0.526  0.145  0.146]  time 0.03s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 4
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 8
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1735GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 4
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 8
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1733GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 4 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/74 loss: 1.5474 acc [ 0.105  0.013  0.048]  time 7.50s 
Epoch 0/600 1/74 loss: 1.5898 acc [ 0.144  0.058  0.140]  time 3.77s 
Epoch 0/600 2/74 loss: 1.6162 acc [ 0.175  0.087  0.168]  time 4.16s 
Epoch 0/600 3/74 loss: 1.6457 acc [ 0.189  0.077  0.150]  time 8.15s 
Epoch 0/600 4/74 loss: 1.6271 acc [ 0.192  0.074  0.158]  time 0.04s 
Epoch 0/600 5/74 loss: 1.6120 acc [ 0.198  0.061  0.138]  time 0.04s 
Epoch 0/600 6/74 loss: 1.6068 acc [ 0.209  0.061  0.135]  time 0.03s 
Epoch 0/600 7/74 loss: 1.5983 acc [ 0.205  0.060  0.127]  time 0.03s 
Epoch 0/600 8/74 loss: 1.5875 acc [ 0.208  0.061  0.133]  time 0.03s 
Epoch 0/600 9/74 loss: 1.5832 acc [ 0.219  0.058  0.127]  time 2.28s 
Epoch 0/600 10/74 loss: 1.5803 acc [ 0.230  0.056  0.132]  time 5.87s 
Epoch 0/600 11/74 loss: 1.5702 acc [ 0.233  0.054  0.127]  time 0.03s 
Epoch 0/600 12/74 loss: 1.5628 acc [ 0.237  0.054  0.128]  time 3.12s 
Epoch 0/600 13/74 loss: 1.5553 acc [ 0.237  0.054  0.125]  time 0.03s 
Epoch 0/600 14/74 loss: 1.5681 acc [ 0.260  0.058  0.133]  time 0.03s 
Epoch 0/600 15/74 loss: 1.5649 acc [ 0.273  0.061  0.132]  time 0.04s 
Epoch 0/600 16/74 loss: 1.5599 acc [ 0.275  0.057  0.125]  time 0.03s 
Epoch 0/600 17/74 loss: 1.5564 acc [ 0.272  0.057  0.123]  time 3.59s 
Epoch 0/600 18/74 loss: 1.5530 acc [ 0.280  0.056  0.123]  time 9.60s 
Epoch 0/600 19/74 loss: 1.5486 acc [ 0.279  0.053  0.116]  time 0.04s 
Epoch 0/600 20/74 loss: 1.5447 acc [ 0.283  0.052  0.112]  time 0.04s 
Epoch 0/600 21/74 loss: 1.5433 acc [ 0.290  0.052  0.109]  time 0.03s 
Epoch 0/600 22/74 loss: 1.5390 acc [ 0.292  0.051  0.107]  time 0.03s 
Epoch 0/600 23/74 loss: 1.5392 acc [ 0.291  0.053  0.112]  time 0.03s 
Epoch 0/600 24/74 loss: 1.5364 acc [ 0.299  0.056  0.112]  time 0.03s 
Epoch 0/600 25/74 loss: 1.5329 acc [ 0.304  0.055  0.109]  time 0.03s 
Epoch 0/600 26/74 loss: 1.5288 acc [ 0.305  0.054  0.107]  time 5.26s 
Epoch 0/600 27/74 loss: 1.5250 acc [ 0.313  0.053  0.105]  time 10.09s 
Epoch 0/600 28/74 loss: 1.5358 acc [ 0.322  0.055  0.111]  time 0.03s 
Epoch 0/600 29/74 loss: 1.5446 acc [ 0.325  0.055  0.113]  time 0.03s 
Epoch 0/600 30/74 loss: 1.5409 acc [ 0.325  0.053  0.108]  time 0.03s 
Epoch 0/600 31/74 loss: 1.5450 acc [ 0.327  0.054  0.109]  time 0.03s 
Epoch 0/600 32/74 loss: 1.5443 acc [ 0.333  0.053  0.108]  time 0.03s 
Epoch 0/600 33/74 loss: 1.5503 acc [ 0.337  0.055  0.110]  time 0.03s 
Epoch 0/600 34/74 loss: 1.5450 acc [ 0.340  0.054  0.108]  time 2.51s 
Epoch 0/600 35/74 loss: 1.5430 acc [ 0.344  0.055  0.108]  time 22.05s 
Epoch 0/600 36/74 loss: 1.5403 acc [ 0.346  0.055  0.109]  time 0.03s 
Epoch 0/600 37/74 loss: 1.5471 acc [ 0.347  0.056  0.108]  time 0.03s 
Epoch 0/600 38/74 loss: 1.5438 acc [ 0.344  0.054  0.104]  time 0.03s 
Epoch 0/600 39/74 loss: 1.5481 acc [ 0.346  0.056  0.105]  time 0.03s 
Epoch 0/600 40/74 loss: 1.5465 acc [ 0.345  0.055  0.104]  time 0.03s 
Epoch 0/600 41/74 loss: 1.5441 acc [ 0.347  0.054  0.103]  time 0.03s 
Epoch 0/600 42/74 loss: 1.5409 acc [ 0.348  0.054  0.103]  time 0.03s 
Epoch 0/600 43/74 loss: 1.5386 acc [ 0.349  0.055  0.103]  time 11.90s 
Epoch 0/600 44/74 loss: 1.5365 acc [ 0.351  0.054  0.101]  time 0.03s 
Epoch 0/600 45/74 loss: 1.5329 acc [ 0.354  0.054  0.099]  time 0.03s 
Epoch 0/600 46/74 loss: 1.5303 acc [ 0.356  0.054  0.099]  time 0.03s 
Epoch 0/600 47/74 loss: 1.5285 acc [ 0.358  0.055  0.100]  time 0.03s 
Epoch 0/600 48/74 loss: 1.5292 acc [ 0.359  0.055  0.101]  time 0.03s 
Epoch 0/600 49/74 loss: 1.5262 acc [ 0.358  0.056  0.101]  time 0.03s 
Epoch 0/600 50/74 loss: 1.5237 acc [ 0.358  0.056  0.101]  time 0.03s 
Epoch 0/600 51/74 loss: 1.5247 acc [ 0.362  0.056  0.103]  time 18.48s 
Epoch 0/600 52/74 loss: 1.5222 acc [ 0.363  0.056  0.102]  time 0.03s 
Epoch 0/600 53/74 loss: 1.5307 acc [ 0.368  0.058  0.102]  time 0.03s 
Epoch 0/600 54/74 loss: 1.5278 acc [ 0.368  0.058  0.101]  time 0.03s 
Epoch 0/600 55/74 loss: 1.5268 acc [ 0.365  0.058  0.101]  time 0.03s 
Epoch 0/600 56/74 loss: 1.5251 acc [ 0.367  0.058  0.100]  time 0.03s 
Epoch 0/600 57/74 loss: 1.5227 acc [ 0.367  0.058  0.100]  time 0.03s 
Epoch 0/600 58/74 loss: 1.5227 acc [ 0.367  0.058  0.099]  time 0.03s 
Epoch 0/600 59/74 loss: 1.5197 acc [ 0.369  0.058  0.099]  time 12.89s 
Epoch 0/600 60/74 loss: 1.5227 acc [ 0.371  0.059  0.100]  time 0.03s 
Epoch 0/600 61/74 loss: 1.5199 acc [ 0.371  0.059  0.100]  time 0.03s 
Epoch 0/600 62/74 loss: 1.5179 acc [ 0.375  0.059  0.099]  time 0.03s 
Epoch 0/600 63/74 loss: 1.5205 acc [ 0.377  0.060  0.099]  time 0.03s 
Epoch 0/600 64/74 loss: 1.5203 acc [ 0.379  0.060  0.099]  time 13.76s 
Epoch 0/600 65/74 loss: 1.5183 acc [ 0.378  0.060  0.099]  time 0.03s 
Epoch 0/600 66/74 loss: 1.5170 acc [ 0.382  0.059  0.098]  time 0.03s 
Epoch 0/600 67/74 loss: 1.5161 acc [ 0.383  0.061  0.099]  time 0.03s 
Epoch 0/600 68/74 loss: 1.5141 acc [ 0.386  0.061  0.099]  time 0.03s 
Epoch 0/600 69/74 loss: 1.5124 acc [ 0.387  0.061  0.099]  time 0.03s 
Epoch 0/600 70/74 loss: 1.5101 acc [ 0.386  0.061  0.099]  time 0.03s 
Epoch 0/600 71/74 loss: 1.5076 acc [ 0.386  0.062  0.099]  time 0.03s 
Epoch 0/600 72/74 loss: 1.5053 acc [ 0.390  0.061  0.098]  time 28.22s 
Epoch 0/600 73/74 loss: 1.5047 acc [ 0.391  0.061  0.098]  time 0.27s 
Final training  0/599 loss: 1.5047 acc_avg: 0.1831 acc [ 0.391  0.061  0.098] time 175.36s  lr: 2.0000e-05
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 0, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 33.71 hr, running time 0.05 hr, est total time 33.76 hr 

Epoch 1/600 0/74 loss: 1.3136 acc [ 0.449  0.059  0.074]  time 10.03s 
Epoch 1/600 1/74 loss: 1.3379 acc [ 0.638  0.067  0.050]  time 4.63s 
Epoch 1/600 2/74 loss: 1.3412 acc [ 0.583  0.059  0.050]  time 0.03s 
Epoch 1/600 3/74 loss: 1.3546 acc [ 0.588  0.064  0.053]  time 0.06s 
Epoch 1/600 4/74 loss: 1.3415 acc [ 0.578  0.066  0.055]  time 0.03s 
Epoch 1/600 5/74 loss: 1.3377 acc [ 0.554  0.066  0.055]  time 0.04s 
Epoch 1/600 6/74 loss: 1.3218 acc [ 0.586  0.069  0.061]  time 0.04s 
Epoch 1/600 7/74 loss: 1.4520 acc [ 0.587  0.057  0.051]  time 0.03s 
Epoch 1/600 8/74 loss: 1.4887 acc [ 0.596  0.050  0.052]  time 1.38s 
Epoch 1/600 9/74 loss: 1.4812 acc [ 0.568  0.052  0.053]  time 2.49s 
Epoch 1/600 10/74 loss: 1.4699 acc [ 0.537  0.049  0.049]  time 0.04s 
Epoch 1/600 11/74 loss: 1.4566 acc [ 0.551  0.056  0.056]  time 0.03s 
Epoch 1/600 12/74 loss: 1.4371 acc [ 0.551  0.063  0.062]  time 1.27s 
Epoch 1/600 13/74 loss: 1.4270 acc [ 0.538  0.063  0.062]  time 0.03s 
Epoch 1/600 14/74 loss: 1.4181 acc [ 0.528  0.087  0.082]  time 0.03s 
Epoch 1/600 15/74 loss: 1.4028 acc [ 0.533  0.098  0.090]  time 0.05s 
Epoch 1/600 16/74 loss: 1.3925 acc [ 0.536  0.103  0.098]  time 1.79s 
Epoch 1/600 17/74 loss: 1.3827 acc [ 0.529  0.105  0.104]  time 0.04s 
Epoch 1/600 18/74 loss: 1.4024 acc [ 0.532  0.102  0.100]  time 0.03s 
Epoch 1/600 19/74 loss: 1.4046 acc [ 0.518  0.097  0.092]  time 4.17s 
Epoch 1/600 20/74 loss: 1.3940 acc [ 0.527  0.099  0.092]  time 2.50s 
Epoch 1/600 21/74 loss: 1.3922 acc [ 0.523  0.099  0.092]  time 0.04s 
Epoch 1/600 22/74 loss: 1.3940 acc [ 0.528  0.095  0.090]  time 0.06s 
Epoch 1/600 23/74 loss: 1.3987 acc [ 0.521  0.092  0.087]  time 0.04s 
Epoch 1/600 24/74 loss: 1.3948 acc [ 0.515  0.090  0.085]  time 0.06s 
Epoch 1/600 25/74 loss: 1.3865 acc [ 0.515  0.091  0.085]  time 0.03s 
Epoch 1/600 26/74 loss: 1.3801 acc [ 0.517  0.099  0.094]  time 0.04s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 4
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 8
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1709GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 4 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/74 loss: 1.7991 acc [ 0.107  0.165  0.159]  time 9.99s 
Epoch 0/600 1/74 loss: 1.7569 acc [ 0.206  0.133  0.159]  time 11.64s 
Epoch 0/600 2/74 loss: 1.7981 acc [ 0.254  0.198  0.232]  time 9.73s 
Epoch 0/600 3/74 loss: 1.7350 acc [ 0.295  0.175  0.232]  time 4.63s 
Epoch 0/600 4/74 loss: 1.8447 acc [ 0.303  0.213  0.242]  time 0.03s 
Epoch 0/600 5/74 loss: 1.7869 acc [ 0.295  0.213  0.242]  time 0.03s 
Epoch 0/600 6/74 loss: 1.7645 acc [ 0.268  0.183  0.209]  time 0.03s 
Epoch 0/600 7/74 loss: 1.7549 acc [ 0.285  0.173  0.193]  time 0.03s 
Epoch 0/600 8/74 loss: 1.7346 acc [ 0.280  0.165  0.180]  time 0.03s 
Epoch 0/600 9/74 loss: 1.7184 acc [ 0.294  0.163  0.168]  time 0.70s 
Epoch 0/600 10/74 loss: 1.6991 acc [ 0.272  0.163  0.168]  time 5.31s 
Epoch 0/600 11/74 loss: 1.7095 acc [ 0.275  0.169  0.171]  time 10.97s 
Epoch 0/600 12/74 loss: 1.7061 acc [ 0.294  0.166  0.164]  time 0.03s 
Epoch 0/600 13/74 loss: 1.7070 acc [ 0.298  0.175  0.176]  time 0.03s 
Epoch 0/600 14/74 loss: 1.6963 acc [ 0.303  0.172  0.173]  time 0.03s 
Epoch 0/600 15/74 loss: 1.6895 acc [ 0.306  0.164  0.171]  time 0.03s 
Epoch 0/600 16/74 loss: 1.6860 acc [ 0.311  0.156  0.162]  time 0.03s 
Epoch 0/600 17/74 loss: 1.6812 acc [ 0.313  0.156  0.158]  time 0.03s 
Epoch 0/600 18/74 loss: 1.6723 acc [ 0.310  0.150  0.155]  time 1.16s 
Epoch 0/600 19/74 loss: 1.6714 acc [ 0.321  0.143  0.147]  time 16.40s 
Epoch 0/600 20/74 loss: 1.6631 acc [ 0.313  0.140  0.143]  time 0.03s 
Epoch 0/600 21/74 loss: 1.6569 acc [ 0.316  0.133  0.135]  time 0.03s 
Epoch 0/600 22/74 loss: 1.6602 acc [ 0.323  0.139  0.144]  time 0.03s 
Epoch 0/600 23/74 loss: 1.6516 acc [ 0.326  0.137  0.144]  time 0.03s 
Epoch 0/600 24/74 loss: 1.6534 acc [ 0.335  0.140  0.149]  time 0.03s 
Epoch 0/600 25/74 loss: 1.6471 acc [ 0.332  0.140  0.149]  time 0.03s 
Epoch 0/600 26/74 loss: 1.6406 acc [ 0.325  0.134  0.142]  time 0.03s 
Epoch 0/600 27/74 loss: 1.6361 acc [ 0.327  0.130  0.140]  time 11.72s 
Epoch 0/600 28/74 loss: 1.6309 acc [ 0.329  0.129  0.137]  time 0.03s 
Epoch 0/600 29/74 loss: 1.6312 acc [ 0.335  0.128  0.136]  time 0.03s 
Epoch 0/600 30/74 loss: 1.6270 acc [ 0.335  0.128  0.136]  time 0.03s 
Epoch 0/600 31/74 loss: 1.6206 acc [ 0.338  0.128  0.135]  time 0.03s 
Epoch 0/600 32/74 loss: 1.6159 acc [ 0.345  0.126  0.135]  time 0.03s 
Epoch 0/600 33/74 loss: 1.6126 acc [ 0.353  0.123  0.134]  time 0.03s 
Epoch 0/600 34/74 loss: 1.6085 acc [ 0.357  0.121  0.134]  time 0.03s 
Epoch 0/600 35/74 loss: 1.6087 acc [ 0.354  0.122  0.134]  time 5.44s 
Epoch 0/600 36/74 loss: 1.6077 acc [ 0.360  0.121  0.133]  time 0.03s 
Epoch 0/600 37/74 loss: 1.6038 acc [ 0.359  0.120  0.131]  time 0.03s 
Epoch 0/600 38/74 loss: 1.6004 acc [ 0.358  0.120  0.131]  time 0.03s 
Epoch 0/600 39/74 loss: 1.5976 acc [ 0.365  0.119  0.130]  time 14.76s 
Epoch 0/600 40/74 loss: 1.5933 acc [ 0.368  0.116  0.128]  time 0.03s 
Epoch 0/600 41/74 loss: 1.5909 acc [ 0.370  0.117  0.129]  time 0.03s 
Epoch 0/600 42/74 loss: 1.5879 acc [ 0.376  0.116  0.128]  time 0.03s 
Epoch 0/600 43/74 loss: 1.5886 acc [ 0.374  0.115  0.127]  time 0.03s 
Epoch 0/600 44/74 loss: 1.5844 acc [ 0.372  0.115  0.127]  time 0.03s 
Epoch 0/600 45/74 loss: 1.5896 acc [ 0.372  0.118  0.129]  time 0.03s 
Epoch 0/600 46/74 loss: 1.5864 acc [ 0.370  0.117  0.129]  time 0.03s 
Epoch 0/600 47/74 loss: 1.5821 acc [ 0.373  0.116  0.128]  time 8.14s 
Epoch 0/600 48/74 loss: 1.5793 acc [ 0.372  0.116  0.128]  time 0.03s 
Epoch 0/600 49/74 loss: 1.5769 acc [ 0.367  0.113  0.124]  time 0.03s 
Epoch 0/600 50/74 loss: 1.5771 acc [ 0.369  0.111  0.124]  time 0.03s 
Epoch 0/600 51/74 loss: 1.5734 acc [ 0.368  0.110  0.123]  time 0.03s 
Epoch 0/600 52/74 loss: 1.5698 acc [ 0.371  0.109  0.123]  time 0.03s 
Epoch 0/600 53/74 loss: 1.5672 acc [ 0.373  0.109  0.122]  time 0.03s 
Epoch 0/600 54/74 loss: 1.5648 acc [ 0.372  0.108  0.122]  time 0.03s 
Epoch 0/600 55/74 loss: 1.5626 acc [ 0.371  0.107  0.122]  time 7.55s 
Epoch 0/600 56/74 loss: 1.5602 acc [ 0.373  0.105  0.121]  time 0.03s 
Epoch 0/600 57/74 loss: 1.5575 acc [ 0.372  0.103  0.119]  time 0.03s 
Epoch 0/600 58/74 loss: 1.5551 acc [ 0.371  0.103  0.118]  time 0.03s 
Epoch 0/600 59/74 loss: 1.5520 acc [ 0.372  0.102  0.117]  time 13.95s 
Epoch 0/600 60/74 loss: 1.5485 acc [ 0.372  0.101  0.117]  time 0.04s 
Epoch 0/600 61/74 loss: 1.5485 acc [ 0.374  0.101  0.118]  time 0.04s 
Epoch 0/600 62/74 loss: 1.5445 acc [ 0.375  0.101  0.118]  time 0.04s 
Epoch 0/600 63/74 loss: 1.5422 acc [ 0.376  0.100  0.117]  time 0.52s 
Epoch 0/600 64/74 loss: 1.5492 acc [ 0.379  0.100  0.118]  time 0.04s 
Epoch 0/600 65/74 loss: 1.5456 acc [ 0.383  0.100  0.117]  time 2.21s 
Epoch 0/600 66/74 loss: 1.5445 acc [ 0.383  0.098  0.115]  time 0.03s 
Epoch 0/600 67/74 loss: 1.5429 acc [ 0.384  0.096  0.114]  time 4.15s 
Epoch 0/600 68/74 loss: 1.5405 acc [ 0.386  0.096  0.115]  time 1.74s 
Epoch 0/600 69/74 loss: 1.5387 acc [ 0.385  0.095  0.115]  time 0.03s 
Epoch 0/600 70/74 loss: 1.5367 acc [ 0.386  0.095  0.114]  time 0.03s 
Epoch 0/600 71/74 loss: 1.5333 acc [ 0.387  0.095  0.114]  time 7.21s 
Epoch 0/600 72/74 loss: 1.5304 acc [ 0.389  0.094  0.115]  time 0.03s 
Epoch 0/600 73/74 loss: 1.5303 acc [ 0.389  0.094  0.115]  time 0.51s 
Final training  0/599 loss: 1.5303 acc_avg: 0.1993 acc [ 0.389  0.094  0.115] time 150.23s  lr: 2.0000e-05
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 0, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 28.88 hr, running time 0.04 hr, est total time 28.92 hr 

Epoch 1/600 0/74 loss: 1.6358 acc [ 0.367  0.082  0.181]  time 7.62s 
Epoch 1/600 1/74 loss: 1.5017 acc [ 0.439  0.068  0.138]  time 0.04s 
Epoch 1/600 2/74 loss: 1.4356 acc [ 0.493  0.050  0.121]  time 4.56s 
Epoch 1/600 3/74 loss: 1.4197 acc [ 0.462  0.065  0.113]  time 0.03s 
Epoch 1/600 4/74 loss: 1.4064 acc [ 0.450  0.061  0.098]  time 0.03s 
Epoch 1/600 5/74 loss: 1.4059 acc [ 0.475  0.051  0.088]  time 0.04s 
Epoch 1/600 6/74 loss: 1.3988 acc [ 0.476  0.047  0.082]  time 0.03s 
Epoch 1/600 7/74 loss: 1.3981 acc [ 0.512  0.063  0.107]  time 4.17s 
Epoch 1/600 8/74 loss: 1.3976 acc [ 0.488  0.065  0.103]  time 0.03s 
Epoch 1/600 9/74 loss: 1.3896 acc [ 0.487  0.058  0.087]  time 0.05s 
Epoch 1/600 10/74 loss: 1.3842 acc [ 0.497  0.054  0.084]  time 0.53s 
Epoch 1/600 11/74 loss: 1.3834 acc [ 0.494  0.055  0.082]  time 0.03s 
Epoch 1/600 12/74 loss: 1.3770 acc [ 0.495  0.053  0.082]  time 0.06s 
Epoch 1/600 13/74 loss: 1.3684 acc [ 0.504  0.054  0.082]  time 0.03s 
Epoch 1/600 14/74 loss: 1.3823 acc [ 0.492  0.055  0.082]  time 0.05s 
Epoch 1/600 15/74 loss: 1.3812 acc [ 0.479  0.054  0.078]  time 3.40s 
Epoch 1/600 16/74 loss: 1.3706 acc [ 0.485  0.068  0.099]  time 1.44s 
Epoch 1/600 17/74 loss: 1.3637 acc [ 0.501  0.068  0.096]  time 0.03s 
Epoch 1/600 18/74 loss: 1.3595 acc [ 0.517  0.067  0.097]  time 6.60s 
Epoch 1/600 19/74 loss: 1.3623 acc [ 0.530  0.082  0.113]  time 0.05s 
Epoch 1/600 20/74 loss: 1.3608 acc [ 0.525  0.084  0.114]  time 0.03s 
Epoch 1/600 21/74 loss: 1.3555 acc [ 0.529  0.083  0.112]  time 0.05s 
Epoch 1/600 22/74 loss: 1.3518 acc [ 0.537  0.080  0.109]  time 0.03s 
Epoch 1/600 23/74 loss: 1.3523 acc [ 0.531  0.082  0.110]  time 0.04s 
Epoch 1/600 24/74 loss: 1.3575 acc [ 0.525  0.082  0.110]  time 0.04s 
Epoch 1/600 25/74 loss: 1.3537 acc [ 0.523  0.083  0.111]  time 0.03s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 4
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: 0.5
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 8
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Using user specified cache_rate=0.5 to cache data in RAM
Prioritizing cache_rate training 0.8344709897610921 validation 0
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 4 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/74 loss: 1.5405 acc [ 0.235  0.004  0.005]  time 10.17s 
Epoch 0/600 1/74 loss: 1.6281 acc [ 0.342  0.008  0.027]  time 7.60s 
Epoch 0/600 2/74 loss: 1.5873 acc [ 0.331  0.043  0.063]  time 0.03s 
Epoch 0/600 3/74 loss: 1.5754 acc [ 0.326  0.038  0.056]  time 0.03s 
Epoch 0/600 4/74 loss: 1.6386 acc [ 0.378  0.066  0.125]  time 0.03s 
Epoch 0/600 5/74 loss: 1.6370 acc [ 0.394  0.077  0.140]  time 0.03s 
Epoch 0/600 6/74 loss: 1.6175 acc [ 0.380  0.071  0.123]  time 2.67s 
Epoch 0/600 7/74 loss: 1.6049 acc [ 0.376  0.067  0.110]  time 2.59s 
Epoch 0/600 8/74 loss: 1.5911 acc [ 0.370  0.065  0.104]  time 0.03s 
Epoch 0/600 9/74 loss: 1.6027 acc [ 0.379  0.072  0.125]  time 3.62s 
Epoch 0/600 10/74 loss: 1.5895 acc [ 0.381  0.072  0.125]  time 0.03s 
Epoch 0/600 11/74 loss: 1.5817 acc [ 0.374  0.076  0.132]  time 0.03s 
Epoch 0/600 12/74 loss: 1.5876 acc [ 0.383  0.075  0.144]  time 0.03s 
Epoch 0/600 13/74 loss: 1.5887 acc [ 0.388  0.082  0.150]  time 0.03s 
Epoch 0/600 14/74 loss: 1.5815 acc [ 0.374  0.079  0.141]  time 0.03s 
Epoch 0/600 15/74 loss: 1.5756 acc [ 0.376  0.087  0.144]  time 3.96s 
Epoch 0/600 16/74 loss: 1.5813 acc [ 0.372  0.089  0.149]  time 0.03s 
Epoch 0/600 17/74 loss: 1.5760 acc [ 0.377  0.087  0.150]  time 7.51s 
Epoch 0/600 18/74 loss: 1.5711 acc [ 0.383  0.087  0.146]  time 0.03s 
Epoch 0/600 19/74 loss: 1.5776 acc [ 0.393  0.093  0.155]  time 0.03s 
Epoch 0/600 20/74 loss: 1.5723 acc [ 0.398  0.093  0.150]  time 0.04s 
Epoch 0/600 21/74 loss: 1.5669 acc [ 0.398  0.091  0.145]  time 0.03s 
Epoch 0/600 22/74 loss: 1.5605 acc [ 0.398  0.090  0.142]  time 0.03s 
Epoch 0/600 23/74 loss: 1.5568 acc [ 0.395  0.089  0.140]  time 12.00s 
Epoch 0/600 24/74 loss: 1.5498 acc [ 0.395  0.085  0.132]  time 0.04s 
Epoch 0/600 25/74 loss: 1.5447 acc [ 0.401  0.082  0.130]  time 0.04s 
Epoch 0/600 26/74 loss: 1.5418 acc [ 0.401  0.083  0.127]  time 0.04s 
Epoch 0/600 27/74 loss: 1.5484 acc [ 0.403  0.081  0.136]  time 0.04s 
Epoch 0/600 28/74 loss: 1.5433 acc [ 0.408  0.078  0.131]  time 0.04s 
Epoch 0/600 29/74 loss: 1.5391 acc [ 0.404  0.075  0.129]  time 0.04s 
Epoch 0/600 30/74 loss: 1.5345 acc [ 0.405  0.075  0.126]  time 0.04s 
Epoch 0/600 31/74 loss: 1.5311 acc [ 0.407  0.074  0.125]  time 12.31s 
Epoch 0/600 32/74 loss: 1.5279 acc [ 0.405  0.073  0.123]  time 0.79s 
Epoch 0/600 33/74 loss: 1.5246 acc [ 0.402  0.074  0.122]  time 0.03s 
Epoch 0/600 34/74 loss: 1.5218 acc [ 0.405  0.074  0.121]  time 0.03s 
Epoch 0/600 35/74 loss: 1.5181 acc [ 0.407  0.072  0.119]  time 0.03s 
Epoch 0/600 36/74 loss: 1.5167 acc [ 0.416  0.071  0.117]  time 6.57s 
Epoch 0/600 37/74 loss: 1.5144 acc [ 0.414  0.070  0.115]  time 0.03s 
Epoch 0/600 38/74 loss: 1.5145 acc [ 0.415  0.069  0.117]  time 0.04s 
Epoch 0/600 39/74 loss: 1.5125 acc [ 0.412  0.068  0.116]  time 0.04s 
Epoch 0/600 40/74 loss: 1.5097 acc [ 0.410  0.068  0.115]  time 0.85s 
Epoch 0/600 41/74 loss: 1.5081 acc [ 0.410  0.067  0.116]  time 0.03s 
Epoch 0/600 42/74 loss: 1.5060 acc [ 0.414  0.067  0.115]  time 0.03s 
Epoch 0/600 43/74 loss: 1.5032 acc [ 0.409  0.067  0.114]  time 0.03s 
Epoch 0/600 44/74 loss: 1.5041 acc [ 0.409  0.068  0.113]  time 14.19s 
Epoch 0/600 45/74 loss: 1.5025 acc [ 0.409  0.068  0.112]  time 0.03s 
Epoch 0/600 46/74 loss: 1.5002 acc [ 0.411  0.067  0.113]  time 0.03s 
Epoch 0/600 47/74 loss: 1.5003 acc [ 0.417  0.067  0.113]  time 4.84s 
Epoch 0/600 48/74 loss: 1.5017 acc [ 0.419  0.067  0.113]  time 0.03s 
Epoch 0/600 49/74 loss: 1.5036 acc [ 0.422  0.067  0.114]  time 0.03s 
Epoch 0/600 50/74 loss: 1.5076 acc [ 0.429  0.067  0.114]  time 0.03s 
Epoch 0/600 51/74 loss: 1.5050 acc [ 0.431  0.067  0.113]  time 0.03s 
Epoch 0/600 52/74 loss: 1.5011 acc [ 0.432  0.067  0.111]  time 2.36s 
Epoch 0/600 53/74 loss: 1.5046 acc [ 0.440  0.067  0.112]  time 7.77s 
Epoch 0/600 54/74 loss: 1.5231 acc [ 0.443  0.067  0.116]  time 0.03s 
Epoch 0/600 55/74 loss: 1.5376 acc [ 0.448  0.068  0.118]  time 15.02s 
Epoch 0/600 56/74 loss: 1.5336 acc [ 0.448  0.068  0.118]  time 0.03s 
Epoch 0/600 57/74 loss: 1.5305 acc [ 0.451  0.067  0.116]  time 0.03s 
Epoch 0/600 58/74 loss: 1.5289 acc [ 0.446  0.066  0.115]  time 0.03s 
Epoch 0/600 59/74 loss: 1.5283 acc [ 0.447  0.065  0.116]  time 0.03s 
Epoch 0/600 60/74 loss: 1.5279 acc [ 0.448  0.068  0.116]  time 0.03s 
Epoch 0/600 61/74 loss: 1.5282 acc [ 0.450  0.070  0.119]  time 3.32s 
Epoch 0/600 62/74 loss: 1.5272 acc [ 0.446  0.069  0.119]  time 0.04s 
Epoch 0/600 63/74 loss: 1.5290 acc [ 0.447  0.071  0.121]  time 2.56s 
Epoch 0/600 64/74 loss: 1.5262 acc [ 0.447  0.070  0.119]  time 0.03s 
Epoch 0/600 65/74 loss: 1.5244 acc [ 0.444  0.070  0.119]  time 0.03s 
Epoch 0/600 66/74 loss: 1.5229 acc [ 0.444  0.070  0.118]  time 0.03s 
Epoch 0/600 67/74 loss: 1.5207 acc [ 0.442  0.069  0.117]  time 0.03s 
Epoch 0/600 68/74 loss: 1.5184 acc [ 0.441  0.068  0.116]  time 0.03s 
Epoch 0/600 69/74 loss: 1.5165 acc [ 0.442  0.069  0.117]  time 12.23s 
Epoch 0/600 70/74 loss: 1.5136 acc [ 0.445  0.069  0.118]  time 0.03s 
Epoch 0/600 71/74 loss: 1.5111 acc [ 0.445  0.069  0.119]  time 2.40s 
Epoch 0/600 72/74 loss: 1.5092 acc [ 0.443  0.069  0.120]  time 0.03s 
Epoch 0/600 73/74 loss: 1.5093 acc [ 0.445  0.069  0.120]  time 0.26s 
Final training  0/599 loss: 1.5093 acc_avg: 0.2114 acc [ 0.445  0.069  0.120] time 137.38s  lr: 2.0000e-05
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 0, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 26.41 hr, running time 0.04 hr, est total time 26.45 hr 

Epoch 1/600 0/74 loss: 1.3742 acc [ 0.385  0.212  0.253]  time 5.59s 
Epoch 1/600 1/74 loss: 1.3861 acc [ 0.423  0.127  0.262]  time 2.18s 
Epoch 1/600 2/74 loss: 1.3865 acc [ 0.400  0.101  0.167]  time 0.03s 
Epoch 1/600 3/74 loss: 1.3802 acc [ 0.419  0.085  0.133]  time 0.03s 
Epoch 1/600 4/74 loss: 1.3567 acc [ 0.442  0.083  0.123]  time 9.85s 
Epoch 1/600 5/74 loss: 1.3542 acc [ 0.427  0.071  0.096]  time 0.11s 
Epoch 1/600 6/74 loss: 1.3531 acc [ 0.428  0.081  0.100]  time 0.03s 
Epoch 1/600 7/74 loss: 1.3486 acc [ 0.425  0.075  0.088]  time 0.06s 
Epoch 1/600 8/74 loss: 1.3654 acc [ 0.425  0.075  0.088]  time 0.08s 
Epoch 1/600 9/74 loss: 1.3750 acc [ 0.435  0.064  0.084]  time 0.04s 
Epoch 1/600 10/74 loss: 1.3669 acc [ 0.457  0.063  0.085]  time 0.03s 
Epoch 1/600 11/74 loss: 1.3592 acc [ 0.475  0.063  0.079]  time 0.04s 
Epoch 1/600 12/74 loss: 1.3499 acc [ 0.484  0.065  0.082]  time 3.44s 
Epoch 1/600 13/74 loss: 1.3566 acc [ 0.498  0.066  0.083]  time 0.03s 
Epoch 1/600 14/74 loss: 1.3764 acc [ 0.513  0.062  0.084]  time 0.03s 
Epoch 1/600 15/74 loss: 1.3687 acc [ 0.517  0.065  0.089]  time 0.03s 
Epoch 1/600 16/74 loss: 1.3731 acc [ 0.500  0.065  0.085]  time 0.05s 
Epoch 1/600 17/74 loss: 1.3653 acc [ 0.494  0.087  0.104]  time 0.04s 
Epoch 1/600 18/74 loss: 1.3594 acc [ 0.499  0.094  0.113]  time 0.03s 
Epoch 1/600 19/74 loss: 1.3546 acc [ 0.501  0.104  0.121]  time 0.05s 
Epoch 1/600 20/74 loss: 1.3673 acc [ 0.511  0.108  0.129]  time 5.59s 
Epoch 1/600 21/74 loss: 1.3671 acc [ 0.518  0.113  0.139]  time 0.21s 
Epoch 1/600 22/74 loss: 1.3701 acc [ 0.523  0.110  0.139]  time 0.20s 
Epoch 1/600 23/74 loss: 1.3686 acc [ 0.518  0.108  0.136]  time 0.10s 
Epoch 1/600 24/74 loss: 1.3590 acc [ 0.520  0.114  0.141]  time 0.03s 
Epoch 1/600 25/74 loss: 1.3568 acc [ 0.517  0.124  0.149]  time 0.03s 
Epoch 1/600 26/74 loss: 1.3542 acc [ 0.512  0.124  0.148]  time 0.03s 
Epoch 1/600 27/74 loss: 1.3530 acc [ 0.505  0.122  0.145]  time 0.19s 
Epoch 1/600 28/74 loss: 1.3511 acc [ 0.506  0.119  0.142]  time 5.24s 
Epoch 1/600 29/74 loss: 1.3600 acc [ 0.506  0.117  0.138]  time 0.06s 
Epoch 1/600 30/74 loss: 1.3545 acc [ 0.510  0.114  0.132]  time 0.11s 
Epoch 1/600 31/74 loss: 1.3581 acc [ 0.511  0.108  0.129]  time 0.03s 
Epoch 1/600 32/74 loss: 1.3627 acc [ 0.509  0.104  0.127]  time 0.05s 
Epoch 1/600 33/74 loss: 1.3641 acc [ 0.514  0.101  0.122]  time 8.89s 
Epoch 1/600 34/74 loss: 1.3631 acc [ 0.521  0.098  0.120]  time 0.03s 
Epoch 1/600 35/74 loss: 1.3596 acc [ 0.520  0.101  0.121]  time 0.03s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 4
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 4
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1709GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 4
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 4
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1726GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 4 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/74 loss: 1.6166 acc [ 0.364  0.008  0.022]  time 13.37s 
Epoch 0/600 1/74 loss: 1.6164 acc [ 0.296  0.012  0.135]  time 0.05s 
Epoch 0/600 2/74 loss: 1.6016 acc [ 0.320  0.015  0.113]  time 3.62s 
Epoch 0/600 3/74 loss: 1.5792 acc [ 0.348  0.024  0.114]  time 0.03s 
Epoch 0/600 4/74 loss: 1.5752 acc [ 0.346  0.026  0.120]  time 5.56s 
Epoch 0/600 5/74 loss: 1.5767 acc [ 0.360  0.021  0.102]  time 6.87s 
Epoch 0/600 6/74 loss: 1.5721 acc [ 0.350  0.025  0.105]  time 6.34s 
Epoch 0/600 7/74 loss: 1.5641 acc [ 0.354  0.028  0.112]  time 0.03s 
Epoch 0/600 8/74 loss: 1.5550 acc [ 0.354  0.029  0.111]  time 0.03s 
Epoch 0/600 9/74 loss: 1.5469 acc [ 0.357  0.034  0.114]  time 8.85s 
Epoch 0/600 10/74 loss: 1.5411 acc [ 0.344  0.033  0.113]  time 0.12s 
Epoch 0/600 11/74 loss: 1.5373 acc [ 0.343  0.032  0.114]  time 0.03s 
Epoch 0/600 12/74 loss: 1.5490 acc [ 0.342  0.044  0.130]  time 4.27s 
Epoch 0/600 13/74 loss: 1.5408 acc [ 0.339  0.043  0.126]  time 0.83s 
Epoch 0/600 14/74 loss: 1.5381 acc [ 0.328  0.041  0.120]  time 0.04s 
Epoch 0/600 15/74 loss: 1.5344 acc [ 0.320  0.040  0.118]  time 0.03s 
Epoch 0/600 16/74 loss: 1.5302 acc [ 0.321  0.041  0.117]  time 11.07s 
Epoch 0/600 17/74 loss: 1.5298 acc [ 0.316  0.043  0.126]  time 0.04s 
Epoch 0/600 18/74 loss: 1.5298 acc [ 0.323  0.044  0.131]  time 0.03s 
Epoch 0/600 19/74 loss: 1.5473 acc [ 0.332  0.049  0.142]  time 5.14s 
Epoch 0/600 20/74 loss: 1.5444 acc [ 0.329  0.050  0.144]  time 1.90s 
Epoch 0/600 21/74 loss: 1.5403 acc [ 0.321  0.048  0.137]  time 0.03s 
Epoch 0/600 22/74 loss: 1.5404 acc [ 0.319  0.048  0.141]  time 3.49s 
Epoch 0/600 23/74 loss: 1.5384 acc [ 0.321  0.048  0.141]  time 14.53s 
Epoch 0/600 24/74 loss: 1.5339 acc [ 0.318  0.046  0.135]  time 0.03s 
Epoch 0/600 25/74 loss: 1.5296 acc [ 0.318  0.045  0.133]  time 0.03s 
Epoch 0/600 26/74 loss: 1.5273 acc [ 0.319  0.045  0.132]  time 0.03s 
Epoch 0/600 27/74 loss: 1.5233 acc [ 0.321  0.046  0.134]  time 16.48s 
Epoch 0/600 28/74 loss: 1.5200 acc [ 0.325  0.045  0.133]  time 0.03s 
Epoch 0/600 29/74 loss: 1.5185 acc [ 0.321  0.049  0.135]  time 0.03s 
Epoch 0/600 30/74 loss: 1.5150 acc [ 0.321  0.049  0.131]  time 0.03s 
Epoch 0/600 31/74 loss: 1.5120 acc [ 0.326  0.049  0.131]  time 10.09s 
Epoch 0/600 32/74 loss: 1.5102 acc [ 0.326  0.050  0.129]  time 0.03s 
Epoch 0/600 33/74 loss: 1.5077 acc [ 0.327  0.051  0.126]  time 0.03s 
Epoch 0/600 34/74 loss: 1.5048 acc [ 0.331  0.053  0.127]  time 0.03s 
Epoch 0/600 35/74 loss: 1.5028 acc [ 0.335  0.054  0.127]  time 20.18s 
Epoch 0/600 36/74 loss: 1.5003 acc [ 0.337  0.055  0.128]  time 0.03s 
Epoch 0/600 37/74 loss: 1.4975 acc [ 0.337  0.054  0.128]  time 0.03s 
Epoch 0/600 38/74 loss: 1.4951 acc [ 0.335  0.054  0.125]  time 0.03s 
Epoch 0/600 39/74 loss: 1.4944 acc [ 0.342  0.055  0.127]  time 17.16s 
Epoch 0/600 40/74 loss: 1.4930 acc [ 0.343  0.055  0.123]  time 0.04s 
Epoch 0/600 41/74 loss: 1.4942 acc [ 0.348  0.056  0.123]  time 0.03s 
Epoch 0/600 42/74 loss: 1.4908 acc [ 0.348  0.056  0.122]  time 0.04s 
Epoch 0/600 43/74 loss: 1.4897 acc [ 0.346  0.058  0.122]  time 14.93s 
Epoch 0/600 44/74 loss: 1.4881 acc [ 0.344  0.059  0.122]  time 0.03s 
Epoch 0/600 45/74 loss: 1.4851 acc [ 0.347  0.059  0.120]  time 0.03s 
Epoch 0/600 46/74 loss: 1.4843 acc [ 0.344  0.060  0.119]  time 0.03s 
Epoch 0/600 47/74 loss: 1.4817 acc [ 0.345  0.060  0.118]  time 6.29s 
Epoch 0/600 48/74 loss: 1.4799 acc [ 0.346  0.060  0.117]  time 0.03s 
Epoch 0/600 49/74 loss: 1.4850 acc [ 0.354  0.062  0.115]  time 5.86s 
Epoch 0/600 50/74 loss: 1.4844 acc [ 0.351  0.062  0.115]  time 0.03s 
Epoch 0/600 51/74 loss: 1.4805 acc [ 0.353  0.063  0.115]  time 10.33s 
Epoch 0/600 52/74 loss: 1.4783 acc [ 0.351  0.062  0.113]  time 0.03s 
Epoch 0/600 53/74 loss: 1.4757 acc [ 0.350  0.061  0.110]  time 0.04s 
Epoch 0/600 54/74 loss: 1.4748 acc [ 0.352  0.061  0.110]  time 0.03s 
Epoch 0/600 55/74 loss: 1.4785 acc [ 0.353  0.064  0.110]  time 11.71s 
Epoch 0/600 56/74 loss: 1.4835 acc [ 0.354  0.066  0.111]  time 0.03s 
Epoch 0/600 57/74 loss: 1.4828 acc [ 0.355  0.067  0.110]  time 0.03s 
Epoch 0/600 58/74 loss: 1.4809 acc [ 0.355  0.067  0.110]  time 0.03s 
Epoch 0/600 59/74 loss: 1.4785 acc [ 0.359  0.067  0.107]  time 12.27s 
Epoch 0/600 60/74 loss: 1.4772 acc [ 0.359  0.067  0.107]  time 0.03s 
Epoch 0/600 61/74 loss: 1.4749 acc [ 0.360  0.066  0.105]  time 0.03s 
Epoch 0/600 62/74 loss: 1.4728 acc [ 0.360  0.066  0.104]  time 0.03s 
Epoch 0/600 63/74 loss: 1.4716 acc [ 0.359  0.067  0.105]  time 7.25s 
Epoch 0/600 64/74 loss: 1.4712 acc [ 0.363  0.066  0.104]  time 0.69s 
Epoch 0/600 65/74 loss: 1.4696 acc [ 0.362  0.065  0.102]  time 0.03s 
Epoch 0/600 66/74 loss: 1.4684 acc [ 0.364  0.066  0.103]  time 7.27s 
Epoch 0/600 67/74 loss: 1.4667 acc [ 0.363  0.065  0.101]  time 0.03s 
Epoch 0/600 68/74 loss: 1.4650 acc [ 0.364  0.066  0.101]  time 1.68s 
Epoch 0/600 69/74 loss: 1.4631 acc [ 0.365  0.070  0.101]  time 5.54s 
Epoch 0/600 70/74 loss: 1.4643 acc [ 0.367  0.071  0.102]  time 10.03s 
Epoch 0/600 71/74 loss: 1.4657 acc [ 0.370  0.072  0.101]  time 0.03s 
Epoch 0/600 72/74 loss: 1.4668 acc [ 0.375  0.071  0.101]  time 4.10s 
Epoch 0/600 73/74 loss: 1.4664 acc [ 0.375  0.071  0.101]  time 0.36s 
Final training  0/599 loss: 1.4664 acc_avg: 0.1823 acc [ 0.375  0.071  0.101] time 249.63s  lr: 2.0000e-05
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 0, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 47.99 hr, running time 0.07 hr, est total time 48.05 hr 

Epoch 1/600 0/74 loss: 1.3235 acc [ 0.447  0.111  0.099]  time 5.11s 
Epoch 1/600 1/74 loss: 1.3595 acc [ 0.499  0.191  0.146]  time 0.03s 
Epoch 1/600 2/74 loss: 1.4098 acc [ 0.501  0.140  0.113]  time 0.72s 
Epoch 1/600 3/74 loss: 1.3842 acc [ 0.565  0.148  0.111]  time 3.05s 
Epoch 1/600 4/74 loss: 1.3661 acc [ 0.529  0.168  0.120]  time 0.03s 
Epoch 1/600 5/74 loss: 1.3610 acc [ 0.491  0.160  0.102]  time 0.05s 
Epoch 1/600 6/74 loss: 1.3467 acc [ 0.502  0.159  0.100]  time 3.36s 
Epoch 1/600 7/74 loss: 1.3422 acc [ 0.490  0.154  0.094]  time 1.49s 
Epoch 1/600 8/74 loss: 1.4058 acc [ 0.498  0.144  0.089]  time 0.03s 
Epoch 1/600 9/74 loss: 1.4208 acc [ 0.504  0.134  0.081]  time 0.03s 
Epoch 1/600 10/74 loss: 1.4199 acc [ 0.497  0.136  0.080]  time 2.19s 
Epoch 1/600 11/74 loss: 1.4104 acc [ 0.483  0.135  0.076]  time 0.04s 
Epoch 1/600 12/74 loss: 1.4020 acc [ 0.487  0.139  0.077]  time 0.51s 
Epoch 1/600 13/74 loss: 1.3986 acc [ 0.483  0.144  0.080]  time 0.85s 
Epoch 1/600 14/74 loss: 1.3937 acc [ 0.495  0.151  0.086]  time 2.44s 
Epoch 1/600 15/74 loss: 1.3872 acc [ 0.498  0.147  0.085]  time 0.03s 
Epoch 1/600 16/74 loss: 1.3857 acc [ 0.497  0.140  0.083]  time 0.03s 
Epoch 1/600 17/74 loss: 1.3835 acc [ 0.483  0.134  0.083]  time 0.03s 
Epoch 1/600 18/74 loss: 1.3728 acc [ 0.486  0.133  0.086]  time 2.91s 
Epoch 1/600 19/74 loss: 1.3772 acc [ 0.495  0.146  0.098]  time 0.04s 
Epoch 1/600 20/74 loss: 1.3751 acc [ 0.494  0.141  0.097]  time 0.04s 
Epoch 1/600 21/74 loss: 1.3707 acc [ 0.499  0.137  0.095]  time 0.43s 
Epoch 1/600 22/74 loss: 1.3665 acc [ 0.507  0.135  0.096]  time 2.04s 
Epoch 1/600 23/74 loss: 1.3605 acc [ 0.513  0.139  0.101]  time 0.03s 
Epoch 1/600 24/74 loss: 1.3555 acc [ 0.511  0.139  0.100]  time 0.05s 
Epoch 1/600 25/74 loss: 1.3458 acc [ 0.520  0.149  0.109]  time 0.22s 
Epoch 1/600 26/74 loss: 1.3425 acc [ 0.519  0.152  0.112]  time 4.92s 
Epoch 1/600 27/74 loss: 1.3373 acc [ 0.521  0.152  0.111]  time 0.28s 
Epoch 1/600 28/74 loss: 1.3419 acc [ 0.524  0.151  0.109]  time 0.12s 
Epoch 1/600 29/74 loss: 1.3377 acc [ 0.512  0.153  0.113]  time 0.10s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 8
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 0
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1716GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 8 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/37 loss: 1.5870 acc [ 0.298  0.023  0.144]  time 81.39s 
Epoch 0/600 1/37 loss: 1.5569 acc [ 0.259  0.025  0.106]  time 74.75s 
Epoch 0/600 2/37 loss: 1.5541 acc [ 0.225  0.022  0.099]  time 61.38s 
Epoch 0/600 3/37 loss: 1.5679 acc [ 0.245  0.023  0.117]  time 73.41s 
Epoch 0/600 4/37 loss: 1.6088 acc [ 0.282  0.022  0.127]  time 96.01s 
Epoch 0/600 5/37 loss: 1.5920 acc [ 0.271  0.021  0.127]  time 71.20s 
Epoch 0/600 6/37 loss: 1.5800 acc [ 0.278  0.019  0.115]  time 60.01s 
Epoch 0/600 7/37 loss: 1.5892 acc [ 0.298  0.019  0.130]  time 77.59s 
Epoch 0/600 8/37 loss: 1.5748 acc [ 0.305  0.019  0.130]  time 83.40s 
Epoch 0/600 9/37 loss: 1.5788 acc [ 0.318  0.021  0.132]  time 95.69s 
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 8
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 8
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1710GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 8
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 4
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1704GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 8 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/37 loss: 1.8468 acc [ 0.233  0.242  0.113]  time 29.82s 
Epoch 0/600 1/37 loss: 1.7806 acc [ 0.232  0.260  0.099]  time 3.29s 
Epoch 0/600 2/37 loss: 1.7573 acc [ 0.245  0.207  0.078]  time 0.06s 
Epoch 0/600 3/37 loss: 1.7173 acc [ 0.264  0.199  0.080]  time 14.87s 
Epoch 0/600 4/37 loss: 1.7176 acc [ 0.262  0.204  0.077]  time 0.06s 
Epoch 0/600 5/37 loss: 1.6905 acc [ 0.264  0.196  0.076]  time 3.75s 
Epoch 0/600 6/37 loss: 1.6661 acc [ 0.273  0.190  0.073]  time 16.87s 
Epoch 0/600 7/37 loss: 1.6546 acc [ 0.287  0.181  0.072]  time 13.36s 
Epoch 0/600 8/37 loss: 1.6442 acc [ 0.289  0.172  0.069]  time 0.06s 
Epoch 0/600 9/37 loss: 1.6322 acc [ 0.301  0.163  0.067]  time 0.05s 
Epoch 0/600 10/37 loss: 1.6331 acc [ 0.313  0.170  0.073]  time 13.70s 
Epoch 0/600 11/37 loss: 1.6296 acc [ 0.321  0.164  0.070]  time 22.80s 
Epoch 0/600 12/37 loss: 1.6179 acc [ 0.319  0.162  0.070]  time 0.06s 
Epoch 0/600 13/37 loss: 1.6096 acc [ 0.321  0.159  0.066]  time 0.06s 
Epoch 0/600 14/37 loss: 1.5998 acc [ 0.317  0.154  0.066]  time 0.05s 
Epoch 0/600 15/37 loss: 1.5929 acc [ 0.321  0.151  0.065]  time 38.36s 
Epoch 0/600 16/37 loss: 1.5886 acc [ 0.315  0.149  0.064]  time 0.06s 
Epoch 0/600 17/37 loss: 1.5806 acc [ 0.308  0.146  0.064]  time 0.06s 
Epoch 0/600 18/37 loss: 1.5737 acc [ 0.309  0.142  0.062]  time 0.06s 
Epoch 0/600 19/37 loss: 1.5677 acc [ 0.314  0.142  0.061]  time 28.47s 
Epoch 0/600 20/37 loss: 1.5662 acc [ 0.315  0.140  0.060]  time 0.06s 
Epoch 0/600 21/37 loss: 1.5597 acc [ 0.318  0.136  0.059]  time 0.05s 
Epoch 0/600 22/37 loss: 1.5590 acc [ 0.323  0.134  0.061]  time 0.06s 
Epoch 0/600 23/37 loss: 1.5562 acc [ 0.324  0.133  0.061]  time 27.05s 
Epoch 0/600 24/37 loss: 1.5535 acc [ 0.322  0.130  0.062]  time 0.06s 
Epoch 0/600 25/37 loss: 1.5498 acc [ 0.322  0.131  0.061]  time 0.06s 
Epoch 0/600 26/37 loss: 1.5452 acc [ 0.324  0.130  0.062]  time 0.06s 
Epoch 0/600 27/37 loss: 1.5416 acc [ 0.328  0.128  0.060]  time 23.71s 
Epoch 0/600 28/37 loss: 1.5363 acc [ 0.331  0.126  0.060]  time 0.06s 
Epoch 0/600 29/37 loss: 1.5438 acc [ 0.334  0.127  0.058]  time 0.06s 
Epoch 0/600 30/37 loss: 1.5409 acc [ 0.336  0.125  0.057]  time 0.06s 
Epoch 0/600 31/37 loss: 1.5384 acc [ 0.333  0.124  0.057]  time 30.83s 
Epoch 0/600 32/37 loss: 1.5357 acc [ 0.331  0.124  0.057]  time 0.05s 
Epoch 0/600 33/37 loss: 1.5341 acc [ 0.333  0.123  0.055]  time 0.06s 
Epoch 0/600 34/37 loss: 1.5373 acc [ 0.340  0.122  0.057]  time 0.06s 
Epoch 0/600 35/37 loss: 1.5337 acc [ 0.340  0.122  0.056]  time 26.29s 
Epoch 0/600 36/37 loss: 1.5354 acc [ 0.343  0.122  0.057]  time 0.45s 
Final training  0/599 loss: 1.5354 acc_avg: 0.1738 acc [ 0.343  0.122  0.057] time 294.86s  lr: 2.0000e-05
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 0, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 56.68 hr, running time 0.08 hr, est total time 56.76 hr 

Epoch 1/600 0/37 loss: 1.4093 acc [ 0.263  0.039  0.052]  time 7.53s 
Epoch 1/600 1/37 loss: 1.4408 acc [ 0.353  0.101  0.044]  time 1.22s 
Epoch 1/600 2/37 loss: 1.4325 acc [ 0.343  0.088  0.056]  time 0.06s 
Epoch 1/600 3/37 loss: 1.4344 acc [ 0.340  0.076  0.047]  time 0.06s 
Epoch 1/600 4/37 loss: 1.4260 acc [ 0.346  0.078  0.052]  time 7.32s 
Epoch 1/600 5/37 loss: 1.4135 acc [ 0.363  0.087  0.056]  time 0.11s 
Epoch 1/600 6/37 loss: 1.4026 acc [ 0.371  0.087  0.060]  time 0.15s 
Epoch 1/600 7/37 loss: 1.3997 acc [ 0.383  0.077  0.052]  time 0.06s 
Epoch 1/600 8/37 loss: 1.3999 acc [ 0.379  0.072  0.050]  time 6.11s 
Epoch 1/600 9/37 loss: 1.3947 acc [ 0.409  0.073  0.056]  time 0.37s 
Epoch 1/600 10/37 loss: 1.3914 acc [ 0.408  0.071  0.054]  time 0.07s 
Epoch 1/600 11/37 loss: 1.3844 acc [ 0.413  0.069  0.054]  time 0.05s 
Epoch 1/600 12/37 loss: 1.3762 acc [ 0.424  0.067  0.058]  time 9.00s 
Epoch 1/600 13/37 loss: 1.3717 acc [ 0.424  0.064  0.055]  time 0.09s 
Epoch 1/600 14/37 loss: 1.3705 acc [ 0.418  0.064  0.054]  time 0.09s 
Epoch 1/600 15/37 loss: 1.3687 acc [ 0.426  0.062  0.054]  time 0.07s 
Epoch 1/600 16/37 loss: 1.3726 acc [ 0.429  0.061  0.053]  time 8.90s 
Epoch 1/600 17/37 loss: 1.3670 acc [ 0.434  0.061  0.055]  time 0.06s 
Epoch 1/600 18/37 loss: 1.3626 acc [ 0.438  0.061  0.055]  time 0.06s 
Epoch 1/600 19/37 loss: 1.3558 acc [ 0.443  0.070  0.062]  time 0.06s 
Epoch 1/600 20/37 loss: 1.3533 acc [ 0.446  0.067  0.060]  time 5.53s 
Epoch 1/600 21/37 loss: 1.3482 acc [ 0.441  0.073  0.065]  time 0.06s 
Epoch 1/600 22/37 loss: 1.3580 acc [ 0.455  0.072  0.064]  time 0.13s 
Epoch 1/600 23/37 loss: 1.3543 acc [ 0.462  0.076  0.066]  time 0.06s 
Epoch 1/600 24/37 loss: 1.3511 acc [ 0.462  0.076  0.066]  time 4.28s 
Epoch 1/600 25/37 loss: 1.3494 acc [ 0.460  0.074  0.065]  time 0.06s 
Epoch 1/600 26/37 loss: 1.3451 acc [ 0.458  0.076  0.068]  time 0.06s 
Epoch 1/600 27/37 loss: 1.3397 acc [ 0.462  0.081  0.070]  time 0.06s 
Epoch 1/600 28/37 loss: 1.3390 acc [ 0.470  0.082  0.068]  time 7.70s 
Epoch 1/600 29/37 loss: 1.3338 acc [ 0.478  0.084  0.068]  time 0.06s 
Epoch 1/600 30/37 loss: 1.3365 acc [ 0.479  0.083  0.067]  time 0.06s 
Epoch 1/600 31/37 loss: 1.3330 acc [ 0.480  0.085  0.069]  time 0.05s 
Epoch 1/600 32/37 loss: 1.3339 acc [ 0.479  0.083  0.068]  time 4.88s 
Epoch 1/600 33/37 loss: 1.3298 acc [ 0.480  0.084  0.068]  time 0.15s 
Epoch 1/600 34/37 loss: 1.3242 acc [ 0.481  0.089  0.073]  time 0.39s 
Epoch 1/600 35/37 loss: 1.3209 acc [ 0.485  0.086  0.073]  time 0.14s 
Epoch 1/600 36/37 loss: 1.3222 acc [ 0.486  0.088  0.075]  time 1.07s 
Final training  1/599 loss: 1.3222 acc_avg: 0.2164 acc [ 0.486  0.088  0.075] time 66.16s  lr: 8.0000e-05
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 1, 'best_metric': -1}, save_time 0.12s
Estimated remaining training time for the current model fold 0 is 18.54 hr, running time 0.10 hr, est total time 18.64 hr 

Epoch 2/600 0/37 loss: 1.2620 acc [ 0.503  0.016  0.047]  time 0.29s 
Epoch 2/600 1/37 loss: 1.1552 acc [ 0.701  0.127  0.171]  time 0.06s 
Epoch 2/600 2/37 loss: 1.1302 acc [ 0.659  0.125  0.203]  time 0.06s 
Epoch 2/600 3/37 loss: 1.2008 acc [ 0.680  0.143  0.181]  time 0.06s 
Epoch 2/600 4/37 loss: 1.1975 acc [ 0.650  0.141  0.171]  time 0.06s 
Epoch 2/600 5/37 loss: 1.2314 acc [ 0.639  0.117  0.143]  time 0.06s 
Epoch 2/600 6/37 loss: 1.1958 acc [ 0.645  0.140  0.174]  time 0.06s 
Epoch 2/600 7/37 loss: 1.2255 acc [ 0.628  0.149  0.176]  time 0.07s 
Epoch 2/600 8/37 loss: 1.2340 acc [ 0.614  0.146  0.165]  time 0.06s 
Epoch 2/600 9/37 loss: 1.2279 acc [ 0.599  0.151  0.169]  time 0.06s 
Epoch 2/600 10/37 loss: 1.2291 acc [ 0.591  0.150  0.170]  time 0.06s 
Epoch 2/600 11/37 loss: 1.2265 acc [ 0.595  0.151  0.167]  time 0.07s 
Epoch 2/600 12/37 loss: 1.2369 acc [ 0.585  0.152  0.164]  time 0.06s 
Epoch 2/600 13/37 loss: 1.2360 acc [ 0.586  0.154  0.163]  time 0.06s 
Epoch 2/600 14/37 loss: 1.2395 acc [ 0.589  0.149  0.151]  time 0.06s 
Epoch 2/600 15/37 loss: 1.2370 acc [ 0.586  0.147  0.146]  time 0.07s 
Epoch 2/600 16/37 loss: 1.2410 acc [ 0.584  0.143  0.146]  time 0.06s 
Epoch 2/600 17/37 loss: 1.2487 acc [ 0.568  0.141  0.142]  time 0.06s 
Epoch 2/600 18/37 loss: 1.2441 acc [ 0.571  0.147  0.147]  time 0.07s 
Epoch 2/600 19/37 loss: 1.2501 acc [ 0.570  0.144  0.145]  time 0.06s 
Epoch 2/600 20/37 loss: 1.2515 acc [ 0.568  0.151  0.152]  time 0.07s 
Epoch 2/600 21/37 loss: 1.2635 acc [ 0.562  0.154  0.156]  time 0.06s 
Epoch 2/600 22/37 loss: 1.2642 acc [ 0.561  0.153  0.154]  time 0.06s 
Epoch 2/600 23/37 loss: 1.2536 acc [ 0.564  0.166  0.168]  time 0.06s 
Epoch 2/600 24/37 loss: 1.2540 acc [ 0.569  0.171  0.174]  time 0.06s 
Epoch 2/600 25/37 loss: 1.2435 acc [ 0.571  0.179  0.183]  time 0.06s 
Epoch 2/600 26/37 loss: 1.2394 acc [ 0.569  0.184  0.191]  time 0.06s 
Epoch 2/600 27/37 loss: 1.2610 acc [ 0.572  0.191  0.198]  time 0.06s 
Epoch 2/600 28/37 loss: 1.2627 acc [ 0.577  0.190  0.198]  time 0.05s 
Epoch 2/600 29/37 loss: 1.2642 acc [ 0.575  0.185  0.195]  time 0.06s 
Epoch 2/600 30/37 loss: 1.2683 acc [ 0.578  0.181  0.191]  time 0.07s 
Epoch 2/600 31/37 loss: 1.2660 acc [ 0.578  0.181  0.192]  time 0.06s 
Epoch 2/600 32/37 loss: 1.2636 acc [ 0.573  0.181  0.191]  time 0.06s 
Epoch 2/600 33/37 loss: 1.2594 acc [ 0.573  0.185  0.196]  time 0.06s 
Epoch 2/600 34/37 loss: 1.2571 acc [ 0.573  0.186  0.197]  time 0.05s 
Epoch 2/600 35/37 loss: 1.2543 acc [ 0.571  0.187  0.197]  time 0.06s 
Epoch 2/600 36/37 loss: 1.2516 acc [ 0.566  0.189  0.200]  time 0.04s 
Final training  2/599 loss: 1.2516 acc_avg: 0.3183 acc [ 0.566  0.189  0.200] time 2.40s  lr: 1.4000e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 2, 'best_metric': -1}, save_time 0.12s
Estimated remaining training time for the current model fold 0 is 7.93 hr, running time 0.10 hr, est total time 8.03 hr 

Epoch 3/600 0/37 loss: 1.1116 acc [ 0.633  0.361  0.346]  time 0.36s 
Epoch 3/600 1/37 loss: 1.2774 acc [ 0.537  0.181  0.240]  time 0.06s 
Epoch 3/600 2/37 loss: 1.2603 acc [ 0.551  0.116  0.200]  time 0.05s 
Epoch 3/600 3/37 loss: 1.3070 acc [ 0.539  0.092  0.159]  time 0.06s 
Epoch 3/600 4/37 loss: 1.3019 acc [ 0.542  0.082  0.159]  time 0.06s 
Epoch 3/600 5/37 loss: 1.2595 acc [ 0.566  0.101  0.152]  time 0.06s 
Epoch 3/600 6/37 loss: 1.2530 acc [ 0.575  0.087  0.120]  time 0.06s 
Epoch 3/600 7/37 loss: 1.2402 acc [ 0.584  0.119  0.160]  time 0.06s 
Epoch 3/600 8/37 loss: 1.2399 acc [ 0.561  0.114  0.148]  time 0.06s 
Epoch 3/600 9/37 loss: 1.2326 acc [ 0.576  0.116  0.156]  time 0.06s 
Epoch 3/600 10/37 loss: 1.2282 acc [ 0.573  0.109  0.144]  time 0.06s 
Epoch 3/600 11/37 loss: 1.2140 acc [ 0.568  0.116  0.150]  time 0.06s 
Epoch 3/600 12/37 loss: 1.2363 acc [ 0.570  0.111  0.137]  time 0.06s 
Epoch 3/600 13/37 loss: 1.2414 acc [ 0.569  0.117  0.140]  time 0.06s 
Epoch 3/600 14/37 loss: 1.2302 acc [ 0.578  0.124  0.146]  time 0.05s 
Epoch 3/600 15/37 loss: 1.2235 acc [ 0.586  0.129  0.153]  time 0.06s 
Epoch 3/600 16/37 loss: 1.2189 acc [ 0.585  0.130  0.157]  time 0.07s 
Epoch 3/600 17/37 loss: 1.2160 acc [ 0.584  0.127  0.153]  time 0.08s 
Epoch 3/600 18/37 loss: 1.2158 acc [ 0.581  0.124  0.148]  time 0.06s 
Epoch 3/600 19/37 loss: 1.2096 acc [ 0.576  0.127  0.153]  time 0.07s 
Epoch 3/600 20/37 loss: 1.2138 acc [ 0.569  0.126  0.144]  time 0.07s 
Epoch 3/600 21/37 loss: 1.2163 acc [ 0.573  0.121  0.139]  time 0.06s 
Epoch 3/600 22/37 loss: 1.2131 acc [ 0.580  0.124  0.141]  time 0.07s 
Epoch 3/600 23/37 loss: 1.2179 acc [ 0.586  0.131  0.148]  time 0.07s 
Epoch 3/600 24/37 loss: 1.2180 acc [ 0.590  0.127  0.142]  time 0.06s 
Epoch 3/600 25/37 loss: 1.2120 acc [ 0.587  0.131  0.146]  time 0.06s 
Epoch 3/600 26/37 loss: 1.2075 acc [ 0.590  0.134  0.149]  time 0.06s 
Epoch 3/600 27/37 loss: 1.2084 acc [ 0.593  0.137  0.153]  time 0.07s 
Epoch 3/600 28/37 loss: 1.2180 acc [ 0.588  0.135  0.151]  time 0.06s 
Epoch 3/600 29/37 loss: 1.2273 acc [ 0.587  0.142  0.159]  time 0.06s 
Epoch 3/600 30/37 loss: 1.2241 acc [ 0.588  0.149  0.167]  time 0.06s 
Epoch 3/600 31/37 loss: 1.2236 acc [ 0.587  0.156  0.174]  time 0.06s 
Epoch 3/600 32/37 loss: 1.2189 acc [ 0.588  0.160  0.180]  time 0.06s 
Epoch 3/600 33/37 loss: 1.2143 acc [ 0.589  0.166  0.187]  time 0.06s 
Epoch 3/600 34/37 loss: 1.2160 acc [ 0.588  0.172  0.193]  time 0.06s 
Epoch 3/600 35/37 loss: 1.2159 acc [ 0.590  0.171  0.191]  time 0.06s 
Epoch 3/600 36/37 loss: 1.2107 acc [ 0.591  0.176  0.196]  time 0.04s 
Final training  3/599 loss: 1.2107 acc_avg: 0.3207 acc [ 0.591  0.176  0.196] time 2.53s  lr: 2.0000e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 3, 'best_metric': -1}, save_time 0.12s
Estimated remaining training time for the current model fold 0 is 7.95 hr, running time 0.10 hr, est total time 8.06 hr 

Epoch 4/600 0/37 loss: 1.1012 acc [ 0.818  0.435  0.390]  time 0.33s 
Epoch 4/600 1/37 loss: 1.2581 acc [ 0.566  0.224  0.208]  time 0.06s 
Epoch 4/600 2/37 loss: 1.2001 acc [ 0.551  0.231  0.203]  time 0.05s 
Epoch 4/600 3/37 loss: 1.1832 acc [ 0.534  0.196  0.180]  time 0.06s 
Epoch 4/600 4/37 loss: 1.1524 acc [ 0.576  0.196  0.203]  time 0.06s 
Epoch 4/600 5/37 loss: 1.1836 acc [ 0.577  0.174  0.175]  time 0.06s 
Epoch 4/600 6/37 loss: 1.2143 acc [ 0.563  0.173  0.176]  time 0.06s 
Epoch 4/600 7/37 loss: 1.1935 acc [ 0.544  0.179  0.185]  time 0.06s 
Epoch 4/600 8/37 loss: 1.1756 acc [ 0.551  0.177  0.180]  time 0.06s 
Epoch 4/600 9/37 loss: 1.1742 acc [ 0.561  0.177  0.181]  time 0.10s 
Epoch 4/600 10/37 loss: 1.1651 acc [ 0.567  0.180  0.185]  time 0.05s 
Epoch 4/600 11/37 loss: 1.1603 acc [ 0.567  0.177  0.183]  time 0.06s 
Epoch 4/600 12/37 loss: 1.1631 acc [ 0.566  0.168  0.172]  time 0.06s 
Epoch 4/600 13/37 loss: 1.1657 acc [ 0.574  0.166  0.170]  time 0.06s 
Epoch 4/600 14/37 loss: 1.1620 acc [ 0.586  0.165  0.169]  time 0.06s 
Epoch 4/600 15/37 loss: 1.1597 acc [ 0.586  0.172  0.176]  time 0.06s 
Epoch 4/600 16/37 loss: 1.1819 acc [ 0.584  0.167  0.176]  time 0.07s 
Epoch 4/600 17/37 loss: 1.1772 acc [ 0.579  0.168  0.178]  time 0.06s 
Epoch 4/600 18/37 loss: 1.1790 acc [ 0.577  0.169  0.178]  time 0.05s 
Epoch 4/600 19/37 loss: 1.1773 acc [ 0.583  0.175  0.185]  time 0.06s 
Epoch 4/600 20/37 loss: 1.1769 acc [ 0.579  0.181  0.194]  time 0.08s 
Epoch 4/600 21/37 loss: 1.1693 acc [ 0.583  0.193  0.208]  time 0.06s 
Epoch 4/600 22/37 loss: 1.1705 acc [ 0.589  0.197  0.212]  time 0.06s 
Epoch 4/600 23/37 loss: 1.1669 acc [ 0.593  0.205  0.218]  time 0.06s 
Epoch 4/600 24/37 loss: 1.1672 acc [ 0.597  0.204  0.221]  time 0.06s 
Epoch 4/600 25/37 loss: 1.1716 acc [ 0.590  0.202  0.221]  time 0.06s 
Epoch 4/600 26/37 loss: 1.1714 acc [ 0.593  0.202  0.223]  time 0.06s 
Epoch 4/600 27/37 loss: 1.1688 acc [ 0.592  0.206  0.228]  time 0.06s 
Epoch 4/600 28/37 loss: 1.1645 acc [ 0.595  0.212  0.235]  time 0.06s 
Epoch 4/600 29/37 loss: 1.1663 acc [ 0.601  0.213  0.237]  time 0.06s 
Epoch 4/600 30/37 loss: 1.1620 acc [ 0.603  0.217  0.245]  time 0.06s 
Epoch 4/600 31/37 loss: 1.1593 acc [ 0.600  0.216  0.244]  time 0.06s 
Epoch 4/600 32/37 loss: 1.1595 acc [ 0.600  0.221  0.248]  time 0.06s 
Epoch 4/600 33/37 loss: 1.1609 acc [ 0.592  0.223  0.249]  time 0.05s 
Epoch 4/600 34/37 loss: 1.1612 acc [ 0.589  0.227  0.255]  time 0.05s 
Epoch 4/600 35/37 loss: 1.1594 acc [ 0.590  0.231  0.259]  time 0.06s 
Epoch 4/600 36/37 loss: 1.1622 acc [ 0.591  0.236  0.264]  time 0.04s 
Final training  4/599 loss: 1.1622 acc_avg: 0.3636 acc [ 0.591  0.236  0.264] time 2.46s  lr: 2.0000e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 4, 'best_metric': -1}, save_time 0.12s
Estimated remaining training time for the current model fold 0 is 7.94 hr, running time 0.10 hr, est total time 8.05 hr 

Epoch 5/600 0/37 loss: 1.0302 acc [ 0.604  0.439  0.430]  time 0.31s 
Epoch 5/600 1/37 loss: 1.0669 acc [ 0.582  0.335  0.330]  time 0.06s 
Epoch 5/600 2/37 loss: 1.0516 acc [ 0.621  0.361  0.386]  time 0.06s 
Epoch 5/600 3/37 loss: 1.0600 acc [ 0.615  0.334  0.366]  time 0.06s 
Epoch 5/600 4/37 loss: 1.0616 acc [ 0.607  0.338  0.366]  time 0.06s 
Epoch 5/600 5/37 loss: 1.0908 acc [ 0.589  0.337  0.361]  time 0.06s 
Epoch 5/600 6/37 loss: 1.1059 acc [ 0.576  0.313  0.333]  time 0.06s 
Epoch 5/600 7/37 loss: 1.1009 acc [ 0.594  0.308  0.344]  time 0.07s 
Epoch 5/600 8/37 loss: 1.1058 acc [ 0.583  0.298  0.326]  time 0.06s 
Epoch 5/600 9/37 loss: 1.1149 acc [ 0.597  0.290  0.326]  time 0.05s 
Epoch 5/600 10/37 loss: 1.1049 acc [ 0.603  0.296  0.330]  time 0.06s 
Epoch 5/600 11/37 loss: 1.1292 acc [ 0.592  0.286  0.313]  time 0.06s 
Epoch 5/600 12/37 loss: 1.1511 acc [ 0.584  0.274  0.297]  time 0.07s 
Epoch 5/600 13/37 loss: 1.1387 acc [ 0.587  0.273  0.293]  time 0.06s 
Epoch 5/600 14/37 loss: 1.1508 acc [ 0.592  0.285  0.297]  time 0.07s 
Epoch 5/600 15/37 loss: 1.1723 acc [ 0.587  0.270  0.280]  time 0.07s 
Epoch 5/600 16/37 loss: 1.1741 acc [ 0.590  0.259  0.274]  time 0.06s 
Epoch 5/600 17/37 loss: 1.1632 acc [ 0.591  0.262  0.282]  time 0.06s 
Epoch 5/600 18/37 loss: 1.1729 acc [ 0.593  0.259  0.278]  time 0.06s 
Epoch 5/600 19/37 loss: 1.1937 acc [ 0.586  0.254  0.271]  time 0.06s 
Epoch 5/600 20/37 loss: 1.1922 acc [ 0.588  0.248  0.267]  time 0.06s 
Epoch 5/600 21/37 loss: 1.2045 acc [ 0.591  0.254  0.271]  time 0.07s 
Epoch 5/600 22/37 loss: 1.2047 acc [ 0.582  0.251  0.270]  time 0.06s 
Epoch 5/600 23/37 loss: 1.2034 acc [ 0.582  0.246  0.266]  time 0.06s 
Epoch 5/600 24/37 loss: 1.2069 acc [ 0.579  0.242  0.263]  time 0.06s 
Epoch 5/600 25/37 loss: 1.2037 acc [ 0.586  0.245  0.267]  time 0.06s 
Epoch 5/600 26/37 loss: 1.2010 acc [ 0.588  0.244  0.270]  time 0.06s 
Epoch 5/600 27/37 loss: 1.1987 acc [ 0.592  0.244  0.271]  time 0.06s 
Epoch 5/600 28/37 loss: 1.1995 acc [ 0.589  0.242  0.269]  time 0.06s 
Epoch 5/600 29/37 loss: 1.1951 acc [ 0.591  0.242  0.270]  time 0.06s 
Epoch 5/600 30/37 loss: 1.1956 acc [ 0.592  0.245  0.272]  time 0.06s 
Epoch 5/600 31/37 loss: 1.1971 acc [ 0.588  0.240  0.266]  time 0.06s 
Epoch 5/600 32/37 loss: 1.1974 acc [ 0.589  0.238  0.264]  time 0.06s 
Epoch 5/600 33/37 loss: 1.2048 acc [ 0.580  0.236  0.263]  time 0.06s 
Epoch 5/600 34/37 loss: 1.2026 acc [ 0.579  0.234  0.261]  time 0.06s 
Epoch 5/600 35/37 loss: 1.2196 acc [ 0.582  0.237  0.261]  time 0.06s 
Epoch 5/600 36/37 loss: 1.2155 acc [ 0.584  0.238  0.261]  time 0.04s 
Final training  5/599 loss: 1.2155 acc_avg: 0.3610 acc [ 0.584  0.238  0.261] time 2.45s  lr: 1.9999e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 5, 'best_metric': -1}, save_time 0.12s
Estimated remaining training time for the current model fold 0 is 7.94 hr, running time 0.10 hr, est total time 8.04 hr 

Epoch 6/600 0/37 loss: 1.3707 acc [ 0.352  0.025  0.031]  time 0.34s 
Epoch 6/600 1/37 loss: 1.3712 acc [ 0.422  0.019  0.025]  time 0.06s 
Epoch 6/600 2/37 loss: 1.3377 acc [ 0.505  0.169  0.209]  time 0.06s 
Epoch 6/600 3/37 loss: 1.2711 acc [ 0.546  0.176  0.216]  time 0.06s 
Epoch 6/600 4/37 loss: 1.2789 acc [ 0.554  0.204  0.238]  time 0.09s 
Epoch 6/600 5/37 loss: 1.2406 acc [ 0.575  0.224  0.269]  time 0.06s 
Epoch 6/600 6/37 loss: 1.2272 acc [ 0.592  0.246  0.289]  time 0.06s 
Epoch 6/600 7/37 loss: 1.2288 acc [ 0.587  0.231  0.275]  time 0.06s 
Epoch 6/600 8/37 loss: 1.2060 acc [ 0.591  0.241  0.286]  time 0.06s 
Epoch 6/600 9/37 loss: 1.2163 acc [ 0.604  0.248  0.289]  time 0.06s 
Epoch 6/600 10/37 loss: 1.1982 acc [ 0.613  0.256  0.298]  time 0.06s 
Epoch 6/600 11/37 loss: 1.1811 acc [ 0.624  0.267  0.307]  time 0.06s 
Epoch 6/600 12/37 loss: 1.2245 acc [ 0.619  0.260  0.294]  time 0.06s 
Epoch 6/600 13/37 loss: 1.2288 acc [ 0.617  0.252  0.290]  time 0.06s 
Epoch 6/600 14/37 loss: 1.2287 acc [ 0.625  0.256  0.292]  time 0.07s 
Epoch 6/600 15/37 loss: 1.2182 acc [ 0.623  0.254  0.288]  time 0.06s 
Epoch 6/600 16/37 loss: 1.2194 acc [ 0.627  0.254  0.286]  time 0.06s 
Epoch 6/600 17/37 loss: 1.2093 acc [ 0.628  0.257  0.288]  time 0.06s 
Epoch 6/600 18/37 loss: 1.2004 acc [ 0.631  0.257  0.287]  time 0.06s 
Epoch 6/600 19/37 loss: 1.2174 acc [ 0.631  0.258  0.286]  time 0.06s 
Epoch 6/600 20/37 loss: 1.2090 acc [ 0.635  0.256  0.282]  time 0.06s 
Epoch 6/600 21/37 loss: 1.2062 acc [ 0.635  0.250  0.283]  time 0.06s 
Epoch 6/600 22/37 loss: 1.2044 acc [ 0.635  0.243  0.274]  time 0.07s 
Epoch 6/600 23/37 loss: 1.2164 acc [ 0.632  0.236  0.266]  time 0.06s 
Epoch 6/600 24/37 loss: 1.2095 acc [ 0.630  0.238  0.267]  time 0.06s 
Epoch 6/600 25/37 loss: 1.2101 acc [ 0.630  0.235  0.262]  time 0.06s 
Epoch 6/600 26/37 loss: 1.2094 acc [ 0.633  0.233  0.263]  time 0.07s 
Epoch 6/600 27/37 loss: 1.2143 acc [ 0.634  0.231  0.261]  time 0.06s 
Epoch 6/600 28/37 loss: 1.2327 acc [ 0.630  0.228  0.257]  time 0.06s 
Epoch 6/600 29/37 loss: 1.2324 acc [ 0.634  0.228  0.254]  time 0.06s 
Epoch 6/600 30/37 loss: 1.2500 acc [ 0.633  0.227  0.251]  time 0.06s 
Epoch 6/600 31/37 loss: 1.2469 acc [ 0.632  0.225  0.246]  time 0.06s 
Epoch 6/600 32/37 loss: 1.2461 acc [ 0.636  0.230  0.251]  time 0.06s 
Epoch 6/600 33/37 loss: 1.2459 acc [ 0.635  0.228  0.250]  time 0.06s 
Epoch 6/600 34/37 loss: 1.2423 acc [ 0.636  0.232  0.254]  time 0.06s 
Epoch 6/600 35/37 loss: 1.2435 acc [ 0.632  0.227  0.248]  time 0.05s 
Epoch 6/600 36/37 loss: 1.2434 acc [ 0.635  0.230  0.251]  time 0.04s 
Final training  6/599 loss: 1.2434 acc_avg: 0.3722 acc [ 0.635  0.230  0.251] time 2.50s  lr: 1.9999e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 6, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 7.95 hr, running time 0.10 hr, est total time 8.05 hr 

Epoch 7/600 0/37 loss: 1.1321 acc [ 0.481  0.172  0.334]  time 0.34s 
Epoch 7/600 1/37 loss: 1.1490 acc [ 0.556  0.252  0.343]  time 0.06s 
Epoch 7/600 2/37 loss: 1.1536 acc [ 0.573  0.292  0.319]  time 0.06s 
Epoch 7/600 3/37 loss: 1.1711 acc [ 0.609  0.315  0.325]  time 0.06s 
Epoch 7/600 4/37 loss: 1.1819 acc [ 0.619  0.272  0.280]  time 0.06s 
Epoch 7/600 5/37 loss: 1.1594 acc [ 0.621  0.261  0.277]  time 0.06s 
Epoch 7/600 6/37 loss: 1.1997 acc [ 0.606  0.246  0.247]  time 0.06s 
Epoch 7/600 7/37 loss: 1.1938 acc [ 0.608  0.250  0.253]  time 0.06s 
Epoch 7/600 8/37 loss: 1.1735 acc [ 0.613  0.245  0.246]  time 0.06s 
Epoch 7/600 9/37 loss: 1.1695 acc [ 0.611  0.230  0.233]  time 0.06s 
Epoch 7/600 10/37 loss: 1.1744 acc [ 0.622  0.226  0.228]  time 0.06s 
Epoch 7/600 11/37 loss: 1.1652 acc [ 0.632  0.227  0.227]  time 0.06s 
Epoch 7/600 12/37 loss: 1.1632 acc [ 0.628  0.240  0.240]  time 0.06s 
Epoch 7/600 13/37 loss: 1.1549 acc [ 0.637  0.237  0.240]  time 0.06s 
Epoch 7/600 14/37 loss: 1.1615 acc [ 0.639  0.225  0.225]  time 0.06s 
Epoch 7/600 15/37 loss: 1.1595 acc [ 0.643  0.225  0.227]  time 0.06s 
Epoch 7/600 16/37 loss: 1.1491 acc [ 0.652  0.229  0.230]  time 0.08s 
Epoch 7/600 17/37 loss: 1.1574 acc [ 0.643  0.224  0.227]  time 0.06s 
Epoch 7/600 18/37 loss: 1.1562 acc [ 0.638  0.218  0.220]  time 0.07s 
Epoch 7/600 19/37 loss: 1.1535 acc [ 0.639  0.213  0.217]  time 0.07s 
Epoch 7/600 20/37 loss: 1.1467 acc [ 0.644  0.211  0.215]  time 0.06s 
Epoch 7/600 21/37 loss: 1.1508 acc [ 0.647  0.204  0.209]  time 0.06s 
Epoch 7/600 22/37 loss: 1.1610 acc [ 0.644  0.204  0.207]  time 0.07s 
Epoch 7/600 23/37 loss: 1.1663 acc [ 0.647  0.213  0.218]  time 0.06s 
Epoch 7/600 24/37 loss: 1.1680 acc [ 0.643  0.208  0.214]  time 0.07s 
Epoch 7/600 25/37 loss: 1.1707 acc [ 0.639  0.205  0.212]  time 0.06s 
Epoch 7/600 26/37 loss: 1.1703 acc [ 0.630  0.205  0.212]  time 0.06s 
Epoch 7/600 27/37 loss: 1.1725 acc [ 0.632  0.215  0.222]  time 0.06s 
Epoch 7/600 28/37 loss: 1.1761 acc [ 0.631  0.214  0.221]  time 0.06s 
Epoch 7/600 29/37 loss: 1.1755 acc [ 0.635  0.214  0.221]  time 0.06s 
Epoch 7/600 30/37 loss: 1.1720 acc [ 0.637  0.214  0.224]  time 0.06s 
Epoch 7/600 31/37 loss: 1.1738 acc [ 0.631  0.213  0.223]  time 0.06s 
Epoch 7/600 32/37 loss: 1.1665 acc [ 0.630  0.215  0.226]  time 0.06s 
Epoch 7/600 33/37 loss: 1.1734 acc [ 0.629  0.215  0.225]  time 0.06s 
Epoch 7/600 34/37 loss: 1.1926 acc [ 0.626  0.213  0.223]  time 0.06s 
Epoch 7/600 35/37 loss: 1.1887 acc [ 0.624  0.214  0.221]  time 0.06s 
Epoch 7/600 36/37 loss: 1.1904 acc [ 0.624  0.211  0.218]  time 0.04s 
Final training  7/599 loss: 1.1904 acc_avg: 0.3509 acc [ 0.624  0.211  0.218] time 2.48s  lr: 1.9998e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 7, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 7.94 hr, running time 0.10 hr, est total time 8.05 hr 

Epoch 8/600 0/37 loss: 1.0596 acc [ 0.729  0.322  0.311]  time 0.32s 
Epoch 8/600 1/37 loss: 1.0620 acc [ 0.654  0.279  0.298]  time 0.06s 
Epoch 8/600 2/37 loss: 1.1266 acc [ 0.645  0.268  0.284]  time 0.06s 
Epoch 8/600 3/37 loss: 1.1398 acc [ 0.662  0.281  0.245]  time 0.06s 
Epoch 8/600 4/37 loss: 1.1604 acc [ 0.646  0.255  0.239]  time 0.06s 
Epoch 8/600 5/37 loss: 1.1409 acc [ 0.665  0.251  0.249]  time 0.06s 
Epoch 8/600 6/37 loss: 1.1060 acc [ 0.676  0.266  0.280]  time 0.06s 
Epoch 8/600 7/37 loss: 1.1201 acc [ 0.658  0.274  0.275]  time 0.06s 
Epoch 8/600 8/37 loss: 1.1107 acc [ 0.659  0.277  0.288]  time 0.06s 
Epoch 8/600 9/37 loss: 1.1153 acc [ 0.645  0.271  0.280]  time 0.06s 
Epoch 8/600 10/37 loss: 1.1161 acc [ 0.633  0.265  0.271]  time 0.05s 
Epoch 8/600 11/37 loss: 1.1085 acc [ 0.637  0.275  0.282]  time 0.06s 
Epoch 8/600 12/37 loss: 1.1200 acc [ 0.624  0.268  0.269]  time 0.06s 
Epoch 8/600 13/37 loss: 1.1332 acc [ 0.625  0.257  0.257]  time 0.06s 
Epoch 8/600 14/37 loss: 1.1233 acc [ 0.612  0.260  0.259]  time 0.06s 
Epoch 8/600 15/37 loss: 1.1167 acc [ 0.614  0.263  0.266]  time 0.06s 
Epoch 8/600 16/37 loss: 1.1021 acc [ 0.622  0.272  0.277]  time 0.06s 
Epoch 8/600 17/37 loss: 1.1158 acc [ 0.620  0.266  0.270]  time 0.06s 
Epoch 8/600 18/37 loss: 1.1505 acc [ 0.627  0.268  0.271]  time 0.06s 
Epoch 8/600 19/37 loss: 1.1500 acc [ 0.624  0.270  0.273]  time 0.06s 
Epoch 8/600 20/37 loss: 1.1422 acc [ 0.628  0.263  0.265]  time 0.06s 
Epoch 8/600 21/37 loss: 1.1396 acc [ 0.629  0.263  0.264]  time 0.07s 
Epoch 8/600 22/37 loss: 1.1511 acc [ 0.623  0.257  0.258]  time 0.06s 
Epoch 8/600 23/37 loss: 1.1401 acc [ 0.622  0.269  0.270]  time 0.06s 
Epoch 8/600 24/37 loss: 1.1884 acc [ 0.615  0.267  0.269]  time 0.06s 
Epoch 8/600 25/37 loss: 1.1830 acc [ 0.619  0.274  0.279]  time 0.06s 
Epoch 8/600 26/37 loss: 1.1805 acc [ 0.619  0.270  0.274]  time 0.06s 
Epoch 8/600 27/37 loss: 1.1754 acc [ 0.622  0.279  0.285]  time 0.06s 
Epoch 8/600 28/37 loss: 1.1754 acc [ 0.623  0.275  0.280]  time 0.06s 
Epoch 8/600 29/37 loss: 1.1712 acc [ 0.623  0.279  0.284]  time 0.06s 
Epoch 8/600 30/37 loss: 1.1680 acc [ 0.614  0.279  0.284]  time 0.06s 
Epoch 8/600 31/37 loss: 1.1738 acc [ 0.615  0.278  0.282]  time 0.06s 
Epoch 8/600 32/37 loss: 1.1771 acc [ 0.611  0.272  0.278]  time 0.05s 
Epoch 8/600 33/37 loss: 1.1743 acc [ 0.612  0.268  0.277]  time 0.05s 
Epoch 8/600 34/37 loss: 1.1727 acc [ 0.613  0.270  0.279]  time 0.06s 
Epoch 8/600 35/37 loss: 1.1733 acc [ 0.616  0.266  0.277]  time 0.06s 
Epoch 8/600 36/37 loss: 1.1717 acc [ 0.619  0.266  0.277]  time 0.04s 
Final training  8/599 loss: 1.1717 acc_avg: 0.3873 acc [ 0.619  0.266  0.277] time 2.43s  lr: 1.9997e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 8, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 7.94 hr, running time 0.11 hr, est total time 8.04 hr 

Epoch 9/600 0/37 loss: 1.7111 acc [ 0.528  0.301  0.365]  time 0.26s 
Epoch 9/600 1/37 loss: 1.4304 acc [ 0.501  0.285  0.314]  time 0.06s 
Epoch 9/600 2/37 loss: 1.4770 acc [ 0.548  0.300  0.329]  time 0.06s 
Epoch 9/600 3/37 loss: 1.3831 acc [ 0.581  0.290  0.327]  time 0.06s 
Epoch 9/600 4/37 loss: 1.3680 acc [ 0.605  0.318  0.366]  time 0.07s 
Epoch 9/600 5/37 loss: 1.3398 acc [ 0.622  0.291  0.337]  time 0.06s 
Epoch 9/600 6/37 loss: 1.3222 acc [ 0.624  0.268  0.304]  time 0.06s 
Epoch 9/600 7/37 loss: 1.3094 acc [ 0.620  0.253  0.282]  time 0.07s 
Epoch 9/600 8/37 loss: 1.2929 acc [ 0.632  0.260  0.299]  time 0.07s 
Epoch 9/600 9/37 loss: 1.3091 acc [ 0.617  0.249  0.280]  time 0.07s 
Epoch 9/600 10/37 loss: 1.2922 acc [ 0.632  0.242  0.269]  time 0.07s 
Epoch 9/600 11/37 loss: 1.2890 acc [ 0.622  0.240  0.264]  time 0.06s 
Epoch 9/600 12/37 loss: 1.2944 acc [ 0.616  0.229  0.253]  time 0.06s 
Epoch 9/600 13/37 loss: 1.2833 acc [ 0.619  0.227  0.253]  time 0.06s 
Epoch 9/600 14/37 loss: 1.2654 acc [ 0.628  0.236  0.249]  time 0.06s 
Epoch 9/600 15/37 loss: 1.2551 acc [ 0.632  0.236  0.254]  time 0.06s 
Epoch 9/600 16/37 loss: 1.2444 acc [ 0.631  0.238  0.255]  time 0.06s 
Epoch 9/600 17/37 loss: 1.2364 acc [ 0.627  0.244  0.262]  time 0.06s 
Epoch 9/600 18/37 loss: 1.2213 acc [ 0.621  0.255  0.271]  time 0.06s 
Epoch 9/600 19/37 loss: 1.2175 acc [ 0.620  0.252  0.268]  time 0.06s 
Epoch 9/600 20/37 loss: 1.2175 acc [ 0.625  0.250  0.268]  time 0.06s 
Epoch 9/600 21/37 loss: 1.2136 acc [ 0.626  0.253  0.272]  time 0.06s 
Epoch 9/600 22/37 loss: 1.2069 acc [ 0.622  0.251  0.269]  time 0.06s 
Epoch 9/600 23/37 loss: 1.1975 acc [ 0.623  0.253  0.271]  time 0.06s 
Epoch 9/600 24/37 loss: 1.1919 acc [ 0.624  0.258  0.274]  time 0.06s 
Epoch 9/600 25/37 loss: 1.1908 acc [ 0.624  0.263  0.278]  time 0.07s 
Epoch 9/600 26/37 loss: 1.1829 acc [ 0.619  0.262  0.280]  time 0.06s 
Epoch 9/600 27/37 loss: 1.1800 acc [ 0.621  0.264  0.280]  time 0.06s 
Epoch 9/600 28/37 loss: 1.1805 acc [ 0.618  0.260  0.275]  time 0.07s 
Epoch 9/600 29/37 loss: 1.1739 acc [ 0.618  0.256  0.271]  time 0.06s 
Epoch 9/600 30/37 loss: 1.1779 acc [ 0.618  0.253  0.267]  time 0.07s 
Epoch 9/600 31/37 loss: 1.1676 acc [ 0.623  0.255  0.271]  time 0.06s 
Epoch 9/600 32/37 loss: 1.1610 acc [ 0.627  0.259  0.275]  time 0.06s 
Epoch 9/600 33/37 loss: 1.1568 acc [ 0.627  0.258  0.278]  time 0.06s 
Epoch 9/600 34/37 loss: 1.1480 acc [ 0.633  0.258  0.277]  time 0.06s 
Epoch 9/600 35/37 loss: 1.1463 acc [ 0.631  0.257  0.276]  time 0.06s 
Epoch 9/600 36/37 loss: 1.1472 acc [ 0.631  0.259  0.277]  time 0.04s 
Final training  9/599 loss: 1.1472 acc_avg: 0.3891 acc [ 0.631  0.259  0.277] time 2.43s  lr: 1.9995e-04
Val 9/600 0/196  loss: 0.9398 acc [ 0.153  0.006  0.006]  time 2.89s ['/workspace/workspace-kits23/kits23/dataset/case_00004/imaging.nii.gz']
Val 9/600 1/196  loss: 0.9539 acc [ 0.081  0.019  0.019]  time 34.50s ['/workspace/workspace-kits23/kits23/dataset/case_00005/imaging.nii.gz']
Val 9/600 2/196  loss: 0.9723 acc [ 0.069  0.001  0.001]  time 3.05s ['/workspace/workspace-kits23/kits23/dataset/case_00006/imaging.nii.gz']
Val 9/600 3/196  loss: 0.9530 acc [ 0.111  0.000  0.000]  time 1.21s ['/workspace/workspace-kits23/kits23/dataset/case_00011/imaging.nii.gz']
Val 9/600 4/196  loss: 0.9655 acc [ 0.087  0.000  0.000]  time 1.81s ['/workspace/workspace-kits23/kits23/dataset/case_00017/imaging.nii.gz']
Val 9/600 5/196  loss: 0.9445 acc [ 0.102  0.020  0.021]  time 2.11s ['/workspace/workspace-kits23/kits23/dataset/case_00029/imaging.nii.gz']
Val 9/600 6/196  loss: 0.9665 acc [ 0.084  0.002  0.002]  time 2.42s ['/workspace/workspace-kits23/kits23/dataset/case_00031/imaging.nii.gz']
Val 9/600 7/196  loss: 0.9567 acc [ 0.104  0.003  0.003]  time 1.83s ['/workspace/workspace-kits23/kits23/dataset/case_00034/imaging.nii.gz']
Val 9/600 8/196  loss: 0.9666 acc [ 0.064  0.012  0.012]  time 3.01s ['/workspace/workspace-kits23/kits23/dataset/case_00047/imaging.nii.gz']
Val 9/600 9/196  loss: 0.9563 acc [ 0.115  0.000  0.000]  time 1.02s ['/workspace/workspace-kits23/kits23/dataset/case_00062/imaging.nii.gz']
Val 9/600 10/196  loss: 0.9472 acc [ 0.139  0.006  0.006]  time 1.66s ['/workspace/workspace-kits23/kits23/dataset/case_00065/imaging.nii.gz']
Val 9/600 11/196  loss: 0.9772 acc [ 0.041  0.010  0.010]  time 9.25s ['/workspace/workspace-kits23/kits23/dataset/case_00066/imaging.nii.gz']
Val 9/600 12/196  loss: 0.7367 acc [ 0.312  0.309  0.307]  time 5.91s ['/workspace/workspace-kits23/kits23/dataset/case_00067/imaging.nii.gz']
Val 9/600 13/196  loss: 0.9604 acc [ 0.095  0.002  0.002]  time 1.14s ['/workspace/workspace-kits23/kits23/dataset/case_00085/imaging.nii.gz']
Val 9/600 14/196  loss: 0.8902 acc [ 0.140  0.077  0.075]  time 0.96s ['/workspace/workspace-kits23/kits23/dataset/case_00090/imaging.nii.gz']
Val 9/600 15/196  loss: 0.8162 acc [ 0.173  0.169  0.168]  time 1.90s ['/workspace/workspace-kits23/kits23/dataset/case_00092/imaging.nii.gz']
Val 9/600 16/196  loss: 0.8249 acc [ 0.188  0.168  0.163]  time 6.12s ['/workspace/workspace-kits23/kits23/dataset/case_00102/imaging.nii.gz']
Val 9/600 17/196  loss: 0.7783 acc [ 0.191  0.238  0.236]  time 1.09s ['/workspace/workspace-kits23/kits23/dataset/case_00107/imaging.nii.gz']
Val 9/600 18/196  loss: 0.9284 acc [ 0.162  0.017  0.011]  time 0.57s ['/workspace/workspace-kits23/kits23/dataset/case_00110/imaging.nii.gz']
Val 9/600 19/196  loss: 0.8560 acc [ 0.304  0.060  0.023]  time 1.02s ['/workspace/workspace-kits23/kits23/dataset/case_00117/imaging.nii.gz']
Val 9/600 20/196  loss: 0.9101 acc [ 0.134  0.069  0.058]  time 1.65s ['/workspace/workspace-kits23/kits23/dataset/case_00119/imaging.nii.gz']
Val 9/600 21/196  loss: 0.9737 acc [ 0.061  0.001  0.001]  time 4.37s ['/workspace/workspace-kits23/kits23/dataset/case_00128/imaging.nii.gz']
Val 9/600 22/196  loss: 0.9541 acc [ 0.114  0.003  0.003]  time 11.35s ['/workspace/workspace-kits23/kits23/dataset/case_00132/imaging.nii.gz']
Val 9/600 23/196  loss: 0.9473 acc [ 0.110  0.010  0.010]  time 5.35s ['/workspace/workspace-kits23/kits23/dataset/case_00142/imaging.nii.gz']
Val 9/600 24/196  loss: 0.9385 acc [ 0.147  0.006  0.005]  time 1.07s ['/workspace/workspace-kits23/kits23/dataset/case_00147/imaging.nii.gz']
Val 9/600 25/196  loss: 0.9438 acc [ 0.104  0.018  0.017]  time 17.46s ['/workspace/workspace-kits23/kits23/dataset/case_00159/imaging.nii.gz']
Val 9/600 26/196  loss: 0.9589 acc [ 0.101  0.001  0.001]  time 1.19s ['/workspace/workspace-kits23/kits23/dataset/case_00164/imaging.nii.gz']
Val 9/600 27/196  loss: 0.9866 acc [ 0.033  0.000  0.000]  time 15.77s ['/workspace/workspace-kits23/kits23/dataset/case_00165/imaging.nii.gz']
Val 9/600 28/196  loss: 0.7984 acc [ 0.184  0.196  0.194]  time 1.34s ['/workspace/workspace-kits23/kits23/dataset/case_00172/imaging.nii.gz']
Val 9/600 29/196  loss: 0.9587 acc [ 0.101  0.004  0.003]  time 1.62s ['/workspace/workspace-kits23/kits23/dataset/case_00173/imaging.nii.gz']
Val 9/600 30/196  loss: 0.9728 acc [ 0.067  0.002  0.001]  time 5.53s ['/workspace/workspace-kits23/kits23/dataset/case_00185/imaging.nii.gz']
Val 9/600 31/196  loss: 0.9422 acc [ 0.085  0.042  0.042]  time 2.70s ['/workspace/workspace-kits23/kits23/dataset/case_00189/imaging.nii.gz']
Val 9/600 32/196  loss: 0.8299 acc [ 0.164  0.171  0.169]  time 2.18s ['/workspace/workspace-kits23/kits23/dataset/case_00192/imaging.nii.gz']
Val 9/600 33/196  loss: 0.9396 acc [ 0.079  0.044  0.043]  time 3.26s ['/workspace/workspace-kits23/kits23/dataset/case_00197/imaging.nii.gz']
Val 9/600 34/196  loss: 0.9249 acc [ 0.179  0.002  0.002]  time 1.93s ['/workspace/workspace-kits23/kits23/dataset/case_00201/imaging.nii.gz']
Val 9/600 35/196  loss: 0.9669 acc [ 0.082  0.000  0.000]  time 1.64s ['/workspace/workspace-kits23/kits23/dataset/case_00208/imaging.nii.gz']
Val 9/600 36/196  loss: 0.9830 acc [ 0.040  0.001  0.001]  time 12.46s ['/workspace/workspace-kits23/kits23/dataset/case_00213/imaging.nii.gz']
Val 9/600 37/196  loss: 0.9682 acc [ 0.074  0.001  0.001]  time 3.23s ['/workspace/workspace-kits23/kits23/dataset/case_00218/imaging.nii.gz']
Val 9/600 38/196  loss: 0.9551 acc [ 0.104  0.008  0.008]  time 1.86s ['/workspace/workspace-kits23/kits23/dataset/case_00225/imaging.nii.gz']
Val 9/600 39/196  loss: 0.9237 acc [ 0.124  0.039  0.039]  time 1.71s ['/workspace/workspace-kits23/kits23/dataset/case_00230/imaging.nii.gz']
Val 9/600 40/196  loss: 0.7351 acc [ 0.251  0.308  0.308]  time 1.83s ['/workspace/workspace-kits23/kits23/dataset/case_00233/imaging.nii.gz']
Val 9/600 41/196  loss: 0.8787 acc [ 0.125  0.119  0.118]  time 2.86s ['/workspace/workspace-kits23/kits23/dataset/case_00245/imaging.nii.gz']
Val 9/600 42/196  loss: 0.9370 acc [ 0.080  0.046  0.046]  time 14.07s ['/workspace/workspace-kits23/kits23/dataset/case_00246/imaging.nii.gz']
Val 9/600 43/196  loss: 0.9692 acc [ 0.076  0.003  0.003]  time 4.79s ['/workspace/workspace-kits23/kits23/dataset/case_00250/imaging.nii.gz']
Val 9/600 44/196  loss: 0.9047 acc [ 0.191  0.062  0.001]  time 1.15s ['/workspace/workspace-kits23/kits23/dataset/case_00256/imaging.nii.gz']
Val 9/600 45/196  loss: 0.9201 acc [ 0.145  0.036  0.035]  time 0.54s ['/workspace/workspace-kits23/kits23/dataset/case_00260/imaging.nii.gz']
Val 9/600 46/196  loss: 0.9507 acc [ 0.085  0.015  0.015]  time 13.09s ['/workspace/workspace-kits23/kits23/dataset/case_00261/imaging.nii.gz']
Val 9/600 47/196  loss: 0.9541 acc [ 0.098  0.009  0.009]  time 4.98s ['/workspace/workspace-kits23/kits23/dataset/case_00273/imaging.nii.gz']
Val 9/600 48/196  loss: 0.8981 acc [ 0.141  0.061  0.060]  time 1.75s ['/workspace/workspace-kits23/kits23/dataset/case_00275/imaging.nii.gz']
Val 9/600 49/196  loss: 0.9328 acc [ 0.079  0.051  0.051]  time 4.87s ['/workspace/workspace-kits23/kits23/dataset/case_00284/imaging.nii.gz']
Val 9/600 50/196  loss: 0.8913 acc [ 0.221  0.032  0.030]  time 0.55s ['/workspace/workspace-kits23/kits23/dataset/case_00287/imaging.nii.gz']
Val 9/600 51/196  loss: 0.9603 acc [ 0.085  0.002  0.002]  time 14.31s ['/workspace/workspace-kits23/kits23/dataset/case_00290/imaging.nii.gz']
Val 9/600 52/196  loss: 0.9564 acc [ 0.079  0.012  0.012]  time 2.35s ['/workspace/workspace-kits23/kits23/dataset/case_00291/imaging.nii.gz']
Val 9/600 53/196  loss: 0.9755 acc [ 0.056  0.004  0.004]  time 13.36s ['/workspace/workspace-kits23/kits23/dataset/case_00294/imaging.nii.gz']
Val 9/600 54/196  loss: 0.9552 acc [ 0.105  0.008  0.008]  time 12.67s ['/workspace/workspace-kits23/kits23/dataset/case_00295/imaging.nii.gz']
Val 9/600 55/196  loss: 0.9331 acc [ 0.115  0.027  0.027]  time 14.43s ['/workspace/workspace-kits23/kits23/dataset/case_00298/imaging.nii.gz']
Val 9/600 56/196  loss: 0.9751 acc [ 0.060  0.005  0.001]  time 2.07s ['/workspace/workspace-kits23/kits23/dataset/case_00400/imaging.nii.gz']
Val 9/600 57/196  loss: 0.9051 acc [ 0.181  0.053  0.017]  time 0.60s ['/workspace/workspace-kits23/kits23/dataset/case_00403/imaging.nii.gz']
Val 9/600 58/196  loss: 0.9701 acc [ 0.063  0.014  0.000]  time 2.71s ['/workspace/workspace-kits23/kits23/dataset/case_00404/imaging.nii.gz']
Val 9/600 59/196  loss: 0.9622 acc [ 0.094  0.002  0.002]  time 1.83s ['/workspace/workspace-kits23/kits23/dataset/case_00414/imaging.nii.gz']
Val 9/600 60/196  loss: 0.9427 acc [ 0.131  0.007  0.007]  time 2.57s ['/workspace/workspace-kits23/kits23/dataset/case_00415/imaging.nii.gz']
Val 9/600 61/196  loss: 0.8376 acc [ 0.238  0.100  0.102]  time 0.58s ['/workspace/workspace-kits23/kits23/dataset/case_00418/imaging.nii.gz']
Val 9/600 62/196  loss: 0.9380 acc [ 0.168  0.002  0.002]  time 0.57s ['/workspace/workspace-kits23/kits23/dataset/case_00422/imaging.nii.gz']
Val 9/600 63/196  loss: 0.8462 acc [ 0.135  0.148  0.145]  time 1.44s ['/workspace/workspace-kits23/kits23/dataset/case_00426/imaging.nii.gz']
Val 9/600 64/196  loss: 0.7249 acc [ 0.404  0.440  0.440]  time 0.53s ['/workspace/workspace-kits23/kits23/dataset/case_00430/imaging.nii.gz']
Val 9/600 65/196  loss: 0.9288 acc [ 0.096  0.055  0.056]  time 2.01s ['/workspace/workspace-kits23/kits23/dataset/case_00431/imaging.nii.gz']
Val 9/600 66/196  loss: 0.9380 acc [ 0.147  0.001  0.000]  time 0.65s ['/workspace/workspace-kits23/kits23/dataset/case_00434/imaging.nii.gz']
Val 9/600 67/196  loss: 0.8890 acc [ 0.185  0.052  0.048]  time 1.20s ['/workspace/workspace-kits23/kits23/dataset/case_00439/imaging.nii.gz']
Val 9/600 68/196  loss: 0.9767 acc [ 0.058  0.000  0.000]  time 4.11s ['/workspace/workspace-kits23/kits23/dataset/case_00447/imaging.nii.gz']
Val 9/600 69/196  loss: 0.9166 acc [ 0.160  0.033  0.032]  time 1.36s ['/workspace/workspace-kits23/kits23/dataset/case_00458/imaging.nii.gz']
Val 9/600 70/196  loss: 0.9735 acc [ 0.061  0.002  0.002]  time 1.91s ['/workspace/workspace-kits23/kits23/dataset/case_00462/imaging.nii.gz']
Val 9/600 71/196  loss: 0.8863 acc [ 0.268  0.020  0.020]  time 0.68s ['/workspace/workspace-kits23/kits23/dataset/case_00464/imaging.nii.gz']
Val 9/600 72/196  loss: 0.9069 acc [ 0.166  0.076  0.009]  time 0.58s ['/workspace/workspace-kits23/kits23/dataset/case_00470/imaging.nii.gz']
Val 9/600 73/196  loss: 0.9317 acc [ 0.133  0.021  0.020]  time 1.86s ['/workspace/workspace-kits23/kits23/dataset/case_00475/imaging.nii.gz']
Val 9/600 74/196  loss: 0.9215 acc [ 0.187  0.004  0.004]  time 0.54s ['/workspace/workspace-kits23/kits23/dataset/case_00476/imaging.nii.gz']
Val 9/600 75/196  loss: 0.8904 acc [ 0.194  0.041  0.041]  time 1.07s ['/workspace/workspace-kits23/kits23/dataset/case_00485/imaging.nii.gz']
Val 9/600 76/196  loss: 0.9239 acc [ 0.119  0.051  0.050]  time 3.49s ['/workspace/workspace-kits23/kits23/dataset/case_00489/imaging.nii.gz']
Val 9/600 77/196  loss: 0.9541 acc [ 0.101  0.002  0.001]  time 1.67s ['/workspace/workspace-kits23/kits23/dataset/case_00492/imaging.nii.gz']
Val 9/600 78/196  loss: 0.9502 acc [ 0.112  0.008  0.005]  time 1.69s ['/workspace/workspace-kits23/kits23/dataset/case_00494/imaging.nii.gz']
Val 9/600 79/196  loss: 0.9693 acc [ 0.071  0.003  0.001]  time 4.92s ['/workspace/workspace-kits23/kits23/dataset/case_00504/imaging.nii.gz']
Val 9/600 80/196  loss: 0.9743 acc [ 0.066  0.000  0.000]  time 3.93s ['/workspace/workspace-kits23/kits23/dataset/case_00509/imaging.nii.gz']
Val 9/600 81/196  loss: 0.9663 acc [ 0.050  0.021  0.020]  time 16.43s ['/workspace/workspace-kits23/kits23/dataset/case_00510/imaging.nii.gz']
Val 9/600 82/196  loss: 0.9450 acc [ 0.128  0.004  0.002]  time 1.04s ['/workspace/workspace-kits23/kits23/dataset/case_00512/imaging.nii.gz']
Val 9/600 83/196  loss: 0.6860 acc [ 0.443  0.520  0.519]  time 1.81s ['/workspace/workspace-kits23/kits23/dataset/case_00516/imaging.nii.gz']
Val 9/600 84/196  loss: 0.9694 acc [ 0.077  0.000  0.000]  time 1.64s ['/workspace/workspace-kits23/kits23/dataset/case_00520/imaging.nii.gz']
Val 9/600 85/196  loss: 0.9644 acc [ 0.083  0.005  0.005]  time 11.01s ['/workspace/workspace-kits23/kits23/dataset/case_00528/imaging.nii.gz']
Val 9/600 86/196  loss: 0.9534 acc [ 0.097  0.024  0.021]  time 2.44s ['/workspace/workspace-kits23/kits23/dataset/case_00532/imaging.nii.gz']
Val 9/600 87/196  loss: 0.9083 acc [ 0.187  0.026  0.025]  time 0.51s ['/workspace/workspace-kits23/kits23/dataset/case_00534/imaging.nii.gz']
Val 9/600 88/196  loss: 0.9697 acc [ 0.055  0.012  0.011]  time 13.03s ['/workspace/workspace-kits23/kits23/dataset/case_00535/imaging.nii.gz']
Val 9/600 89/196  loss: 0.9896 acc [ 0.019  0.005  0.002]  time 9.95s ['/workspace/workspace-kits23/kits23/dataset/case_00540/imaging.nii.gz']
Val 9/600 90/196  loss: 0.8630 acc [ 0.304  0.063  0.062]  time 0.52s ['/workspace/workspace-kits23/kits23/dataset/case_00546/imaging.nii.gz']
Val 9/600 91/196  loss: 0.8720 acc [ 0.140  0.102  0.101]  time 2.91s ['/workspace/workspace-kits23/kits23/dataset/case_00552/imaging.nii.gz']
Val 9/600 92/196  loss: 0.9655 acc [ 0.072  0.014  0.013]  time 9.08s ['/workspace/workspace-kits23/kits23/dataset/case_00553/imaging.nii.gz']
Val 9/600 93/196  loss: 0.9689 acc [ 0.081  0.001  0.001]  time 3.08s ['/workspace/workspace-kits23/kits23/dataset/case_00557/imaging.nii.gz']
Val 9/600 94/196  loss: 0.9487 acc [ 0.116  0.007  0.007]  time 10.41s ['/workspace/workspace-kits23/kits23/dataset/case_00559/imaging.nii.gz']
Val 9/600 95/196  loss: 0.8852 acc [ 0.300  0.000  0.000]  time 0.53s ['/workspace/workspace-kits23/kits23/dataset/case_00569/imaging.nii.gz']
Val 9/600 96/196  loss: 0.9298 acc [ 0.168  0.000  0.000]  time 0.55s ['/workspace/workspace-kits23/kits23/dataset/case_00574/imaging.nii.gz']
Val 9/600 97/196  loss: 0.9384 acc [ 0.095  0.061  0.002]  time 2.92s ['/workspace/workspace-kits23/kits23/dataset/case_00586/imaging.nii.gz']
Val 9/600 98/196  loss: 0.8535 acc [ 0.136  0.137  0.134]  time 1.76s ['/workspace/workspace-kits23/kits23/dataset/case_00012/imaging.nii.gz']
Val 9/600 99/196  loss: 0.9409 acc [ 0.109  0.023  0.022]  time 1.63s ['/workspace/workspace-kits23/kits23/dataset/case_00013/imaging.nii.gz']
Val 9/600 100/196  loss: 0.9438 acc [ 0.128  0.006  0.007]  time 2.31s ['/workspace/workspace-kits23/kits23/dataset/case_00018/imaging.nii.gz']
Val 9/600 101/196  loss: 0.9457 acc [ 0.135  0.002  0.002]  time 1.55s ['/workspace/workspace-kits23/kits23/dataset/case_00023/imaging.nii.gz']
Val 9/600 102/196  loss: 0.9675 acc [ 0.073  0.000  0.000]  time 15.70s ['/workspace/workspace-kits23/kits23/dataset/case_00027/imaging.nii.gz']
Val 9/600 103/196  loss: 0.9629 acc [ 0.095  0.000  0.000]  time 1.79s ['/workspace/workspace-kits23/kits23/dataset/case_00039/imaging.nii.gz']
Val 9/600 104/196  loss: 0.9614 acc [ 0.096  0.003  0.003]  time 4.19s ['/workspace/workspace-kits23/kits23/dataset/case_00040/imaging.nii.gz']
Val 9/600 105/196  loss: 0.9083 acc [ 0.192  0.023  0.023]  time 0.55s ['/workspace/workspace-kits23/kits23/dataset/case_00041/imaging.nii.gz']
Val 9/600 106/196  loss: 0.9606 acc [ 0.076  0.012  0.011]  time 2.72s ['/workspace/workspace-kits23/kits23/dataset/case_00046/imaging.nii.gz']
Val 9/600 107/196  loss: 0.9215 acc [ 0.131  0.042  0.036]  time 1.84s ['/workspace/workspace-kits23/kits23/dataset/case_00058/imaging.nii.gz']
Val 9/600 108/196  loss: 0.9651 acc [ 0.073  0.004  0.004]  time 15.60s ['/workspace/workspace-kits23/kits23/dataset/case_00059/imaging.nii.gz']
Val 9/600 109/196  loss: 0.8328 acc [ 0.318  0.082  0.081]  time 0.58s ['/workspace/workspace-kits23/kits23/dataset/case_00061/imaging.nii.gz']
Val 9/600 110/196  loss: 0.9252 acc [ 0.122  0.037  0.026]  time 1.75s ['/workspace/workspace-kits23/kits23/dataset/case_00069/imaging.nii.gz']
Val 9/600 111/196  loss: 0.9223 acc [ 0.101  0.052  0.051]  time 2.89s ['/workspace/workspace-kits23/kits23/dataset/case_00073/imaging.nii.gz']
Val 9/600 112/196  loss: 0.8479 acc [ 0.200  0.131  0.131]  time 6.35s ['/workspace/workspace-kits23/kits23/dataset/case_00078/imaging.nii.gz']
Val 9/600 113/196  loss: 0.9510 acc [ 0.102  0.013  0.013]  time 1.13s ['/workspace/workspace-kits23/kits23/dataset/case_00080/imaging.nii.gz']
Val 9/600 114/196  loss: 0.9608 acc [ 0.099  0.000  0.000]  time 3.13s ['/workspace/workspace-kits23/kits23/dataset/case_00081/imaging.nii.gz']
Val 9/600 115/196  loss: 0.9596 acc [ 0.093  0.003  0.003]  time 2.09s ['/workspace/workspace-kits23/kits23/dataset/case_00082/imaging.nii.gz']
Val 9/600 116/196  loss: 0.8915 acc [ 0.196  0.047  0.047]  time 0.56s ['/workspace/workspace-kits23/kits23/dataset/case_00089/imaging.nii.gz']
Val 9/600 117/196  loss: 0.9667 acc [ 0.076  0.004  0.004]  time 19.25s ['/workspace/workspace-kits23/kits23/dataset/case_00091/imaging.nii.gz']
Val 9/600 118/196  loss: 0.9627 acc [ 0.082  0.007  0.002]  time 19.92s ['/workspace/workspace-kits23/kits23/dataset/case_00093/imaging.nii.gz']
Val 9/600 119/196  loss: 0.9651 acc [ 0.065  0.014  0.013]  time 6.62s ['/workspace/workspace-kits23/kits23/dataset/case_00095/imaging.nii.gz']
Val 9/600 120/196  loss: 0.9400 acc [ 0.130  0.013  0.013]  time 5.18s ['/workspace/workspace-kits23/kits23/dataset/case_00098/imaging.nii.gz']
Val 9/600 121/196  loss: 0.9578 acc [ 0.100  0.001  0.001]  time 9.60s ['/workspace/workspace-kits23/kits23/dataset/case_00101/imaging.nii.gz']
Val 9/600 122/196  loss: 0.9688 acc [ 0.081  0.001  0.001]  time 1.64s ['/workspace/workspace-kits23/kits23/dataset/case_00106/imaging.nii.gz']
Val 9/600 123/196  loss: 0.9792 acc [ 0.051  0.001  0.000]  time 6.72s ['/workspace/workspace-kits23/kits23/dataset/case_00120/imaging.nii.gz']
Val 9/600 124/196  loss: 0.9110 acc [ 0.225  0.012  0.012]  time 0.65s ['/workspace/workspace-kits23/kits23/dataset/case_00122/imaging.nii.gz']
Val 9/600 125/196  loss: 0.9791 acc [ 0.055  0.001  0.000]  time 3.15s ['/workspace/workspace-kits23/kits23/dataset/case_00125/imaging.nii.gz']
Val 9/600 126/196  loss: 0.9568 acc [ 0.105  0.004  0.004]  time 1.45s ['/workspace/workspace-kits23/kits23/dataset/case_00127/imaging.nii.gz']
Val 9/600 127/196  loss: 0.8452 acc [ 0.173  0.140  0.139]  time 13.79s ['/workspace/workspace-kits23/kits23/dataset/case_00135/imaging.nii.gz']
Val 9/600 128/196  loss: 0.7559 acc [ 0.214  0.246  0.245]  time 1.51s ['/workspace/workspace-kits23/kits23/dataset/case_00139/imaging.nii.gz']
Val 9/600 129/196  loss: 0.9497 acc [ 0.123  0.003  0.003]  time 14.96s ['/workspace/workspace-kits23/kits23/dataset/case_00146/imaging.nii.gz']
_meta_: {}
acc: null
algos: segresnet2d
amp: true
anisotropic_scales: false
auto_scale_allowed: false
auto_scale_batch: true
auto_scale_filters: false
auto_scale_roi: false
batch_size: 8
bundle_root: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0
cache_class_indices: null
cache_rate: null
calc_val_loss: true
channels_last: true
ckpt_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
ckpt_save: true
class_index:
- - 1
  - 2
  - 3
- - 2
  - 3
- - 2
class_names:
- kidney_and_mass
- mass
- tumor
crop_mode: ratio
crop_ratios: null
cuda: true
data_file_base_dir: /workspace/workspace-kits23/kits23
data_list_file_path: /workspace/workspace-kits23/exp/kits23-train_val/kits23_folds-train_val.json
debug: true
determ: false
early_stopping_fraction: 0.001
ensemble: false
extra_modalities: {}
finetune:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
fold: 0
fork: true
global_rank: 0
image_size:
- 623
- 623
- 707
image_size_mm_90:
- 487.2
- 487.2
- 552.8499999999999
image_size_mm_median:
- 400.0
- 400.0
- 417.0
infer:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  data_list_key: testing
  enabled: false
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_testing
input_channels: 1
intensity_bounds:
- -54.36023523373594
- 242.71830265848672
learning_rate: 0.0002
log_output_file: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/training.log
loss:
  _target_: DiceCELoss
  batch: true
  include_background: true
  sigmoid: true
  smooth_dr: 1.0e-05
  smooth_nr: 0
  softmax: false
  squared_pred: true
  to_onehot_y: false
max_samples_per_class: 6000
mlflow_experiment_name: Auto3DSeg
mlflow_tracking_uri: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/mlruns/
modality: ct
network:
  _target_: SegResNetDS
  blocks_down:
  - 1
  - 2
  - 2
  - 4
  - 4
  dsdepth: 2
  in_channels: 1
  init_filters: 32
  norm: BATCH
  out_channels: 3
  spatial_dims: 2
normalize_mode: range
num_crops_per_image: 1
num_epochs: 600
num_epochs_per_saving: 1
num_epochs_per_validation: null
num_fold: 1
num_images_per_batch: 1
num_steps_per_image: null
num_warmup_epochs: 3
num_workers: 4
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0002
  weight_decay: 1.0e-05
output_classes: 3
pretrained_ckpt_name: null
quick: false
rank: 0
resample: false
resample_resolution:
- 0.78125
- 0.78125
- 0.78125
roi_size:
- 64
- 64
- 64
sigmoid: true
spacing_lower:
- 0.4602125036716461
- 0.4602123200893402
- 0.5
spacing_median:
- 0.78125
- 0.78125
- 3.0
spacing_upper:
- 0.9765625
- 0.9765625
- 5.0
start_epoch: 0
stop_on_lowacc: false
validate:
  ckpt_name: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt
  enabled: false
  invert: true
  output_path: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/prediction_validation
  save_mask: false
validate_final_original_res: true
work_dir: exp/kits23-train_val

WrappedModel2D is initialized
WrappedModel2D(
  (net): SegResNetDS(
    (encoder): SegResEncoder(
      (conv_init): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layers): ModuleList(
        (0): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (1): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (2): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (3): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
        (4): ModuleDict(
          (blocks): Sequential(
            (0): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (1): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (2): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (3): SegResBlock(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act1): ReLU(inplace=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (downsample): Identity()
        )
      )
    )
    (up_layers): ModuleList(
      (0): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (1): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Identity()
      )
      (2): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ModuleDict(
        (upsample): UpSample(
          (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)
        )
        (blocks): Sequential(
          (0): SegResBlock(
            (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (head): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Total parameters count: 29065286 distributed: False
Segmenter train called
train_files files 293, validation files 196
Calculating cache required 624GB, available RAM 1665GB given avg image size [623, 623, 707].
Caching full dataset in RAM
Auto setting max_samples_per_class: 6000 cache_class_indices: True
Given num_crops_per_image 1, num_epochs was adjusted 600 => 600
Scheduling validation loops at epochs: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 169, 178, 187, 196, 205, 214, 223, 232, 241, 250, 259, 268, 277, 286, 295, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 399, 406, 413, 420, 427, 434, 441, 448, 455, 461, 467, 473, 479, 485, 491, 497, 502, 507, 512, 517, 522, 527, 532, 537, 541, 545, 549, 553, 557, 561, 564, 567, 570, 573, 576, 579, 582, 584, 586, 588, 590, 592, 594, 595, 596, 597, 598, 599, 600]
Writing Tensorboard logs to /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model
Using num_epochs => 600
 Using start_epoch => 0
 batch_size => 8 
 num_crops_per_image => 1 
 num_steps_per_image => 1 
 num_warmup_epochs => 3 
 
Epoch 0/600 0/37 loss: 1.5579 acc [ 0.183  0.037  0.022]  time 28.58s 
Epoch 0/600 1/37 loss: 1.6013 acc [ 0.254  0.053  0.088]  time 0.06s 
Epoch 0/600 2/37 loss: 1.5964 acc [ 0.270  0.056  0.078]  time 24.47s 
Epoch 0/600 3/37 loss: 1.5805 acc [ 0.266  0.085  0.091]  time 0.05s 
Epoch 0/600 4/37 loss: 1.5806 acc [ 0.276  0.090  0.081]  time 2.07s 
Epoch 0/600 5/37 loss: 1.5744 acc [ 0.277  0.080  0.084]  time 0.05s 
Epoch 0/600 6/37 loss: 1.5647 acc [ 0.276  0.075  0.084]  time 23.97s 
Epoch 0/600 7/37 loss: 1.5602 acc [ 0.280  0.079  0.078]  time 0.06s 
Epoch 0/600 8/37 loss: 1.5678 acc [ 0.301  0.087  0.075]  time 3.02s 
Epoch 0/600 9/37 loss: 1.5612 acc [ 0.299  0.091  0.070]  time 0.05s 
Epoch 0/600 10/37 loss: 1.5616 acc [ 0.313  0.097  0.071]  time 31.06s 
Epoch 0/600 11/37 loss: 1.5681 acc [ 0.317  0.105  0.073]  time 0.05s 
Epoch 0/600 12/37 loss: 1.5611 acc [ 0.317  0.102  0.068]  time 0.05s 
Epoch 0/600 13/37 loss: 1.5557 acc [ 0.319  0.099  0.068]  time 0.05s 
Epoch 0/600 14/37 loss: 1.5601 acc [ 0.328  0.097  0.069]  time 21.98s 
Epoch 0/600 15/37 loss: 1.5563 acc [ 0.330  0.100  0.069]  time 0.05s 
Epoch 0/600 16/37 loss: 1.5511 acc [ 0.333  0.099  0.068]  time 4.19s 
Epoch 0/600 17/37 loss: 1.5666 acc [ 0.346  0.100  0.067]  time 0.06s 
Epoch 0/600 18/37 loss: 1.5616 acc [ 0.352  0.099  0.065]  time 22.49s 
Epoch 0/600 19/37 loss: 1.5565 acc [ 0.349  0.097  0.063]  time 0.06s 
Epoch 0/600 20/37 loss: 1.5530 acc [ 0.349  0.095  0.061]  time 10.94s 
Epoch 0/600 21/37 loss: 1.5483 acc [ 0.349  0.093  0.061]  time 0.06s 
Epoch 0/600 22/37 loss: 1.5516 acc [ 0.355  0.097  0.059]  time 9.62s 
Epoch 0/600 23/37 loss: 1.5483 acc [ 0.359  0.097  0.059]  time 0.06s 
Epoch 0/600 24/37 loss: 1.5442 acc [ 0.356  0.095  0.058]  time 21.05s 
Epoch 0/600 25/37 loss: 1.5393 acc [ 0.353  0.094  0.057]  time 0.06s 
Epoch 0/600 26/37 loss: 1.5412 acc [ 0.360  0.093  0.056]  time 0.88s 
Epoch 0/600 27/37 loss: 1.5359 acc [ 0.359  0.090  0.056]  time 0.06s 
Epoch 0/600 28/37 loss: 1.5316 acc [ 0.357  0.090  0.055]  time 15.34s 
Epoch 0/600 29/37 loss: 1.5274 acc [ 0.355  0.090  0.054]  time 0.05s 
Epoch 0/600 30/37 loss: 1.5235 acc [ 0.357  0.088  0.053]  time 0.05s 
Epoch 0/600 31/37 loss: 1.5194 acc [ 0.353  0.087  0.052]  time 0.05s 
Epoch 0/600 32/37 loss: 1.5168 acc [ 0.349  0.086  0.051]  time 11.04s 
Epoch 0/600 33/37 loss: 1.5143 acc [ 0.349  0.084  0.049]  time 0.06s 
Epoch 0/600 34/37 loss: 1.5113 acc [ 0.353  0.082  0.047]  time 12.76s 
Epoch 0/600 35/37 loss: 1.5067 acc [ 0.354  0.081  0.046]  time 0.06s 
Epoch 0/600 36/37 loss: 1.5046 acc [ 0.355  0.081  0.045]  time 4.25s 
Final training  0/599 loss: 1.5046 acc_avg: 0.1602 acc [ 0.355  0.081  0.045] time 248.81s  lr: 2.0000e-05
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 0, 'best_metric': -1}, save_time 0.14s
Estimated remaining training time for the current model fold 0 is 47.83 hr, running time 0.07 hr, est total time 47.90 hr 

Epoch 1/600 0/37 loss: 1.3606 acc [ 0.365  0.023  0.011]  time 5.92s 
Epoch 1/600 1/37 loss: 1.6922 acc [ 0.449  0.033  0.012]  time 0.69s 
Epoch 1/600 2/37 loss: 1.5895 acc [ 0.432  0.028  0.011]  time 0.06s 
Epoch 1/600 3/37 loss: 1.5354 acc [ 0.410  0.025  0.008]  time 0.06s 
Epoch 1/600 4/37 loss: 1.4969 acc [ 0.424  0.024  0.008]  time 4.84s 
Epoch 1/600 5/37 loss: 1.4878 acc [ 0.444  0.025  0.009]  time 2.94s 
Epoch 1/600 6/37 loss: 1.4791 acc [ 0.446  0.027  0.008]  time 0.06s 
Epoch 1/600 7/37 loss: 1.4864 acc [ 0.451  0.029  0.008]  time 0.06s 
Epoch 1/600 8/37 loss: 1.4672 acc [ 0.449  0.031  0.007]  time 6.14s 
Epoch 1/600 9/37 loss: 1.4499 acc [ 0.440  0.035  0.006]  time 0.06s 
Epoch 1/600 10/37 loss: 1.4367 acc [ 0.447  0.037  0.007]  time 0.06s 
Epoch 1/600 11/37 loss: 1.4488 acc [ 0.456  0.035  0.008]  time 2.01s 
Epoch 1/600 12/37 loss: 1.4428 acc [ 0.467  0.034  0.007]  time 8.33s 
Epoch 1/600 13/37 loss: 1.4319 acc [ 0.470  0.032  0.011]  time 0.06s 
Epoch 1/600 14/37 loss: 1.4306 acc [ 0.483  0.036  0.019]  time 0.07s 
Epoch 1/600 15/37 loss: 1.4191 acc [ 0.483  0.038  0.021]  time 0.09s 
Epoch 1/600 16/37 loss: 1.4381 acc [ 0.489  0.041  0.025]  time 5.73s 
Epoch 1/600 17/37 loss: 1.4309 acc [ 0.489  0.043  0.029]  time 0.09s 
Epoch 1/600 18/37 loss: 1.4263 acc [ 0.489  0.044  0.031]  time 0.07s 
Epoch 1/600 19/37 loss: 1.4220 acc [ 0.501  0.051  0.040]  time 0.06s 
Epoch 1/600 20/37 loss: 1.4088 acc [ 0.503  0.057  0.046]  time 7.06s 
Epoch 1/600 21/37 loss: 1.3956 acc [ 0.501  0.063  0.053]  time 0.18s 
Epoch 1/600 22/37 loss: 1.3916 acc [ 0.506  0.070  0.061]  time 0.10s 
Epoch 1/600 23/37 loss: 1.3894 acc [ 0.508  0.074  0.065]  time 0.14s 
Epoch 1/600 24/37 loss: 1.3826 acc [ 0.510  0.076  0.067]  time 7.49s 
Epoch 1/600 25/37 loss: 1.3765 acc [ 0.512  0.077  0.069]  time 0.06s 
Epoch 1/600 26/37 loss: 1.3733 acc [ 0.514  0.079  0.071]  time 0.06s 
Epoch 1/600 27/37 loss: 1.3664 acc [ 0.514  0.078  0.069]  time 0.06s 
Epoch 1/600 28/37 loss: 1.3608 acc [ 0.518  0.078  0.069]  time 6.87s 
Epoch 1/600 29/37 loss: 1.3599 acc [ 0.517  0.075  0.067]  time 0.05s 
Epoch 1/600 30/37 loss: 1.3516 acc [ 0.518  0.077  0.069]  time 0.06s 
Epoch 1/600 31/37 loss: 1.3448 acc [ 0.519  0.077  0.069]  time 0.05s 
Epoch 1/600 32/37 loss: 1.3446 acc [ 0.514  0.076  0.069]  time 3.15s 
Epoch 1/600 33/37 loss: 1.3392 acc [ 0.516  0.078  0.071]  time 0.10s 
Epoch 1/600 34/37 loss: 1.3340 acc [ 0.520  0.083  0.078]  time 0.09s 
Epoch 1/600 35/37 loss: 1.3325 acc [ 0.520  0.082  0.077]  time 0.21s 
Epoch 1/600 36/37 loss: 1.3302 acc [ 0.522  0.086  0.083]  time 1.10s 
Final training  1/599 loss: 1.3302 acc_avg: 0.2301 acc [ 0.522  0.086  0.083] time 64.21s  lr: 8.0000e-05
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 1, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 17.04 hr, running time 0.09 hr, est total time 17.13 hr 

Epoch 2/600 0/37 loss: 1.1880 acc [ 0.606  0.111  0.086]  time 0.36s 
Epoch 2/600 1/37 loss: 1.2796 acc [ 0.589  0.166  0.155]  time 0.06s 
Epoch 2/600 2/37 loss: 1.2380 acc [ 0.613  0.207  0.216]  time 0.06s 
Epoch 2/600 3/37 loss: 1.1993 acc [ 0.604  0.219  0.225]  time 0.06s 
Epoch 2/600 4/37 loss: 1.2092 acc [ 0.594  0.225  0.226]  time 0.10s 
Epoch 2/600 5/37 loss: 1.2016 acc [ 0.562  0.224  0.224]  time 0.06s 
Epoch 2/600 6/37 loss: 1.1981 acc [ 0.559  0.230  0.231]  time 0.06s 
Epoch 2/600 7/37 loss: 1.2318 acc [ 0.581  0.244  0.245]  time 0.06s 
Epoch 2/600 8/37 loss: 1.2292 acc [ 0.581  0.241  0.239]  time 0.11s 
Epoch 2/600 9/37 loss: 1.2125 acc [ 0.583  0.239  0.242]  time 0.06s 
Epoch 2/600 10/37 loss: 1.2080 acc [ 0.581  0.241  0.240]  time 0.06s 
Epoch 2/600 11/37 loss: 1.2232 acc [ 0.572  0.231  0.230]  time 0.06s 
Epoch 2/600 12/37 loss: 1.2247 acc [ 0.570  0.233  0.232]  time 0.06s 
Epoch 2/600 13/37 loss: 1.2182 acc [ 0.573  0.232  0.231]  time 0.06s 
Epoch 2/600 14/37 loss: 1.2178 acc [ 0.569  0.234  0.233]  time 0.06s 
Epoch 2/600 15/37 loss: 1.2185 acc [ 0.566  0.230  0.228]  time 0.06s 
Epoch 2/600 16/37 loss: 1.2241 acc [ 0.562  0.220  0.221]  time 0.05s 
Epoch 2/600 17/37 loss: 1.2235 acc [ 0.560  0.217  0.218]  time 0.05s 
Epoch 2/600 18/37 loss: 1.2219 acc [ 0.553  0.208  0.211]  time 0.06s 
Epoch 2/600 19/37 loss: 1.2345 acc [ 0.545  0.202  0.204]  time 0.06s 
Epoch 2/600 20/37 loss: 1.2347 acc [ 0.540  0.201  0.203]  time 0.06s 
Epoch 2/600 21/37 loss: 1.2295 acc [ 0.536  0.194  0.196]  time 0.06s 
Epoch 2/600 22/37 loss: 1.2275 acc [ 0.537  0.187  0.190]  time 0.06s 
Epoch 2/600 23/37 loss: 1.2301 acc [ 0.526  0.181  0.185]  time 0.06s 
Epoch 2/600 24/37 loss: 1.2279 acc [ 0.529  0.181  0.183]  time 0.06s 
Epoch 2/600 25/37 loss: 1.2232 acc [ 0.534  0.178  0.180]  time 0.07s 
Epoch 2/600 26/37 loss: 1.2254 acc [ 0.537  0.174  0.175]  time 0.06s 
Epoch 2/600 27/37 loss: 1.2229 acc [ 0.539  0.171  0.169]  time 0.06s 
Epoch 2/600 28/37 loss: 1.2309 acc [ 0.540  0.166  0.164]  time 0.06s 
Epoch 2/600 29/37 loss: 1.2375 acc [ 0.546  0.166  0.164]  time 0.06s 
Epoch 2/600 30/37 loss: 1.2295 acc [ 0.548  0.172  0.167]  time 0.05s 
Epoch 2/600 31/37 loss: 1.2260 acc [ 0.548  0.173  0.168]  time 0.06s 
Epoch 2/600 32/37 loss: 1.2254 acc [ 0.551  0.173  0.168]  time 0.06s 
Epoch 2/600 33/37 loss: 1.2261 acc [ 0.553  0.175  0.172]  time 0.06s 
Epoch 2/600 34/37 loss: 1.2265 acc [ 0.550  0.175  0.173]  time 0.06s 
Epoch 2/600 35/37 loss: 1.2254 acc [ 0.553  0.182  0.182]  time 0.06s 
Epoch 2/600 36/37 loss: 1.2255 acc [ 0.557  0.183  0.183]  time 0.04s 
Final training  2/599 loss: 1.2255 acc_avg: 0.3078 acc [ 0.557  0.183  0.183] time 2.56s  lr: 1.4000e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 2, 'best_metric': -1}, save_time 0.14s
Estimated remaining training time for the current model fold 0 is 6.78 hr, running time 0.09 hr, est total time 6.87 hr 

Epoch 3/600 0/37 loss: 1.1336 acc [ 0.655  0.442  0.438]  time 0.36s 
Epoch 3/600 1/37 loss: 1.1501 acc [ 0.641  0.308  0.301]  time 0.06s 
Epoch 3/600 2/37 loss: 1.1356 acc [ 0.669  0.342  0.338]  time 0.06s 
Epoch 3/600 3/37 loss: 1.1522 acc [ 0.622  0.271  0.316]  time 0.06s 
Epoch 3/600 4/37 loss: 1.1749 acc [ 0.602  0.224  0.274]  time 0.06s 
Epoch 3/600 5/37 loss: 1.2097 acc [ 0.604  0.201  0.238]  time 0.06s 
Epoch 3/600 6/37 loss: 1.1701 acc [ 0.617  0.224  0.266]  time 0.06s 
Epoch 3/600 7/37 loss: 1.1645 acc [ 0.634  0.224  0.256]  time 0.07s 
Epoch 3/600 8/37 loss: 1.1634 acc [ 0.629  0.215  0.245]  time 0.06s 
Epoch 3/600 9/37 loss: 1.2033 acc [ 0.610  0.187  0.207]  time 0.06s 
Epoch 3/600 10/37 loss: 1.2099 acc [ 0.580  0.173  0.186]  time 0.06s 
Epoch 3/600 11/37 loss: 1.1948 acc [ 0.583  0.171  0.177]  time 0.06s 
Epoch 3/600 12/37 loss: 1.1862 acc [ 0.588  0.167  0.171]  time 0.06s 
Epoch 3/600 13/37 loss: 1.2075 acc [ 0.593  0.172  0.173]  time 0.06s 
Epoch 3/600 14/37 loss: 1.2150 acc [ 0.598  0.168  0.167]  time 0.06s 
Epoch 3/600 15/37 loss: 1.2079 acc [ 0.600  0.170  0.173]  time 0.06s 
Epoch 3/600 16/37 loss: 1.2157 acc [ 0.599  0.169  0.171]  time 0.05s 
Epoch 3/600 17/37 loss: 1.2126 acc [ 0.604  0.180  0.179]  time 0.06s 
Epoch 3/600 18/37 loss: 1.2084 acc [ 0.598  0.181  0.181]  time 0.06s 
Epoch 3/600 19/37 loss: 1.2122 acc [ 0.594  0.178  0.180]  time 0.06s 
Epoch 3/600 20/37 loss: 1.2123 acc [ 0.590  0.184  0.185]  time 0.06s 
Epoch 3/600 21/37 loss: 1.2276 acc [ 0.586  0.183  0.183]  time 0.06s 
Epoch 3/600 22/37 loss: 1.2300 acc [ 0.580  0.179  0.177]  time 0.06s 
Epoch 3/600 23/37 loss: 1.2338 acc [ 0.578  0.179  0.179]  time 0.06s 
Epoch 3/600 24/37 loss: 1.2312 acc [ 0.581  0.182  0.182]  time 0.06s 
Epoch 3/600 25/37 loss: 1.2297 acc [ 0.582  0.182  0.182]  time 0.06s 
Epoch 3/600 26/37 loss: 1.2279 acc [ 0.581  0.190  0.192]  time 0.06s 
Epoch 3/600 27/37 loss: 1.2358 acc [ 0.579  0.189  0.191]  time 0.06s 
Epoch 3/600 28/37 loss: 1.2318 acc [ 0.578  0.189  0.191]  time 0.06s 
Epoch 3/600 29/37 loss: 1.2472 acc [ 0.575  0.189  0.191]  time 0.06s 
Epoch 3/600 30/37 loss: 1.2446 acc [ 0.581  0.190  0.191]  time 0.07s 
Epoch 3/600 31/37 loss: 1.2458 acc [ 0.581  0.188  0.189]  time 0.06s 
Epoch 3/600 32/37 loss: 1.2485 acc [ 0.577  0.185  0.185]  time 0.05s 
Epoch 3/600 33/37 loss: 1.2437 acc [ 0.580  0.186  0.187]  time 0.06s 
Epoch 3/600 34/37 loss: 1.2449 acc [ 0.580  0.183  0.183]  time 0.06s 
Epoch 3/600 35/37 loss: 1.2474 acc [ 0.578  0.182  0.182]  time 0.06s 
Epoch 3/600 36/37 loss: 1.2452 acc [ 0.580  0.186  0.185]  time 0.04s 
Final training  3/599 loss: 1.2452 acc_avg: 0.3170 acc [ 0.580  0.186  0.185] time 2.47s  lr: 2.0000e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 3, 'best_metric': -1}, save_time 0.12s
Estimated remaining training time for the current model fold 0 is 6.77 hr, running time 0.09 hr, est total time 6.86 hr 

Epoch 4/600 0/37 loss: 1.1151 acc [ 0.678  0.117  0.114]  time 0.32s 
Epoch 4/600 1/37 loss: 1.1203 acc [ 0.726  0.220  0.245]  time 0.06s 
Epoch 4/600 2/37 loss: 1.1056 acc [ 0.689  0.203  0.218]  time 0.06s 
Epoch 4/600 3/37 loss: 1.1392 acc [ 0.688  0.175  0.201]  time 0.06s 
Epoch 4/600 4/37 loss: 1.1936 acc [ 0.668  0.175  0.185]  time 0.06s 
Epoch 4/600 5/37 loss: 1.2481 acc [ 0.610  0.161  0.172]  time 0.06s 
Epoch 4/600 6/37 loss: 1.2287 acc [ 0.601  0.172  0.183]  time 0.06s 
Epoch 4/600 7/37 loss: 1.2312 acc [ 0.591  0.161  0.171]  time 0.06s 
Epoch 4/600 8/37 loss: 1.2462 acc [ 0.590  0.167  0.177]  time 0.06s 
Epoch 4/600 9/37 loss: 1.2405 acc [ 0.596  0.176  0.190]  time 0.06s 
Epoch 4/600 10/37 loss: 1.2243 acc [ 0.603  0.183  0.196]  time 0.06s 
Epoch 4/600 11/37 loss: 1.2131 acc [ 0.595  0.190  0.203]  time 0.06s 
Epoch 4/600 12/37 loss: 1.2190 acc [ 0.595  0.183  0.195]  time 0.06s 
Epoch 4/600 13/37 loss: 1.2155 acc [ 0.604  0.196  0.209]  time 0.06s 
Epoch 4/600 14/37 loss: 1.2161 acc [ 0.596  0.192  0.206]  time 0.05s 
Epoch 4/600 15/37 loss: 1.1961 acc [ 0.598  0.196  0.212]  time 0.05s 
Epoch 4/600 16/37 loss: 1.1926 acc [ 0.601  0.196  0.208]  time 0.06s 
Epoch 4/600 17/37 loss: 1.1849 acc [ 0.595  0.205  0.216]  time 0.06s 
Epoch 4/600 18/37 loss: 1.1872 acc [ 0.595  0.205  0.218]  time 0.06s 
Epoch 4/600 19/37 loss: 1.1772 acc [ 0.598  0.207  0.219]  time 0.05s 
Epoch 4/600 20/37 loss: 1.1838 acc [ 0.602  0.207  0.219]  time 0.06s 
Epoch 4/600 21/37 loss: 1.1823 acc [ 0.602  0.212  0.222]  time 0.06s 
Epoch 4/600 22/37 loss: 1.1857 acc [ 0.597  0.210  0.222]  time 0.06s 
Epoch 4/600 23/37 loss: 1.1871 acc [ 0.599  0.210  0.222]  time 0.05s 
Epoch 4/600 24/37 loss: 1.1799 acc [ 0.604  0.211  0.225]  time 0.05s 
Epoch 4/600 25/37 loss: 1.1786 acc [ 0.598  0.212  0.226]  time 0.06s 
Epoch 4/600 26/37 loss: 1.1832 acc [ 0.595  0.208  0.223]  time 0.06s 
Epoch 4/600 27/37 loss: 1.1748 acc [ 0.589  0.211  0.225]  time 0.06s 
Epoch 4/600 28/37 loss: 1.1742 acc [ 0.578  0.206  0.222]  time 0.06s 
Epoch 4/600 29/37 loss: 1.1791 acc [ 0.574  0.202  0.217]  time 0.08s 
Epoch 4/600 30/37 loss: 1.1833 acc [ 0.569  0.199  0.213]  time 0.06s 
Epoch 4/600 31/37 loss: 1.1827 acc [ 0.572  0.199  0.212]  time 0.05s 
Epoch 4/600 32/37 loss: 1.1823 acc [ 0.573  0.197  0.210]  time 0.05s 
Epoch 4/600 33/37 loss: 1.1872 acc [ 0.572  0.195  0.208]  time 0.06s 
Epoch 4/600 34/37 loss: 1.1841 acc [ 0.571  0.197  0.211]  time 0.05s 
Epoch 4/600 35/37 loss: 1.1879 acc [ 0.567  0.195  0.209]  time 0.05s 
Epoch 4/600 36/37 loss: 1.1876 acc [ 0.566  0.195  0.208]  time 0.04s 
Final training  4/599 loss: 1.1876 acc_avg: 0.3229 acc [ 0.566  0.195  0.208] time 2.41s  lr: 2.0000e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 4, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 6.76 hr, running time 0.09 hr, est total time 6.85 hr 

Epoch 5/600 0/37 loss: 1.1611 acc [ 0.597  0.052  0.073]  time 0.32s 
Epoch 5/600 1/37 loss: 1.1235 acc [ 0.606  0.125  0.165]  time 0.05s 
Epoch 5/600 2/37 loss: 1.0998 acc [ 0.647  0.155  0.156]  time 0.06s 
Epoch 5/600 3/37 loss: 1.2121 acc [ 0.640  0.160  0.164]  time 0.06s 
Epoch 5/600 4/37 loss: 1.2144 acc [ 0.603  0.149  0.160]  time 0.06s 
Epoch 5/600 5/37 loss: 1.2005 acc [ 0.617  0.147  0.174]  time 0.05s 
Epoch 5/600 6/37 loss: 1.2263 acc [ 0.575  0.136  0.161]  time 0.06s 
Epoch 5/600 7/37 loss: 1.2235 acc [ 0.554  0.137  0.157]  time 0.05s 
Epoch 5/600 8/37 loss: 1.2112 acc [ 0.548  0.134  0.152]  time 0.06s 
Epoch 5/600 9/37 loss: 1.2096 acc [ 0.548  0.137  0.154]  time 0.06s 
Epoch 5/600 10/37 loss: 1.2061 acc [ 0.555  0.135  0.150]  time 0.07s 
Epoch 5/600 11/37 loss: 1.2064 acc [ 0.560  0.138  0.154]  time 0.06s 
Epoch 5/600 12/37 loss: 1.2430 acc [ 0.560  0.133  0.149]  time 0.06s 
Epoch 5/600 13/37 loss: 1.2346 acc [ 0.555  0.138  0.157]  time 0.06s 
Epoch 5/600 14/37 loss: 1.2404 acc [ 0.543  0.128  0.146]  time 0.06s 
Epoch 5/600 15/37 loss: 1.2344 acc [ 0.540  0.136  0.154]  time 0.06s 
Epoch 5/600 16/37 loss: 1.2236 acc [ 0.540  0.139  0.155]  time 0.06s 
Epoch 5/600 17/37 loss: 1.2259 acc [ 0.541  0.134  0.148]  time 0.06s 
Epoch 5/600 18/37 loss: 1.2159 acc [ 0.548  0.142  0.155]  time 0.06s 
Epoch 5/600 19/37 loss: 1.2045 acc [ 0.557  0.144  0.156]  time 0.06s 
Epoch 5/600 20/37 loss: 1.1997 acc [ 0.561  0.149  0.161]  time 0.06s 
Epoch 5/600 21/37 loss: 1.2010 acc [ 0.564  0.145  0.161]  time 0.06s 
Epoch 5/600 22/37 loss: 1.2057 acc [ 0.562  0.144  0.157]  time 0.06s 
Epoch 5/600 23/37 loss: 1.2027 acc [ 0.561  0.141  0.152]  time 0.06s 
Epoch 5/600 24/37 loss: 1.2054 acc [ 0.559  0.140  0.150]  time 0.05s 
Epoch 5/600 25/37 loss: 1.1982 acc [ 0.564  0.146  0.155]  time 0.06s 
Epoch 5/600 26/37 loss: 1.1975 acc [ 0.562  0.145  0.152]  time 0.06s 
Epoch 5/600 27/37 loss: 1.1921 acc [ 0.567  0.151  0.158]  time 0.05s 
Epoch 5/600 28/37 loss: 1.1862 acc [ 0.572  0.162  0.164]  time 0.06s 
Epoch 5/600 29/37 loss: 1.1916 acc [ 0.571  0.158  0.160]  time 0.05s 
Epoch 5/600 30/37 loss: 1.1906 acc [ 0.571  0.156  0.159]  time 0.06s 
Epoch 5/600 31/37 loss: 1.1898 acc [ 0.572  0.159  0.163]  time 0.06s 
Epoch 5/600 32/37 loss: 1.1873 acc [ 0.579  0.156  0.161]  time 0.05s 
Epoch 5/600 33/37 loss: 1.1815 acc [ 0.585  0.162  0.168]  time 0.05s 
Epoch 5/600 34/37 loss: 1.1783 acc [ 0.585  0.164  0.170]  time 0.05s 
Epoch 5/600 35/37 loss: 1.1746 acc [ 0.585  0.167  0.172]  time 0.06s 
Epoch 5/600 36/37 loss: 1.1744 acc [ 0.585  0.165  0.173]  time 0.04s 
Final training  5/599 loss: 1.1744 acc_avg: 0.3079 acc [ 0.585  0.165  0.173] time 2.36s  lr: 1.9999e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 5, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 6.75 hr, running time 0.09 hr, est total time 6.84 hr 

Epoch 6/600 0/37 loss: 1.1082 acc [ 0.612  0.337  0.310]  time 0.28s 
Epoch 6/600 1/37 loss: 1.3898 acc [ 0.584  0.296  0.318]  time 0.06s 
Epoch 6/600 2/37 loss: 1.3050 acc [ 0.570  0.224  0.240]  time 0.06s 
Epoch 6/600 3/37 loss: 1.2656 acc [ 0.545  0.236  0.246]  time 0.06s 
Epoch 6/600 4/37 loss: 1.2017 acc [ 0.564  0.260  0.268]  time 0.05s 
Epoch 6/600 5/37 loss: 1.2102 acc [ 0.579  0.272  0.290]  time 0.05s 
Epoch 6/600 6/37 loss: 1.1927 acc [ 0.583  0.262  0.293]  time 0.06s 
Epoch 6/600 7/37 loss: 1.1986 acc [ 0.596  0.265  0.293]  time 0.06s 
Epoch 6/600 8/37 loss: 1.1890 acc [ 0.607  0.258  0.268]  time 0.06s 
Epoch 6/600 9/37 loss: 1.1811 acc [ 0.593  0.253  0.262]  time 0.05s 
Epoch 6/600 10/37 loss: 1.1675 acc [ 0.596  0.239  0.249]  time 0.06s 
Epoch 6/600 11/37 loss: 1.1496 acc [ 0.599  0.241  0.251]  time 0.05s 
Epoch 6/600 12/37 loss: 1.1553 acc [ 0.609  0.250  0.259]  time 0.05s 
Epoch 6/600 13/37 loss: 1.1542 acc [ 0.612  0.247  0.258]  time 0.06s 
Epoch 6/600 14/37 loss: 1.1831 acc [ 0.611  0.248  0.259]  time 0.06s 
Epoch 6/600 15/37 loss: 1.1873 acc [ 0.612  0.247  0.257]  time 0.06s 
Epoch 6/600 16/37 loss: 1.1757 acc [ 0.611  0.254  0.264]  time 0.05s 
Epoch 6/600 17/37 loss: 1.1708 acc [ 0.609  0.248  0.257]  time 0.06s 
Epoch 6/600 18/37 loss: 1.1682 acc [ 0.607  0.244  0.251]  time 0.06s 
Epoch 6/600 19/37 loss: 1.1673 acc [ 0.610  0.251  0.258]  time 0.06s 
Epoch 6/600 20/37 loss: 1.1821 acc [ 0.612  0.248  0.254]  time 0.06s 
Epoch 6/600 21/37 loss: 1.1782 acc [ 0.616  0.249  0.255]  time 0.06s 
Epoch 6/600 22/37 loss: 1.1760 acc [ 0.612  0.249  0.259]  time 0.06s 
Epoch 6/600 23/37 loss: 1.1776 acc [ 0.611  0.244  0.257]  time 0.06s 
Epoch 6/600 24/37 loss: 1.1749 acc [ 0.615  0.242  0.254]  time 0.06s 
Epoch 6/600 25/37 loss: 1.1739 acc [ 0.613  0.246  0.258]  time 0.06s 
Epoch 6/600 26/37 loss: 1.1832 acc [ 0.606  0.238  0.248]  time 0.06s 
Epoch 6/600 27/37 loss: 1.1778 acc [ 0.608  0.238  0.248]  time 0.06s 
Epoch 6/600 28/37 loss: 1.1789 acc [ 0.607  0.236  0.246]  time 0.06s 
Epoch 6/600 29/37 loss: 1.1717 acc [ 0.612  0.238  0.247]  time 0.06s 
Epoch 6/600 30/37 loss: 1.1649 acc [ 0.612  0.240  0.250]  time 0.06s 
Epoch 6/600 31/37 loss: 1.1611 acc [ 0.613  0.243  0.253]  time 0.05s 
Epoch 6/600 32/37 loss: 1.1793 acc [ 0.607  0.242  0.252]  time 0.05s 
Epoch 6/600 33/37 loss: 1.1726 acc [ 0.609  0.244  0.254]  time 0.05s 
Epoch 6/600 34/37 loss: 1.1702 acc [ 0.610  0.246  0.256]  time 0.05s 
Epoch 6/600 35/37 loss: 1.1679 acc [ 0.613  0.245  0.255]  time 0.05s 
Epoch 6/600 36/37 loss: 1.1667 acc [ 0.616  0.245  0.257]  time 0.04s 
Final training  6/599 loss: 1.1667 acc_avg: 0.3726 acc [ 0.616  0.245  0.257] time 2.34s  lr: 1.9999e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 6, 'best_metric': -1}, save_time 0.12s
Estimated remaining training time for the current model fold 0 is 6.74 hr, running time 0.09 hr, est total time 6.83 hr 

Epoch 7/600 0/37 loss: 1.0247 acc [ 0.504  0.182  0.224]  time 0.29s 
Epoch 7/600 1/37 loss: 1.0290 acc [ 0.576  0.228  0.249]  time 0.06s 
Epoch 7/600 2/37 loss: 1.1024 acc [ 0.540  0.202  0.216]  time 0.05s 
Epoch 7/600 3/37 loss: 1.0931 acc [ 0.570  0.243  0.269]  time 0.05s 
Epoch 7/600 4/37 loss: 1.1276 acc [ 0.560  0.214  0.231]  time 0.05s 
Epoch 7/600 5/37 loss: 1.1174 acc [ 0.549  0.232  0.248]  time 0.05s 
Epoch 7/600 6/37 loss: 1.1214 acc [ 0.540  0.213  0.225]  time 0.06s 
Epoch 7/600 7/37 loss: 1.1590 acc [ 0.550  0.212  0.221]  time 0.06s 
Epoch 7/600 8/37 loss: 1.1533 acc [ 0.539  0.213  0.218]  time 0.06s 
Epoch 7/600 9/37 loss: 1.1524 acc [ 0.542  0.221  0.226]  time 0.06s 
Epoch 7/600 10/37 loss: 1.1433 acc [ 0.540  0.220  0.225]  time 0.06s 
Epoch 7/600 11/37 loss: 1.1809 acc [ 0.531  0.228  0.237]  time 0.05s 
Epoch 7/600 12/37 loss: 1.1760 acc [ 0.550  0.224  0.232]  time 0.05s 
Epoch 7/600 13/37 loss: 1.1785 acc [ 0.551  0.221  0.226]  time 0.06s 
Epoch 7/600 14/37 loss: 1.1816 acc [ 0.552  0.221  0.229]  time 0.05s 
Epoch 7/600 15/37 loss: 1.1765 acc [ 0.558  0.216  0.222]  time 0.05s 
Epoch 7/600 16/37 loss: 1.1759 acc [ 0.562  0.225  0.231]  time 0.05s 
Epoch 7/600 17/37 loss: 1.1703 acc [ 0.577  0.234  0.239]  time 0.06s 
Epoch 7/600 18/37 loss: 1.1740 acc [ 0.573  0.229  0.234]  time 0.05s 
Epoch 7/600 19/37 loss: 1.1865 acc [ 0.580  0.226  0.233]  time 0.06s 
Epoch 7/600 20/37 loss: 1.1836 acc [ 0.579  0.230  0.237]  time 0.06s 
Epoch 7/600 21/37 loss: 1.1834 acc [ 0.584  0.234  0.241]  time 0.06s 
Epoch 7/600 22/37 loss: 1.1761 acc [ 0.588  0.237  0.244]  time 0.06s 
Epoch 7/600 23/37 loss: 1.1712 acc [ 0.588  0.237  0.243]  time 0.06s 
Epoch 7/600 24/37 loss: 1.1745 acc [ 0.591  0.233  0.241]  time 0.05s 
Epoch 7/600 25/37 loss: 1.1717 acc [ 0.593  0.234  0.242]  time 0.05s 
Epoch 7/600 26/37 loss: 1.1661 acc [ 0.589  0.232  0.241]  time 0.06s 
Epoch 7/600 27/37 loss: 1.1731 acc [ 0.589  0.234  0.242]  time 0.06s 
Epoch 7/600 28/37 loss: 1.1624 acc [ 0.592  0.239  0.248]  time 0.06s 
Epoch 7/600 29/37 loss: 1.1609 acc [ 0.588  0.236  0.245]  time 0.06s 
Epoch 7/600 30/37 loss: 1.1646 acc [ 0.588  0.234  0.244]  time 0.05s 
Epoch 7/600 31/37 loss: 1.1661 acc [ 0.583  0.230  0.242]  time 0.05s 
Epoch 7/600 32/37 loss: 1.1573 acc [ 0.582  0.228  0.244]  time 0.05s 
Epoch 7/600 33/37 loss: 1.1581 acc [ 0.586  0.228  0.243]  time 0.05s 
Epoch 7/600 34/37 loss: 1.1642 acc [ 0.586  0.223  0.239]  time 0.05s 
Epoch 7/600 35/37 loss: 1.1635 acc [ 0.588  0.225  0.241]  time 0.05s 
Epoch 7/600 36/37 loss: 1.1584 acc [ 0.586  0.230  0.246]  time 0.04s 
Final training  7/599 loss: 1.1584 acc_avg: 0.3540 acc [ 0.586  0.230  0.246] time 2.29s  lr: 1.9998e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 7, 'best_metric': -1}, save_time 0.12s
Estimated remaining training time for the current model fold 0 is 6.74 hr, running time 0.09 hr, est total time 6.83 hr 

Epoch 8/600 0/37 loss: 1.1502 acc [ 0.643  0.074  0.133]  time 0.28s 
Epoch 8/600 1/37 loss: 1.3020 acc [ 0.566  0.160  0.181]  time 0.06s 
Epoch 8/600 2/37 loss: 1.1960 acc [ 0.596  0.232  0.247]  time 0.06s 
Epoch 8/600 3/37 loss: 1.1769 acc [ 0.593  0.209  0.251]  time 0.06s 
Epoch 8/600 4/37 loss: 1.1628 acc [ 0.577  0.208  0.266]  time 0.06s 
Epoch 8/600 5/37 loss: 1.1394 acc [ 0.544  0.221  0.276]  time 0.06s 
Epoch 8/600 6/37 loss: 1.1510 acc [ 0.547  0.203  0.254]  time 0.07s 
Epoch 8/600 7/37 loss: 1.1723 acc [ 0.551  0.188  0.222]  time 0.06s 
Epoch 8/600 8/37 loss: 1.1738 acc [ 0.573  0.174  0.194]  time 0.06s 
Epoch 8/600 9/37 loss: 1.1800 acc [ 0.563  0.183  0.204]  time 0.06s 
Epoch 8/600 10/37 loss: 1.1797 acc [ 0.561  0.181  0.191]  time 0.06s 
Epoch 8/600 11/37 loss: 1.1961 acc [ 0.567  0.172  0.181]  time 0.06s 
Epoch 8/600 12/37 loss: 1.2215 acc [ 0.568  0.164  0.170]  time 0.06s 
Epoch 8/600 13/37 loss: 1.2179 acc [ 0.569  0.168  0.172]  time 0.06s 
Epoch 8/600 14/37 loss: 1.2072 acc [ 0.582  0.168  0.173]  time 0.06s 
Epoch 8/600 15/37 loss: 1.2009 acc [ 0.584  0.165  0.169]  time 0.06s 
Epoch 8/600 16/37 loss: 1.1964 acc [ 0.590  0.163  0.166]  time 0.06s 
Epoch 8/600 17/37 loss: 1.1905 acc [ 0.594  0.173  0.178]  time 0.06s 
Epoch 8/600 18/37 loss: 1.1905 acc [ 0.594  0.185  0.190]  time 0.06s 
Epoch 8/600 19/37 loss: 1.1894 acc [ 0.599  0.180  0.191]  time 0.06s 
Epoch 8/600 20/37 loss: 1.1920 acc [ 0.603  0.174  0.184]  time 0.06s 
Epoch 8/600 21/37 loss: 1.1892 acc [ 0.606  0.174  0.183]  time 0.07s 
Epoch 8/600 22/37 loss: 1.1909 acc [ 0.595  0.175  0.182]  time 0.06s 
Epoch 8/600 23/37 loss: 1.1878 acc [ 0.593  0.170  0.178]  time 0.06s 
Epoch 8/600 24/37 loss: 1.1863 acc [ 0.596  0.174  0.182]  time 0.07s 
Epoch 8/600 25/37 loss: 1.1904 acc [ 0.595  0.169  0.178]  time 0.07s 
Epoch 8/600 26/37 loss: 1.1941 acc [ 0.593  0.165  0.173]  time 0.07s 
Epoch 8/600 27/37 loss: 1.1901 acc [ 0.591  0.163  0.170]  time 0.06s 
Epoch 8/600 28/37 loss: 1.1885 acc [ 0.595  0.159  0.168]  time 0.06s 
Epoch 8/600 29/37 loss: 1.1915 acc [ 0.592  0.154  0.164]  time 0.07s 
Epoch 8/600 30/37 loss: 1.1887 acc [ 0.589  0.151  0.160]  time 0.06s 
Epoch 8/600 31/37 loss: 1.1887 acc [ 0.589  0.152  0.161]  time 0.06s 
Epoch 8/600 32/37 loss: 1.1897 acc [ 0.591  0.146  0.154]  time 0.06s 
Epoch 8/600 33/37 loss: 1.1896 acc [ 0.590  0.142  0.150]  time 0.06s 
Epoch 8/600 34/37 loss: 1.1869 acc [ 0.591  0.145  0.154]  time 0.06s 
Epoch 8/600 35/37 loss: 1.1848 acc [ 0.593  0.148  0.158]  time 0.06s 
Epoch 8/600 36/37 loss: 1.1822 acc [ 0.594  0.152  0.164]  time 0.04s 
Final training  8/599 loss: 1.1822 acc_avg: 0.3029 acc [ 0.594  0.152  0.164] time 2.46s  lr: 1.9997e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 8, 'best_metric': -1}, save_time 0.14s
Estimated remaining training time for the current model fold 0 is 6.76 hr, running time 0.09 hr, est total time 6.86 hr 

Epoch 9/600 0/37 loss: 1.0944 acc [ 0.539  0.188  0.227]  time 0.30s 
Epoch 9/600 1/37 loss: 1.1701 acc [ 0.459  0.153  0.172]  time 0.06s 
Epoch 9/600 2/37 loss: 1.1278 acc [ 0.506  0.263  0.284]  time 0.06s 
Epoch 9/600 3/37 loss: 1.1332 acc [ 0.556  0.268  0.281]  time 0.06s 
Epoch 9/600 4/37 loss: 1.1431 acc [ 0.559  0.253  0.262]  time 0.07s 
Epoch 9/600 5/37 loss: 1.1414 acc [ 0.578  0.272  0.281]  time 0.06s 
Epoch 9/600 6/37 loss: 1.1615 acc [ 0.570  0.264  0.271]  time 0.06s 
Epoch 9/600 7/37 loss: 1.1546 acc [ 0.593  0.265  0.261]  time 0.06s 
Epoch 9/600 8/37 loss: 1.1596 acc [ 0.596  0.252  0.248]  time 0.06s 
Epoch 9/600 9/37 loss: 1.1403 acc [ 0.605  0.268  0.263]  time 0.06s 
Epoch 9/600 10/37 loss: 1.1398 acc [ 0.608  0.269  0.264]  time 0.06s 
Epoch 9/600 11/37 loss: 1.1255 acc [ 0.606  0.258  0.259]  time 0.07s 
Epoch 9/600 12/37 loss: 1.1224 acc [ 0.609  0.266  0.264]  time 0.07s 
Epoch 9/600 13/37 loss: 1.1179 acc [ 0.609  0.267  0.265]  time 0.06s 
Epoch 9/600 14/37 loss: 1.1095 acc [ 0.601  0.270  0.273]  time 0.06s 
Epoch 9/600 15/37 loss: 1.1160 acc [ 0.598  0.279  0.282]  time 0.06s 
Epoch 9/600 16/37 loss: 1.1038 acc [ 0.606  0.284  0.290]  time 0.06s 
Epoch 9/600 17/37 loss: 1.1011 acc [ 0.614  0.290  0.294]  time 0.06s 
Epoch 9/600 18/37 loss: 1.1017 acc [ 0.612  0.283  0.287]  time 0.06s 
Epoch 9/600 19/37 loss: 1.1016 acc [ 0.612  0.274  0.277]  time 0.07s 
Epoch 9/600 20/37 loss: 1.0976 acc [ 0.603  0.272  0.275]  time 0.06s 
Epoch 9/600 21/37 loss: 1.1159 acc [ 0.596  0.269  0.271]  time 0.06s 
Epoch 9/600 22/37 loss: 1.1147 acc [ 0.593  0.265  0.267]  time 0.06s 
Epoch 9/600 23/37 loss: 1.1138 acc [ 0.592  0.266  0.269]  time 0.06s 
Epoch 9/600 24/37 loss: 1.1225 acc [ 0.595  0.268  0.270]  time 0.06s 
Epoch 9/600 25/37 loss: 1.1219 acc [ 0.591  0.265  0.267]  time 0.06s 
Epoch 9/600 26/37 loss: 1.1260 acc [ 0.586  0.266  0.269]  time 0.06s 
Epoch 9/600 27/37 loss: 1.1190 acc [ 0.595  0.268  0.272]  time 0.06s 
Epoch 9/600 28/37 loss: 1.1202 acc [ 0.599  0.268  0.274]  time 0.06s 
Epoch 9/600 29/37 loss: 1.1199 acc [ 0.603  0.274  0.280]  time 0.07s 
Epoch 9/600 30/37 loss: 1.1265 acc [ 0.601  0.273  0.279]  time 0.06s 
Epoch 9/600 31/37 loss: 1.1302 acc [ 0.601  0.272  0.277]  time 0.06s 
Epoch 9/600 32/37 loss: 1.1319 acc [ 0.599  0.265  0.273]  time 0.06s 
Epoch 9/600 33/37 loss: 1.1322 acc [ 0.599  0.266  0.273]  time 0.06s 
Epoch 9/600 34/37 loss: 1.1320 acc [ 0.597  0.268  0.274]  time 0.06s 
Epoch 9/600 35/37 loss: 1.1276 acc [ 0.596  0.268  0.277]  time 0.06s 
Epoch 9/600 36/37 loss: 1.1384 acc [ 0.593  0.265  0.276]  time 0.04s 
Final training  9/599 loss: 1.1384 acc_avg: 0.3779 acc [ 0.593  0.265  0.276] time 2.48s  lr: 1.9995e-04
Val 9/600 0/196  loss: 0.9223 acc [ 0.186  0.010  0.010]  time 3.10s ['/workspace/workspace-kits23/kits23/dataset/case_00004/imaging.nii.gz']
Val 9/600 1/196  loss: 0.9396 acc [ 0.102  0.026  0.026]  time 36.16s ['/workspace/workspace-kits23/kits23/dataset/case_00005/imaging.nii.gz']
Val 9/600 2/196  loss: 0.9614 acc [ 0.095  0.000  0.000]  time 2.85s ['/workspace/workspace-kits23/kits23/dataset/case_00006/imaging.nii.gz']
Val 9/600 3/196  loss: 0.9537 acc [ 0.101  0.000  0.000]  time 1.25s ['/workspace/workspace-kits23/kits23/dataset/case_00011/imaging.nii.gz']
Val 9/600 4/196  loss: 0.9454 acc [ 0.129  0.000  0.000]  time 1.81s ['/workspace/workspace-kits23/kits23/dataset/case_00017/imaging.nii.gz']
Val 9/600 5/196  loss: 0.8883 acc [ 0.134  0.061  0.062]  time 2.16s ['/workspace/workspace-kits23/kits23/dataset/case_00029/imaging.nii.gz']
Val 9/600 6/196  loss: 0.9512 acc [ 0.116  0.003  0.003]  time 2.51s ['/workspace/workspace-kits23/kits23/dataset/case_00031/imaging.nii.gz']
Val 9/600 7/196  loss: 0.9551 acc [ 0.108  0.000  0.000]  time 1.80s ['/workspace/workspace-kits23/kits23/dataset/case_00034/imaging.nii.gz']
Val 9/600 8/196  loss: 0.9156 acc [ 0.120  0.041  0.041]  time 2.72s ['/workspace/workspace-kits23/kits23/dataset/case_00047/imaging.nii.gz']
Val 9/600 9/196  loss: 0.9469 acc [ 0.133  0.001  0.000]  time 1.39s ['/workspace/workspace-kits23/kits23/dataset/case_00062/imaging.nii.gz']
Val 9/600 10/196  loss: 0.9291 acc [ 0.177  0.009  0.009]  time 1.69s ['/workspace/workspace-kits23/kits23/dataset/case_00065/imaging.nii.gz']
Val 9/600 11/196  loss: 0.9694 acc [ 0.054  0.014  0.014]  time 9.22s ['/workspace/workspace-kits23/kits23/dataset/case_00066/imaging.nii.gz']
Val 9/600 12/196  loss: 0.7298 acc [ 0.302  0.245  0.246]  time 5.82s ['/workspace/workspace-kits23/kits23/dataset/case_00067/imaging.nii.gz']
Val 9/600 13/196  loss: 0.9424 acc [ 0.136  0.005  0.004]  time 1.11s ['/workspace/workspace-kits23/kits23/dataset/case_00085/imaging.nii.gz']
Val 9/600 14/196  loss: 0.8409 acc [ 0.228  0.107  0.106]  time 0.95s ['/workspace/workspace-kits23/kits23/dataset/case_00090/imaging.nii.gz']
Val 9/600 15/196  loss: 0.7596 acc [ 0.225  0.205  0.204]  time 1.87s ['/workspace/workspace-kits23/kits23/dataset/case_00092/imaging.nii.gz']
Val 9/600 16/196  loss: 0.8046 acc [ 0.197  0.145  0.142]  time 6.09s ['/workspace/workspace-kits23/kits23/dataset/case_00102/imaging.nii.gz']
Val 9/600 17/196  loss: 0.7283 acc [ 0.259  0.260  0.259]  time 1.08s ['/workspace/workspace-kits23/kits23/dataset/case_00107/imaging.nii.gz']
Val 9/600 18/196  loss: 0.8957 acc [ 0.232  0.023  0.019]  time 0.57s ['/workspace/workspace-kits23/kits23/dataset/case_00110/imaging.nii.gz']
Val 9/600 19/196  loss: 0.7991 acc [ 0.441  0.066  0.037]  time 1.04s ['/workspace/workspace-kits23/kits23/dataset/case_00117/imaging.nii.gz']
Val 9/600 20/196  loss: 0.8836 acc [ 0.180  0.061  0.059]  time 1.61s ['/workspace/workspace-kits23/kits23/dataset/case_00119/imaging.nii.gz']
Val 9/600 21/196  loss: 0.9708 acc [ 0.071  0.000  0.000]  time 4.18s ['/workspace/workspace-kits23/kits23/dataset/case_00128/imaging.nii.gz']
Val 9/600 22/196  loss: 0.9457 acc [ 0.132  0.005  0.005]  time 11.23s ['/workspace/workspace-kits23/kits23/dataset/case_00132/imaging.nii.gz']
Val 9/600 23/196  loss: 0.9319 acc [ 0.144  0.011  0.011]  time 5.46s ['/workspace/workspace-kits23/kits23/dataset/case_00142/imaging.nii.gz']
Val 9/600 24/196  loss: 0.9107 acc [ 0.217  0.007  0.005]  time 1.05s ['/workspace/workspace-kits23/kits23/dataset/case_00147/imaging.nii.gz']
Val 9/600 25/196  loss: 0.9307 acc [ 0.135  0.015  0.015]  time 17.12s ['/workspace/workspace-kits23/kits23/dataset/case_00159/imaging.nii.gz']
Val 9/600 26/196  loss: 0.9344 acc [ 0.158  0.003  0.003]  time 1.12s ['/workspace/workspace-kits23/kits23/dataset/case_00164/imaging.nii.gz']
Val 9/600 27/196  loss: 0.9814 acc [ 0.046  0.000  0.000]  time 15.34s ['/workspace/workspace-kits23/kits23/dataset/case_00165/imaging.nii.gz']
Val 9/600 28/196  loss: 0.6961 acc [ 0.306  0.266  0.263]  time 1.31s ['/workspace/workspace-kits23/kits23/dataset/case_00172/imaging.nii.gz']
Val 9/600 29/196  loss: 0.9436 acc [ 0.129  0.005  0.004]  time 1.59s ['/workspace/workspace-kits23/kits23/dataset/case_00173/imaging.nii.gz']
Val 9/600 30/196  loss: 0.9626 acc [ 0.088  0.001  0.001]  time 5.36s ['/workspace/workspace-kits23/kits23/dataset/case_00185/imaging.nii.gz']
Val 9/600 31/196  loss: 0.9083 acc [ 0.118  0.056  0.056]  time 2.68s ['/workspace/workspace-kits23/kits23/dataset/case_00189/imaging.nii.gz']
Val 9/600 32/196  loss: 0.7300 acc [ 0.232  0.247  0.248]  time 2.11s ['/workspace/workspace-kits23/kits23/dataset/case_00192/imaging.nii.gz']
Val 9/600 33/196  loss: 0.9160 acc [ 0.100  0.056  0.055]  time 3.20s ['/workspace/workspace-kits23/kits23/dataset/case_00197/imaging.nii.gz']
Val 9/600 34/196  loss: 0.9168 acc [ 0.205  0.002  0.002]  time 1.93s ['/workspace/workspace-kits23/kits23/dataset/case_00201/imaging.nii.gz']
Val 9/600 35/196  loss: 0.9443 acc [ 0.136  0.000  0.000]  time 1.64s ['/workspace/workspace-kits23/kits23/dataset/case_00208/imaging.nii.gz']
Val 9/600 36/196  loss: 0.9760 acc [ 0.058  0.001  0.001]  time 11.79s ['/workspace/workspace-kits23/kits23/dataset/case_00213/imaging.nii.gz']
Val 9/600 37/196  loss: 0.9592 acc [ 0.093  0.004  0.004]  time 3.20s ['/workspace/workspace-kits23/kits23/dataset/case_00218/imaging.nii.gz']
Val 9/600 38/196  loss: 0.9340 acc [ 0.148  0.013  0.012]  time 1.87s ['/workspace/workspace-kits23/kits23/dataset/case_00225/imaging.nii.gz']
Val 9/600 39/196  loss: 0.8782 acc [ 0.190  0.066  0.066]  time 1.68s ['/workspace/workspace-kits23/kits23/dataset/case_00230/imaging.nii.gz']
Val 9/600 40/196  loss: 0.6681 acc [ 0.351  0.324  0.320]  time 1.74s ['/workspace/workspace-kits23/kits23/dataset/case_00233/imaging.nii.gz']
Val 9/600 41/196  loss: 0.8317 acc [ 0.162  0.164  0.160]  time 2.89s ['/workspace/workspace-kits23/kits23/dataset/case_00245/imaging.nii.gz']
Val 9/600 42/196  loss: 0.9181 acc [ 0.106  0.051  0.051]  time 13.65s ['/workspace/workspace-kits23/kits23/dataset/case_00246/imaging.nii.gz']
Val 9/600 43/196  loss: 0.9550 acc [ 0.102  0.006  0.006]  time 4.73s ['/workspace/workspace-kits23/kits23/dataset/case_00250/imaging.nii.gz']
Val 9/600 44/196  loss: 0.8744 acc [ 0.256  0.077  0.000]  time 1.18s ['/workspace/workspace-kits23/kits23/dataset/case_00256/imaging.nii.gz']
Val 9/600 45/196  loss: 0.8852 acc [ 0.233  0.039  0.039]  time 0.56s ['/workspace/workspace-kits23/kits23/dataset/case_00260/imaging.nii.gz']
Val 9/600 46/196  loss: 0.9349 acc [ 0.115  0.017  0.017]  time 13.04s ['/workspace/workspace-kits23/kits23/dataset/case_00261/imaging.nii.gz']
Val 9/600 47/196  loss: 0.9368 acc [ 0.136  0.010  0.010]  time 5.15s ['/workspace/workspace-kits23/kits23/dataset/case_00273/imaging.nii.gz']
Val 9/600 48/196  loss: 0.8756 acc [ 0.196  0.065  0.064]  time 1.76s ['/workspace/workspace-kits23/kits23/dataset/case_00275/imaging.nii.gz']
Val 9/600 49/196  loss: 0.9150 acc [ 0.101  0.057  0.058]  time 4.84s ['/workspace/workspace-kits23/kits23/dataset/case_00284/imaging.nii.gz']
Val 9/600 50/196  loss: 0.8817 acc [ 0.247  0.028  0.028]  time 0.55s ['/workspace/workspace-kits23/kits23/dataset/case_00287/imaging.nii.gz']
Val 9/600 51/196  loss: 0.9468 acc [ 0.125  0.001  0.001]  time 13.58s ['/workspace/workspace-kits23/kits23/dataset/case_00290/imaging.nii.gz']
Val 9/600 52/196  loss: 0.9161 acc [ 0.141  0.034  0.035]  time 2.23s ['/workspace/workspace-kits23/kits23/dataset/case_00291/imaging.nii.gz']
Val 9/600 53/196  loss: 0.9686 acc [ 0.074  0.004  0.004]  time 13.56s ['/workspace/workspace-kits23/kits23/dataset/case_00294/imaging.nii.gz']
Val 9/600 54/196  loss: 0.9460 acc [ 0.122  0.011  0.011]  time 12.33s ['/workspace/workspace-kits23/kits23/dataset/case_00295/imaging.nii.gz']
Val 9/600 55/196  loss: 0.9112 acc [ 0.159  0.036  0.036]  time 14.00s ['/workspace/workspace-kits23/kits23/dataset/case_00298/imaging.nii.gz']
Val 9/600 56/196  loss: 0.9634 acc [ 0.078  0.007  0.001]  time 1.99s ['/workspace/workspace-kits23/kits23/dataset/case_00400/imaging.nii.gz']
Val 9/600 57/196  loss: 0.9179 acc [ 0.158  0.020  0.002]  time 0.59s ['/workspace/workspace-kits23/kits23/dataset/case_00403/imaging.nii.gz']
Val 9/600 58/196  loss: 0.9586 acc [ 0.080  0.010  0.004]  time 2.65s ['/workspace/workspace-kits23/kits23/dataset/case_00404/imaging.nii.gz']
Val 9/600 59/196  loss: 0.9548 acc [ 0.112  0.001  0.001]  time 1.82s ['/workspace/workspace-kits23/kits23/dataset/case_00414/imaging.nii.gz']
Val 9/600 60/196  loss: 0.9274 acc [ 0.163  0.008  0.008]  time 2.51s ['/workspace/workspace-kits23/kits23/dataset/case_00415/imaging.nii.gz']
Val 9/600 61/196  loss: 0.7929 acc [ 0.317  0.149  0.146]  time 0.58s ['/workspace/workspace-kits23/kits23/dataset/case_00418/imaging.nii.gz']
Val 9/600 62/196  loss: 0.9160 acc [ 0.218  0.000  0.000]  time 0.57s ['/workspace/workspace-kits23/kits23/dataset/case_00422/imaging.nii.gz']
Val 9/600 63/196  loss: 0.8257 acc [ 0.164  0.145  0.146]  time 1.47s ['/workspace/workspace-kits23/kits23/dataset/case_00426/imaging.nii.gz']
Val 9/600 64/196  loss: 0.6572 acc [ 0.530  0.493  0.489]  time 0.53s ['/workspace/workspace-kits23/kits23/dataset/case_00430/imaging.nii.gz']
Val 9/600 65/196  loss: 0.8870 acc [ 0.119  0.074  0.074]  time 2.05s ['/workspace/workspace-kits23/kits23/dataset/case_00431/imaging.nii.gz']
Val 9/600 66/196  loss: 0.9425 acc [ 0.114  0.000  0.000]  time 0.67s ['/workspace/workspace-kits23/kits23/dataset/case_00434/imaging.nii.gz']
Val 9/600 67/196  loss: 0.8945 acc [ 0.126  0.023  0.020]  time 1.16s ['/workspace/workspace-kits23/kits23/dataset/case_00439/imaging.nii.gz']
Val 9/600 68/196  loss: 0.9709 acc [ 0.070  0.000  0.000]  time 3.97s ['/workspace/workspace-kits23/kits23/dataset/case_00447/imaging.nii.gz']
Val 9/600 69/196  loss: 0.8568 acc [ 0.274  0.051  0.052]  time 1.33s ['/workspace/workspace-kits23/kits23/dataset/case_00458/imaging.nii.gz']
Val 9/600 70/196  loss: 0.9641 acc [ 0.080  0.003  0.003]  time 1.92s ['/workspace/workspace-kits23/kits23/dataset/case_00462/imaging.nii.gz']
Val 9/600 71/196  loss: 0.8529 acc [ 0.319  0.030  0.029]  time 0.64s ['/workspace/workspace-kits23/kits23/dataset/case_00464/imaging.nii.gz']
Val 9/600 72/196  loss: 0.8867 acc [ 0.201  0.068  0.015]  time 0.57s ['/workspace/workspace-kits23/kits23/dataset/case_00470/imaging.nii.gz']
Val 9/600 73/196  loss: 0.9192 acc [ 0.172  0.019  0.017]  time 1.83s ['/workspace/workspace-kits23/kits23/dataset/case_00475/imaging.nii.gz']
Val 9/600 74/196  loss: 0.9171 acc [ 0.178  0.005  0.005]  time 0.54s ['/workspace/workspace-kits23/kits23/dataset/case_00476/imaging.nii.gz']
Val 9/600 75/196  loss: 0.8696 acc [ 0.244  0.039  0.037]  time 1.07s ['/workspace/workspace-kits23/kits23/dataset/case_00485/imaging.nii.gz']
Val 9/600 76/196  loss: 0.8825 acc [ 0.165  0.079  0.080]  time 3.40s ['/workspace/workspace-kits23/kits23/dataset/case_00489/imaging.nii.gz']
Val 9/600 77/196  loss: 0.9180 acc [ 0.137  0.025  0.025]  time 1.59s ['/workspace/workspace-kits23/kits23/dataset/case_00492/imaging.nii.gz']
Val 9/600 78/196  loss: 0.9448 acc [ 0.110  0.005  0.004]  time 1.61s ['/workspace/workspace-kits23/kits23/dataset/case_00494/imaging.nii.gz']
Val 9/600 79/196  loss: 0.9599 acc [ 0.095  0.002  0.001]  time 4.82s ['/workspace/workspace-kits23/kits23/dataset/case_00504/imaging.nii.gz']
Val 9/600 80/196  loss: 0.9703 acc [ 0.071  0.000  0.000]  time 3.77s ['/workspace/workspace-kits23/kits23/dataset/case_00509/imaging.nii.gz']
Val 9/600 81/196  loss: 0.9537 acc [ 0.069  0.026  0.025]  time 17.32s ['/workspace/workspace-kits23/kits23/dataset/case_00510/imaging.nii.gz']
Val 9/600 82/196  loss: 0.9301 acc [ 0.158  0.004  0.002]  time 1.01s ['/workspace/workspace-kits23/kits23/dataset/case_00512/imaging.nii.gz']
Val 9/600 83/196  loss: 0.6567 acc [ 0.506  0.526  0.519]  time 1.74s ['/workspace/workspace-kits23/kits23/dataset/case_00516/imaging.nii.gz']
Val 9/600 84/196  loss: 0.9538 acc [ 0.110  0.003  0.003]  time 1.56s ['/workspace/workspace-kits23/kits23/dataset/case_00520/imaging.nii.gz']
Val 9/600 85/196  loss: 0.9539 acc [ 0.104  0.008  0.008]  time 10.57s ['/workspace/workspace-kits23/kits23/dataset/case_00528/imaging.nii.gz']
Val 9/600 86/196  loss: 0.9217 acc [ 0.148  0.032  0.029]  time 2.35s ['/workspace/workspace-kits23/kits23/dataset/case_00532/imaging.nii.gz']
Val 9/600 87/196  loss: 0.8800 acc [ 0.254  0.027  0.028]  time 0.51s ['/workspace/workspace-kits23/kits23/dataset/case_00534/imaging.nii.gz']
Val 9/600 88/196  loss: 0.9591 acc [ 0.069  0.015  0.015]  time 12.78s ['/workspace/workspace-kits23/kits23/dataset/case_00535/imaging.nii.gz']
Val 9/600 89/196  loss: 0.9850 acc [ 0.023  0.008  0.004]  time 9.47s ['/workspace/workspace-kits23/kits23/dataset/case_00540/imaging.nii.gz']
Val 9/600 90/196  loss: 0.7381 acc [ 0.438  0.123  0.118]  time 0.47s ['/workspace/workspace-kits23/kits23/dataset/case_00546/imaging.nii.gz']
Val 9/600 91/196  loss: 0.8396 acc [ 0.181  0.117  0.118]  time 2.84s ['/workspace/workspace-kits23/kits23/dataset/case_00552/imaging.nii.gz']
Val 9/600 92/196  loss: 0.9544 acc [ 0.088  0.019  0.018]  time 8.89s ['/workspace/workspace-kits23/kits23/dataset/case_00553/imaging.nii.gz']
Val 9/600 93/196  loss: 0.9598 acc [ 0.098  0.001  0.001]  time 3.00s ['/workspace/workspace-kits23/kits23/dataset/case_00557/imaging.nii.gz']
Val 9/600 94/196  loss: 0.9326 acc [ 0.157  0.006  0.006]  time 10.05s ['/workspace/workspace-kits23/kits23/dataset/case_00559/imaging.nii.gz']
Val 9/600 95/196  loss: 0.8390 acc [ 0.432  0.000  0.000]  time 0.51s ['/workspace/workspace-kits23/kits23/dataset/case_00569/imaging.nii.gz']
Val 9/600 96/196  loss: 0.9082 acc [ 0.249  0.000  0.000]  time 0.54s ['/workspace/workspace-kits23/kits23/dataset/case_00574/imaging.nii.gz']
Val 9/600 97/196  loss: 0.9162 acc [ 0.127  0.081  0.003]  time 2.93s ['/workspace/workspace-kits23/kits23/dataset/case_00586/imaging.nii.gz']
Val 9/600 98/196  loss: 0.8068 acc [ 0.194  0.151  0.152]  time 1.71s ['/workspace/workspace-kits23/kits23/dataset/case_00012/imaging.nii.gz']
Val 9/600 99/196  loss: 0.9235 acc [ 0.154  0.013  0.013]  time 1.62s ['/workspace/workspace-kits23/kits23/dataset/case_00013/imaging.nii.gz']
Val 9/600 100/196  loss: 0.9325 acc [ 0.154  0.007  0.007]  time 2.27s ['/workspace/workspace-kits23/kits23/dataset/case_00018/imaging.nii.gz']
Val 9/600 101/196  loss: 0.9324 acc [ 0.169  0.002  0.002]  time 1.53s ['/workspace/workspace-kits23/kits23/dataset/case_00023/imaging.nii.gz']
Val 9/600 102/196  loss: 0.9567 acc [ 0.105  0.001  0.001]  time 15.66s ['/workspace/workspace-kits23/kits23/dataset/case_00027/imaging.nii.gz']
Val 9/600 103/196  loss: 0.9427 acc [ 0.140  0.000  0.000]  time 1.79s ['/workspace/workspace-kits23/kits23/dataset/case_00039/imaging.nii.gz']
Val 9/600 104/196  loss: 0.9499 acc [ 0.120  0.002  0.002]  time 3.94s ['/workspace/workspace-kits23/kits23/dataset/case_00040/imaging.nii.gz']
Val 9/600 105/196  loss: 0.9089 acc [ 0.221  0.018  0.017]  time 0.53s ['/workspace/workspace-kits23/kits23/dataset/case_00041/imaging.nii.gz']
Val 9/600 106/196  loss: 0.9485 acc [ 0.098  0.014  0.014]  time 2.69s ['/workspace/workspace-kits23/kits23/dataset/case_00046/imaging.nii.gz']
Val 9/600 107/196  loss: 0.8887 acc [ 0.182  0.063  0.042]  time 1.85s ['/workspace/workspace-kits23/kits23/dataset/case_00058/imaging.nii.gz']
Val 9/600 108/196  loss: 0.9537 acc [ 0.107  0.005  0.005]  time 15.09s ['/workspace/workspace-kits23/kits23/dataset/case_00059/imaging.nii.gz']
Val 9/600 109/196  loss: 0.8242 acc [ 0.332  0.070  0.068]  time 0.56s ['/workspace/workspace-kits23/kits23/dataset/case_00061/imaging.nii.gz']
Val 9/600 110/196  loss: 0.9047 acc [ 0.176  0.035  0.006]  time 1.88s ['/workspace/workspace-kits23/kits23/dataset/case_00069/imaging.nii.gz']
Val 9/600 111/196  loss: 0.8818 acc [ 0.138  0.070  0.070]  time 2.83s ['/workspace/workspace-kits23/kits23/dataset/case_00073/imaging.nii.gz']
Val 9/600 112/196  loss: 0.8248 acc [ 0.222  0.131  0.130]  time 6.32s ['/workspace/workspace-kits23/kits23/dataset/case_00078/imaging.nii.gz']
Val 9/600 113/196  loss: 0.9307 acc [ 0.141  0.019  0.018]  time 1.07s ['/workspace/workspace-kits23/kits23/dataset/case_00080/imaging.nii.gz']
Val 9/600 114/196  loss: 0.9549 acc [ 0.109  0.001  0.001]  time 2.68s ['/workspace/workspace-kits23/kits23/dataset/case_00081/imaging.nii.gz']
Val 9/600 115/196  loss: 0.9570 acc [ 0.096  0.001  0.001]  time 2.02s ['/workspace/workspace-kits23/kits23/dataset/case_00082/imaging.nii.gz']
Val 9/600 116/196  loss: 0.8769 acc [ 0.230  0.043  0.042]  time 0.57s ['/workspace/workspace-kits23/kits23/dataset/case_00089/imaging.nii.gz']
Val 9/600 117/196  loss: 0.9560 acc [ 0.109  0.006  0.006]  time 21.23s ['/workspace/workspace-kits23/kits23/dataset/case_00091/imaging.nii.gz']
Val 9/600 118/196  loss: 0.9514 acc [ 0.113  0.009  0.003]  time 19.23s ['/workspace/workspace-kits23/kits23/dataset/case_00093/imaging.nii.gz']
Val 9/600 119/196  loss: 0.9539 acc [ 0.077  0.019  0.018]  time 6.37s ['/workspace/workspace-kits23/kits23/dataset/case_00095/imaging.nii.gz']
Val 9/600 120/196  loss: 0.9253 acc [ 0.157  0.017  0.017]  time 5.07s ['/workspace/workspace-kits23/kits23/dataset/case_00098/imaging.nii.gz']
Val 9/600 121/196  loss: 0.9444 acc [ 0.144  0.001  0.001]  time 9.51s ['/workspace/workspace-kits23/kits23/dataset/case_00101/imaging.nii.gz']
Val 9/600 122/196  loss: 0.9563 acc [ 0.106  0.001  0.001]  time 1.59s ['/workspace/workspace-kits23/kits23/dataset/case_00106/imaging.nii.gz']
Val 9/600 123/196  loss: 0.9729 acc [ 0.065  0.001  0.001]  time 6.60s ['/workspace/workspace-kits23/kits23/dataset/case_00120/imaging.nii.gz']
Val 9/600 124/196  loss: 0.9057 acc [ 0.216  0.017  0.016]  time 0.60s ['/workspace/workspace-kits23/kits23/dataset/case_00122/imaging.nii.gz']
Val 9/600 125/196  loss: 0.9724 acc [ 0.063  0.000  0.000]  time 3.01s ['/workspace/workspace-kits23/kits23/dataset/case_00125/imaging.nii.gz']
Val 9/600 126/196  loss: 0.9426 acc [ 0.137  0.003  0.003]  time 1.43s ['/workspace/workspace-kits23/kits23/dataset/case_00127/imaging.nii.gz']
Val 9/600 127/196  loss: 0.8254 acc [ 0.185  0.115  0.114]  time 13.42s ['/workspace/workspace-kits23/kits23/dataset/case_00135/imaging.nii.gz']
Val 9/600 128/196  loss: 0.6614 acc [ 0.326  0.324  0.324]  time 1.48s ['/workspace/workspace-kits23/kits23/dataset/case_00139/imaging.nii.gz']
Val 9/600 129/196  loss: 0.9393 acc [ 0.157  0.003  0.003]  time 14.32s ['/workspace/workspace-kits23/kits23/dataset/case_00146/imaging.nii.gz']
Val 9/600 130/196  loss: 0.9204 acc [ 0.144  0.023  0.022]  time 10.97s ['/workspace/workspace-kits23/kits23/dataset/case_00157/imaging.nii.gz']
Val 9/600 131/196  loss: 0.8601 acc [ 0.270  0.048  0.048]  time 0.50s ['/workspace/workspace-kits23/kits23/dataset/case_00161/imaging.nii.gz']
Val 9/600 132/196  loss: 0.8961 acc [ 0.143  0.057  0.060]  time 1.52s ['/workspace/workspace-kits23/kits23/dataset/case_00162/imaging.nii.gz']
Val 9/600 133/196  loss: 0.9078 acc [ 0.181  0.024  0.023]  time 0.82s ['/workspace/workspace-kits23/kits23/dataset/case_00168/imaging.nii.gz']
Val 9/600 134/196  loss: 0.8733 acc [ 0.152  0.078  0.079]  time 2.18s ['/workspace/workspace-kits23/kits23/dataset/case_00171/imaging.nii.gz']
Val 9/600 135/196  loss: 0.8012 acc [ 0.236  0.145  0.147]  time 1.80s ['/workspace/workspace-kits23/kits23/dataset/case_00179/imaging.nii.gz']
Val 9/600 136/196  loss: 0.9270 acc [ 0.077  0.009  0.007]  time 2.69s ['/workspace/workspace-kits23/kits23/dataset/case_00180/imaging.nii.gz']
Val 9/600 137/196  loss: 0.8645 acc [ 0.150  0.084  0.085]  time 2.51s ['/workspace/workspace-kits23/kits23/dataset/case_00186/imaging.nii.gz']
Val 9/600 138/196  loss: 0.9119 acc [ 0.183  0.007  0.006]  time 1.08s ['/workspace/workspace-kits23/kits23/dataset/case_00187/imaging.nii.gz']
Val 9/600 139/196  loss: 0.9717 acc [ 0.066  0.003  0.003]  time 11.14s ['/workspace/workspace-kits23/kits23/dataset/case_00214/imaging.nii.gz']
Val 9/600 140/196  loss: 0.7344 acc [ 0.238  0.206  0.204]  time 3.16s ['/workspace/workspace-kits23/kits23/dataset/case_00221/imaging.nii.gz']
Val 9/600 141/196  loss: 0.8554 acc [ 0.119  0.111  0.110]  time 11.28s ['/workspace/workspace-kits23/kits23/dataset/case_00223/imaging.nii.gz']
Val 9/600 142/196  loss: 0.7788 acc [ 0.235  0.165  0.167]  time 1.47s ['/workspace/workspace-kits23/kits23/dataset/case_00224/imaging.nii.gz']
Val 9/600 143/196  loss: 0.9162 acc [ 0.171  0.020  0.020]  time 1.50s ['/workspace/workspace-kits23/kits23/dataset/case_00232/imaging.nii.gz']
Val 9/600 144/196  loss: 0.9403 acc [ 0.094  0.039  0.038]  time 0.54s ['/workspace/workspace-kits23/kits23/dataset/case_00236/imaging.nii.gz']
Val 9/600 145/196  loss: 0.9603 acc [ 0.080  0.005  0.000]  time 1.72s ['/workspace/workspace-kits23/kits23/dataset/case_00238/imaging.nii.gz']
Val 9/600 146/196  loss: 0.8767 acc [ 0.306  0.024  0.023]  time 1.14s ['/workspace/workspace-kits23/kits23/dataset/case_00244/imaging.nii.gz']
Val 9/600 147/196  loss: 0.8522 acc [ 0.194  0.088  0.089]  time 5.37s ['/workspace/workspace-kits23/kits23/dataset/case_00249/imaging.nii.gz']
Val 9/600 148/196  loss: 0.7949 acc [ 0.455  0.121  0.001]  time 0.56s ['/workspace/workspace-kits23/kits23/dataset/case_00257/imaging.nii.gz']
Val 9/600 149/196  loss: 0.9162 acc [ 0.153  0.035  0.035]  time 1.59s ['/workspace/workspace-kits23/kits23/dataset/case_00258/imaging.nii.gz']
Val 9/600 150/196  loss: 0.9247 acc [ 0.177  0.000  0.000]  time 0.59s ['/workspace/workspace-kits23/kits23/dataset/case_00267/imaging.nii.gz']
Val 9/600 151/196  loss: 0.9031 acc [ 0.245  0.018  0.016]  time 3.44s ['/workspace/workspace-kits23/kits23/dataset/case_00268/imaging.nii.gz']
Val 9/600 152/196  loss: 0.9351 acc [ 0.133  0.015  0.015]  time 6.99s ['/workspace/workspace-kits23/kits23/dataset/case_00283/imaging.nii.gz']
Val 9/600 153/196  loss: 0.8880 acc [ 0.280  0.001  0.000]  time 0.60s ['/workspace/workspace-kits23/kits23/dataset/case_00285/imaging.nii.gz']
Val 9/600 154/196  loss: 0.8504 acc [ 0.184  0.119  0.117]  time 1.93s ['/workspace/workspace-kits23/kits23/dataset/case_00286/imaging.nii.gz']
Val 9/600 155/196  loss: 0.9074 acc [ 0.138  0.054  0.053]  time 10.80s ['/workspace/workspace-kits23/kits23/dataset/case_00293/imaging.nii.gz']
Val 9/600 156/196  loss: 0.8843 acc [ 0.249  0.025  0.025]  time 1.91s ['/workspace/workspace-kits23/kits23/dataset/case_00405/imaging.nii.gz']
Val 9/600 157/196  loss: 0.9586 acc [ 0.094  0.000  0.000]  time 1.69s ['/workspace/workspace-kits23/kits23/dataset/case_00407/imaging.nii.gz']
Val 9/600 158/196  loss: 0.8963 acc [ 0.155  0.064  0.063]  time 1.81s ['/workspace/workspace-kits23/kits23/dataset/case_00409/imaging.nii.gz']
Val 9/600 159/196  loss: 0.9492 acc [ 0.103  0.000  0.000]  time 0.60s ['/workspace/workspace-kits23/kits23/dataset/case_00423/imaging.nii.gz']
Val 9/600 160/196  loss: 0.9476 acc [ 0.121  0.000  0.000]  time 1.29s ['/workspace/workspace-kits23/kits23/dataset/case_00424/imaging.nii.gz']
Val 9/600 161/196  loss: 0.9205 acc [ 0.140  0.034  0.022]  time 1.69s ['/workspace/workspace-kits23/kits23/dataset/case_00428/imaging.nii.gz']
Val 9/600 162/196  loss: 0.9416 acc [ 0.140  0.001  0.001]  time 1.66s ['/workspace/workspace-kits23/kits23/dataset/case_00429/imaging.nii.gz']
Val 9/600 163/196  loss: 0.9571 acc [ 0.080  0.000  0.000]  time 0.58s ['/workspace/workspace-kits23/kits23/dataset/case_00443/imaging.nii.gz']
Val 9/600 164/196  loss: 0.9531 acc [ 0.114  0.001  0.000]  time 1.65s ['/workspace/workspace-kits23/kits23/dataset/case_00444/imaging.nii.gz']
Val 9/600 165/196  loss: 0.9022 acc [ 0.103  0.052  0.053]  time 1.84s ['/workspace/workspace-kits23/kits23/dataset/case_00450/imaging.nii.gz']
Val 9/600 166/196  loss: 0.9372 acc [ 0.167  0.000  0.000]  time 2.54s ['/workspace/workspace-kits23/kits23/dataset/case_00451/imaging.nii.gz']
Val 9/600 167/196  loss: 0.9558 acc [ 0.094  0.004  0.000]  time 2.86s ['/workspace/workspace-kits23/kits23/dataset/case_00455/imaging.nii.gz']
Val 9/600 168/196  loss: 0.9152 acc [ 0.113  0.082  0.006]  time 3.15s ['/workspace/workspace-kits23/kits23/dataset/case_00461/imaging.nii.gz']
Val 9/600 169/196  loss: 0.9490 acc [ 0.082  0.018  0.018]  time 2.27s ['/workspace/workspace-kits23/kits23/dataset/case_00466/imaging.nii.gz']
Val 9/600 170/196  loss: 0.8258 acc [ 0.185  0.145  0.142]  time 2.80s ['/workspace/workspace-kits23/kits23/dataset/case_00472/imaging.nii.gz']
Val 9/600 171/196  loss: 0.9094 acc [ 0.208  0.001  0.001]  time 1.07s ['/workspace/workspace-kits23/kits23/dataset/case_00477/imaging.nii.gz']
Val 9/600 172/196  loss: 0.9189 acc [ 0.142  0.032  0.032]  time 1.71s ['/workspace/workspace-kits23/kits23/dataset/case_00478/imaging.nii.gz']
Val 9/600 173/196  loss: 0.9206 acc [ 0.170  0.014  0.013]  time 2.99s ['/workspace/workspace-kits23/kits23/dataset/case_00479/imaging.nii.gz']
Val 9/600 174/196  loss: 0.7602 acc [ 0.253  0.225  0.168]  time 2.62s ['/workspace/workspace-kits23/kits23/dataset/case_00480/imaging.nii.gz']
Val 9/600 175/196  loss: 0.8929 acc [ 0.175  0.047  0.047]  time 4.98s ['/workspace/workspace-kits23/kits23/dataset/case_00482/imaging.nii.gz']
Val 9/600 176/196  loss: 0.6444 acc [ 0.429  0.324  0.316]  time 0.62s ['/workspace/workspace-kits23/kits23/dataset/case_00483/imaging.nii.gz']
Val 9/600 177/196  loss: 0.8727 acc [ 0.320  0.011  0.011]  time 0.57s ['/workspace/workspace-kits23/kits23/dataset/case_00497/imaging.nii.gz']
Val 9/600 178/196  loss: 0.9186 acc [ 0.162  0.000  0.000]  time 0.54s ['/workspace/workspace-kits23/kits23/dataset/case_00506/imaging.nii.gz']
Val 9/600 179/196  loss: 0.9552 acc [ 0.101  0.003  0.003]  time 1.69s ['/workspace/workspace-kits23/kits23/dataset/case_00511/imaging.nii.gz']
Val 9/600 180/196  loss: 0.9194 acc [ 0.149  0.045  0.001]  time 6.42s ['/workspace/workspace-kits23/kits23/dataset/case_00515/imaging.nii.gz']
Val 9/600 181/196  loss: 0.9542 acc [ 0.108  0.005  0.005]  time 2.39s ['/workspace/workspace-kits23/kits23/dataset/case_00518/imaging.nii.gz']
Val 9/600 182/196  loss: 0.9491 acc [ 0.125  0.000  0.000]  time 1.51s ['/workspace/workspace-kits23/kits23/dataset/case_00524/imaging.nii.gz']
Val 9/600 183/196  loss: 0.8319 acc [ 0.180  0.142  0.140]  time 3.19s ['/workspace/workspace-kits23/kits23/dataset/case_00526/imaging.nii.gz']
Val 9/600 184/196  loss: 0.6607 acc [ 0.421  0.278  0.274]  time 0.54s ['/workspace/workspace-kits23/kits23/dataset/case_00527/imaging.nii.gz']
Val 9/600 185/196  loss: 0.8715 acc [ 0.306  0.037  0.037]  time 0.50s ['/workspace/workspace-kits23/kits23/dataset/case_00539/imaging.nii.gz']
Val 9/600 186/196  loss: 0.9584 acc [ 0.101  0.002  0.002]  time 8.98s ['/workspace/workspace-kits23/kits23/dataset/case_00541/imaging.nii.gz']
Val 9/600 187/196  loss: 0.9697 acc [ 0.068  0.003  0.000]  time 10.62s ['/workspace/workspace-kits23/kits23/dataset/case_00544/imaging.nii.gz']
Val 9/600 188/196  loss: 0.9465 acc [ 0.113  0.010  0.010]  time 3.52s ['/workspace/workspace-kits23/kits23/dataset/case_00547/imaging.nii.gz']
Val 9/600 189/196  loss: 0.9439 acc [ 0.115  0.020  0.000]  time 4.44s ['/workspace/workspace-kits23/kits23/dataset/case_00563/imaging.nii.gz']
Val 9/600 190/196  loss: 0.7655 acc [ 0.322  0.180  0.176]  time 0.53s ['/workspace/workspace-kits23/kits23/dataset/case_00565/imaging.nii.gz']
Val 9/600 191/196  loss: 0.8539 acc [ 0.196  0.080  0.080]  time 2.26s ['/workspace/workspace-kits23/kits23/dataset/case_00577/imaging.nii.gz']
Val 9/600 192/196  loss: 0.9578 acc [ 0.105  0.000  0.000]  time 3.71s ['/workspace/workspace-kits23/kits23/dataset/case_00579/imaging.nii.gz']
Val 9/600 193/196  loss: 0.9402 acc [ 0.137  0.002  0.002]  time 1.60s ['/workspace/workspace-kits23/kits23/dataset/case_00584/imaging.nii.gz']
Val 9/600 194/196  loss: 0.9618 acc [ 0.097  0.000  0.000]  time 3.17s ['/workspace/workspace-kits23/kits23/dataset/case_00585/imaging.nii.gz']
Val 9/600 195/196  loss: 0.9445 acc [ 0.123  0.009  0.009]  time 4.78s ['/workspace/workspace-kits23/kits23/dataset/case_00588/imaging.nii.gz']
Final validation 9/599 loss: 0.9006 acc_avg: 0.0877 acc: [ 0.167  0.050  0.046] time: 778.49s
New best metric (-1.000000 --> 0.087724). 
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt, {'epoch': 9, 'best_metric': 0.08772376924753189}, save_time 0.10s
Progress: best_avg_dice_score_epoch: 9, best_avg_dice_score: 0.08772376924753189, save_time: 0.09522294998168945, time: 0.31 hr, train_time: 2.48s, validation_time: 778.49s, epoch_time: 780.97s, model: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model.pt, date: 2024-08-20 03:15:30
Estimated remaining training time for the current model fold 0 is 20.09 hr, running time 0.31 hr, est total time 20.39 hr 

Epoch 10/600 0/37 loss: 1.2301 acc [ 0.537  0.182  0.197]  time 0.46s 
Epoch 10/600 1/37 loss: 1.0965 acc [ 0.599  0.229  0.247]  time 0.07s 
Epoch 10/600 2/37 loss: 1.1102 acc [ 0.592  0.237  0.266]  time 0.06s 
Epoch 10/600 3/37 loss: 1.0990 acc [ 0.629  0.264  0.287]  time 0.06s 
Epoch 10/600 4/37 loss: 1.0931 acc [ 0.655  0.257  0.272]  time 0.06s 
Epoch 10/600 5/37 loss: 1.1108 acc [ 0.666  0.226  0.240]  time 0.06s 
Epoch 10/600 6/37 loss: 1.1148 acc [ 0.650  0.234  0.244]  time 0.06s 
Epoch 10/600 7/37 loss: 1.1134 acc [ 0.659  0.229  0.238]  time 0.06s 
Epoch 10/600 8/37 loss: 1.1156 acc [ 0.660  0.238  0.244]  time 0.06s 
Epoch 10/600 9/37 loss: 1.1620 acc [ 0.653  0.232  0.235]  time 0.06s 
Epoch 10/600 10/37 loss: 1.1628 acc [ 0.653  0.227  0.230]  time 0.06s 
Epoch 10/600 11/37 loss: 1.1738 acc [ 0.638  0.224  0.231]  time 0.06s 
Epoch 10/600 12/37 loss: 1.1718 acc [ 0.633  0.221  0.225]  time 0.06s 
Epoch 10/600 13/37 loss: 1.1712 acc [ 0.639  0.211  0.213]  time 0.06s 
Epoch 10/600 14/37 loss: 1.1657 acc [ 0.638  0.213  0.218]  time 0.06s 
Epoch 10/600 15/37 loss: 1.1962 acc [ 0.620  0.198  0.201]  time 0.06s 
Epoch 10/600 16/37 loss: 1.1935 acc [ 0.616  0.195  0.200]  time 0.07s 
Epoch 10/600 17/37 loss: 1.1931 acc [ 0.621  0.199  0.204]  time 0.06s 
Epoch 10/600 18/37 loss: 1.1871 acc [ 0.621  0.205  0.211]  time 0.06s 
Epoch 10/600 19/37 loss: 1.1822 acc [ 0.627  0.225  0.230]  time 0.06s 
Epoch 10/600 20/37 loss: 1.1821 acc [ 0.624  0.220  0.225]  time 0.07s 
Epoch 10/600 21/37 loss: 1.1790 acc [ 0.626  0.216  0.222]  time 0.07s 
Epoch 10/600 22/37 loss: 1.1931 acc [ 0.616  0.218  0.223]  time 0.06s 
Epoch 10/600 23/37 loss: 1.2144 acc [ 0.604  0.213  0.218]  time 0.06s 
Epoch 10/600 24/37 loss: 1.2171 acc [ 0.607  0.204  0.212]  time 0.06s 
Epoch 10/600 25/37 loss: 1.2212 acc [ 0.608  0.207  0.214]  time 0.06s 
Epoch 10/600 26/37 loss: 1.2146 acc [ 0.607  0.214  0.221]  time 0.06s 
Epoch 10/600 27/37 loss: 1.2114 acc [ 0.609  0.214  0.223]  time 0.06s 
Epoch 10/600 28/37 loss: 1.2058 acc [ 0.607  0.221  0.230]  time 0.06s 
Epoch 10/600 29/37 loss: 1.1980 acc [ 0.601  0.226  0.236]  time 0.06s 
Epoch 10/600 30/37 loss: 1.2037 acc [ 0.601  0.230  0.239]  time 0.06s 
Epoch 10/600 31/37 loss: 1.2029 acc [ 0.601  0.231  0.242]  time 0.05s 
Epoch 10/600 32/37 loss: 1.2006 acc [ 0.600  0.230  0.241]  time 0.06s 
Epoch 10/600 33/37 loss: 1.2042 acc [ 0.599  0.224  0.236]  time 0.06s 
Epoch 10/600 34/37 loss: 1.2045 acc [ 0.603  0.222  0.236]  time 0.05s 
Epoch 10/600 35/37 loss: 1.2035 acc [ 0.606  0.221  0.233]  time 0.05s 
Epoch 10/600 36/37 loss: 1.2026 acc [ 0.605  0.221  0.234]  time 0.04s 
Final training  10/599 loss: 1.2026 acc_avg: 0.3534 acc [ 0.605  0.221  0.234] time 2.62s  lr: 1.9993e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 10, 'best_metric': -1}, save_time 0.13s
Estimated remaining training time for the current model fold 0 is 20.11 hr, running time 0.31 hr, est total time 20.42 hr 

Epoch 11/600 0/37 loss: 1.1971 acc [ 0.678  0.076  0.071]  time 0.32s 
Epoch 11/600 1/37 loss: 1.1573 acc [ 0.614  0.131  0.118]  time 0.06s 
Epoch 11/600 2/37 loss: 1.1029 acc [ 0.617  0.164  0.165]  time 0.06s 
Epoch 11/600 3/37 loss: 1.0977 acc [ 0.622  0.151  0.156]  time 0.06s 
Epoch 11/600 4/37 loss: 1.1263 acc [ 0.583  0.118  0.124]  time 0.08s 
Epoch 11/600 5/37 loss: 1.1217 acc [ 0.574  0.116  0.125]  time 0.06s 
Epoch 11/600 6/37 loss: 1.1191 acc [ 0.603  0.103  0.109]  time 0.06s 
Epoch 11/600 7/37 loss: 1.1092 acc [ 0.615  0.105  0.112]  time 0.06s 
Epoch 11/600 8/37 loss: 1.0873 acc [ 0.610  0.115  0.122]  time 0.06s 
Epoch 11/600 9/37 loss: 1.1121 acc [ 0.604  0.105  0.108]  time 0.06s 
Epoch 11/600 10/37 loss: 1.0976 acc [ 0.618  0.120  0.126]  time 0.06s 
Epoch 11/600 11/37 loss: 1.0907 acc [ 0.632  0.117  0.124]  time 0.06s 
Epoch 11/600 12/37 loss: 1.0878 acc [ 0.631  0.120  0.128]  time 0.06s 
Epoch 11/600 13/37 loss: 1.0953 acc [ 0.633  0.115  0.121]  time 0.06s 
Epoch 11/600 14/37 loss: 1.1166 acc [ 0.609  0.116  0.122]  time 0.06s 
Epoch 11/600 15/37 loss: 1.1283 acc [ 0.609  0.117  0.124]  time 0.06s 
Epoch 11/600 16/37 loss: 1.1505 acc [ 0.604  0.108  0.119]  time 0.06s 
Epoch 11/600 17/37 loss: 1.1416 acc [ 0.607  0.114  0.126]  time 0.06s 
Epoch 11/600 18/37 loss: 1.1418 acc [ 0.611  0.118  0.129]  time 0.05s 
Epoch 11/600 19/37 loss: 1.1401 acc [ 0.613  0.116  0.127]  time 0.06s 
Epoch 11/600 20/37 loss: 1.1405 acc [ 0.617  0.119  0.130]  time 0.06s 
Epoch 11/600 21/37 loss: 1.1444 acc [ 0.615  0.128  0.141]  time 0.06s 
Epoch 11/600 22/37 loss: 1.1459 acc [ 0.612  0.127  0.139]  time 0.06s 
Epoch 11/600 23/37 loss: 1.1489 acc [ 0.606  0.130  0.139]  time 0.06s 
Epoch 11/600 24/37 loss: 1.1479 acc [ 0.608  0.132  0.141]  time 0.06s 
Epoch 11/600 25/37 loss: 1.1556 acc [ 0.607  0.139  0.150]  time 0.06s 
Epoch 11/600 26/37 loss: 1.1521 acc [ 0.612  0.144  0.156]  time 0.06s 
Epoch 11/600 27/37 loss: 1.1536 acc [ 0.613  0.142  0.155]  time 0.06s 
Epoch 11/600 28/37 loss: 1.1566 acc [ 0.611  0.143  0.156]  time 0.06s 
Epoch 11/600 29/37 loss: 1.1578 acc [ 0.608  0.145  0.155]  time 0.06s 
Epoch 11/600 30/37 loss: 1.1565 acc [ 0.608  0.150  0.158]  time 0.06s 
Epoch 11/600 31/37 loss: 1.1602 acc [ 0.608  0.147  0.155]  time 0.06s 
Epoch 11/600 32/37 loss: 1.1544 acc [ 0.612  0.153  0.161]  time 0.06s 
Epoch 11/600 33/37 loss: 1.1522 acc [ 0.614  0.159  0.167]  time 0.06s 
Epoch 11/600 34/37 loss: 1.1468 acc [ 0.619  0.167  0.180]  time 0.06s 
Epoch 11/600 35/37 loss: 1.1461 acc [ 0.622  0.168  0.181]  time 0.06s 
Epoch 11/600 36/37 loss: 1.1426 acc [ 0.623  0.170  0.183]  time 0.04s 
Final training  11/599 loss: 1.1426 acc_avg: 0.3257 acc [ 0.623  0.170  0.183] time 2.40s  lr: 1.9991e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 11, 'best_metric': -1}, save_time 0.14s
Estimated remaining training time for the current model fold 0 is 20.07 hr, running time 0.31 hr, est total time 20.38 hr 

Epoch 12/600 0/37 loss: 1.0255 acc [ 0.564  0.288  0.274]  time 0.32s 
Epoch 12/600 1/37 loss: 1.0392 acc [ 0.608  0.299  0.328]  time 0.06s 
Epoch 12/600 2/37 loss: 1.1134 acc [ 0.557  0.250  0.263]  time 0.06s 
Epoch 12/600 3/37 loss: 1.1514 acc [ 0.567  0.233  0.263]  time 0.06s 
Epoch 12/600 4/37 loss: 1.2192 acc [ 0.601  0.223  0.243]  time 0.06s 
Epoch 12/600 5/37 loss: 1.1736 acc [ 0.613  0.206  0.221]  time 0.06s 
Epoch 12/600 6/37 loss: 1.1495 acc [ 0.635  0.235  0.242]  time 0.06s 
Epoch 12/600 7/37 loss: 1.1296 acc [ 0.645  0.238  0.245]  time 0.06s 
Epoch 12/600 8/37 loss: 1.1238 acc [ 0.654  0.216  0.227]  time 0.06s 
Epoch 12/600 9/37 loss: 1.1141 acc [ 0.667  0.209  0.217]  time 0.06s 
Epoch 12/600 10/37 loss: 1.1200 acc [ 0.669  0.209  0.214]  time 0.06s 
Epoch 12/600 11/37 loss: 1.1352 acc [ 0.678  0.224  0.228]  time 0.06s 
Epoch 12/600 12/37 loss: 1.1119 acc [ 0.689  0.221  0.228]  time 0.07s 
Epoch 12/600 13/37 loss: 1.1157 acc [ 0.694  0.212  0.224]  time 0.06s 
Epoch 12/600 14/37 loss: 1.1058 acc [ 0.688  0.221  0.236]  time 0.06s 
Epoch 12/600 15/37 loss: 1.1022 acc [ 0.691  0.226  0.241]  time 0.06s 
Epoch 12/600 16/37 loss: 1.0961 acc [ 0.688  0.229  0.241]  time 0.06s 
Epoch 12/600 17/37 loss: 1.1060 acc [ 0.686  0.224  0.241]  time 0.06s 
Epoch 12/600 18/37 loss: 1.0964 acc [ 0.688  0.230  0.249]  time 0.07s 
Epoch 12/600 19/37 loss: 1.0965 acc [ 0.684  0.229  0.246]  time 0.06s 
Epoch 12/600 20/37 loss: 1.0942 acc [ 0.687  0.222  0.242]  time 0.06s 
Epoch 12/600 21/37 loss: 1.1284 acc [ 0.683  0.224  0.246]  time 0.06s 
Epoch 12/600 22/37 loss: 1.1292 acc [ 0.681  0.218  0.244]  time 0.06s 
Epoch 12/600 23/37 loss: 1.1260 acc [ 0.680  0.215  0.238]  time 0.06s 
Epoch 12/600 24/37 loss: 1.1336 acc [ 0.683  0.213  0.238]  time 0.06s 
Epoch 12/600 25/37 loss: 1.1326 acc [ 0.684  0.219  0.244]  time 0.06s 
Epoch 12/600 26/37 loss: 1.1356 acc [ 0.683  0.217  0.242]  time 0.07s 
Epoch 12/600 27/37 loss: 1.1416 acc [ 0.679  0.213  0.235]  time 0.06s 
Epoch 12/600 28/37 loss: 1.1443 acc [ 0.678  0.216  0.238]  time 0.06s 
Epoch 12/600 29/37 loss: 1.1489 acc [ 0.676  0.215  0.236]  time 0.06s 
Epoch 12/600 30/37 loss: 1.1512 acc [ 0.678  0.213  0.235]  time 0.06s 
Epoch 12/600 31/37 loss: 1.1520 acc [ 0.678  0.222  0.246]  time 0.06s 
Epoch 12/600 32/37 loss: 1.1500 acc [ 0.676  0.224  0.247]  time 0.06s 
Epoch 12/600 33/37 loss: 1.1485 acc [ 0.680  0.222  0.246]  time 0.06s 
Epoch 12/600 34/37 loss: 1.1509 acc [ 0.675  0.220  0.243]  time 0.06s 
Epoch 12/600 35/37 loss: 1.1546 acc [ 0.670  0.216  0.237]  time 0.05s 
Epoch 12/600 36/37 loss: 1.1506 acc [ 0.672  0.219  0.241]  time 0.04s 
Final training  12/599 loss: 1.1506 acc_avg: 0.3773 acc [ 0.672  0.219  0.241] time 2.48s  lr: 1.9989e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 12, 'best_metric': -1}, save_time 0.14s
Estimated remaining training time for the current model fold 0 is 20.08 hr, running time 0.31 hr, est total time 20.39 hr 

Epoch 13/600 0/37 loss: 1.0946 acc [ 0.614  0.191  0.238]  time 0.29s 
Epoch 13/600 1/37 loss: 1.0964 acc [ 0.648  0.208  0.243]  time 0.06s 
Epoch 13/600 2/37 loss: 1.0701 acc [ 0.673  0.240  0.225]  time 0.05s 
Epoch 13/600 3/37 loss: 1.0958 acc [ 0.665  0.219  0.207]  time 0.06s 
Epoch 13/600 4/37 loss: 1.1421 acc [ 0.637  0.196  0.193]  time 0.06s 
Epoch 13/600 5/37 loss: 1.1176 acc [ 0.652  0.204  0.202]  time 0.06s 
Epoch 13/600 6/37 loss: 1.0990 acc [ 0.644  0.218  0.217]  time 0.05s 
Epoch 13/600 7/37 loss: 1.0738 acc [ 0.665  0.210  0.210]  time 0.06s 
Epoch 13/600 8/37 loss: 1.0505 acc [ 0.667  0.229  0.237]  time 0.06s 
Epoch 13/600 9/37 loss: 1.0671 acc [ 0.656  0.204  0.215]  time 0.05s 
Epoch 13/600 10/37 loss: 1.0767 acc [ 0.652  0.190  0.199]  time 0.06s 
Epoch 13/600 11/37 loss: 1.0714 acc [ 0.646  0.186  0.199]  time 0.06s 
Epoch 13/600 12/37 loss: 1.0732 acc [ 0.650  0.190  0.206]  time 0.06s 
Epoch 13/600 13/37 loss: 1.0717 acc [ 0.654  0.195  0.209]  time 0.06s 
Epoch 13/600 14/37 loss: 1.0705 acc [ 0.658  0.203  0.222]  time 0.06s 
Epoch 13/600 15/37 loss: 1.0580 acc [ 0.664  0.205  0.221]  time 0.06s 
Epoch 13/600 16/37 loss: 1.0828 acc [ 0.658  0.202  0.217]  time 0.06s 
Epoch 13/600 17/37 loss: 1.0738 acc [ 0.663  0.214  0.232]  time 0.06s 
Epoch 13/600 18/37 loss: 1.0765 acc [ 0.662  0.216  0.233]  time 0.06s 
Epoch 13/600 19/37 loss: 1.0806 acc [ 0.658  0.208  0.233]  time 0.06s 
Epoch 13/600 20/37 loss: 1.0802 acc [ 0.656  0.208  0.231]  time 0.06s 
Epoch 13/600 21/37 loss: 1.0862 acc [ 0.661  0.201  0.226]  time 0.06s 
Epoch 13/600 22/37 loss: 1.0869 acc [ 0.661  0.197  0.219]  time 0.06s 
Epoch 13/600 23/37 loss: 1.0930 acc [ 0.657  0.194  0.216]  time 0.06s 
Epoch 13/600 24/37 loss: 1.1064 acc [ 0.663  0.201  0.225]  time 0.06s 
Epoch 13/600 25/37 loss: 1.1121 acc [ 0.652  0.198  0.220]  time 0.07s 
Epoch 13/600 26/37 loss: 1.1066 acc [ 0.656  0.196  0.218]  time 0.06s 
Epoch 13/600 27/37 loss: 1.1094 acc [ 0.651  0.193  0.213]  time 0.06s 
Epoch 13/600 28/37 loss: 1.1164 acc [ 0.645  0.192  0.212]  time 0.06s 
Epoch 13/600 29/37 loss: 1.1104 acc [ 0.647  0.200  0.219]  time 0.06s 
Epoch 13/600 30/37 loss: 1.1046 acc [ 0.649  0.202  0.221]  time 0.06s 
Epoch 13/600 31/37 loss: 1.1063 acc [ 0.651  0.196  0.216]  time 0.06s 
Epoch 13/600 32/37 loss: 1.1056 acc [ 0.653  0.190  0.210]  time 0.05s 
Epoch 13/600 33/37 loss: 1.1172 acc [ 0.653  0.187  0.206]  time 0.06s 
Epoch 13/600 34/37 loss: 1.1114 acc [ 0.657  0.187  0.205]  time 0.05s 
Epoch 13/600 35/37 loss: 1.1124 acc [ 0.661  0.182  0.202]  time 0.06s 
Epoch 13/600 36/37 loss: 1.1153 acc [ 0.653  0.181  0.200]  time 0.04s 
Final training  13/599 loss: 1.1153 acc_avg: 0.3449 acc [ 0.653  0.181  0.200] time 2.39s  lr: 1.9986e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 13, 'best_metric': -1}, save_time 0.14s
Estimated remaining training time for the current model fold 0 is 20.07 hr, running time 0.31 hr, est total time 20.38 hr 

Epoch 14/600 0/37 loss: 0.9620 acc [ 0.760  0.277  0.325]  time 0.34s 
Epoch 14/600 1/37 loss: 1.0579 acc [ 0.774  0.218  0.264]  time 0.06s 
Epoch 14/600 2/37 loss: 1.1136 acc [ 0.699  0.227  0.269]  time 0.06s 
Epoch 14/600 3/37 loss: 1.1219 acc [ 0.685  0.229  0.261]  time 0.05s 
Epoch 14/600 4/37 loss: 1.1174 acc [ 0.695  0.200  0.216]  time 0.06s 
Epoch 14/600 5/37 loss: 1.1600 acc [ 0.679  0.178  0.200]  time 0.06s 
Epoch 14/600 6/37 loss: 1.1442 acc [ 0.675  0.171  0.195]  time 0.06s 
Epoch 14/600 7/37 loss: 1.1582 acc [ 0.680  0.160  0.184]  time 0.06s 
Epoch 14/600 8/37 loss: 1.1529 acc [ 0.685  0.156  0.176]  time 0.06s 
Epoch 14/600 9/37 loss: 1.1497 acc [ 0.701  0.170  0.194]  time 0.06s 
Epoch 14/600 10/37 loss: 1.1420 acc [ 0.701  0.166  0.188]  time 0.06s 
Epoch 14/600 11/37 loss: 1.1311 acc [ 0.691  0.168  0.187]  time 0.06s 
Epoch 14/600 12/37 loss: 1.1309 acc [ 0.685  0.179  0.197]  time 0.06s 
Epoch 14/600 13/37 loss: 1.1305 acc [ 0.690  0.198  0.215]  time 0.07s 
Epoch 14/600 14/37 loss: 1.1265 acc [ 0.698  0.197  0.213]  time 0.06s 
Epoch 14/600 15/37 loss: 1.1259 acc [ 0.679  0.191  0.205]  time 0.06s 
Epoch 14/600 16/37 loss: 1.1205 acc [ 0.688  0.199  0.213]  time 0.06s 
Epoch 14/600 17/37 loss: 1.1340 acc [ 0.683  0.196  0.209]  time 0.06s 
Epoch 14/600 18/37 loss: 1.1367 acc [ 0.675  0.193  0.204]  time 0.06s 
Epoch 14/600 19/37 loss: 1.1381 acc [ 0.673  0.196  0.206]  time 0.06s 
Epoch 14/600 20/37 loss: 1.1350 acc [ 0.672  0.193  0.202]  time 0.07s 
Epoch 14/600 21/37 loss: 1.1364 acc [ 0.680  0.186  0.196]  time 0.06s 
Epoch 14/600 22/37 loss: 1.1460 acc [ 0.680  0.193  0.202]  time 0.06s 
Epoch 14/600 23/37 loss: 1.1484 acc [ 0.672  0.182  0.196]  time 0.06s 
Epoch 14/600 24/37 loss: 1.1377 acc [ 0.671  0.192  0.205]  time 0.06s 
Epoch 14/600 25/37 loss: 1.1357 acc [ 0.674  0.191  0.203]  time 0.06s 
Epoch 14/600 26/37 loss: 1.1274 acc [ 0.680  0.196  0.208]  time 0.06s 
Epoch 14/600 27/37 loss: 1.1271 acc [ 0.685  0.195  0.207]  time 0.06s 
Epoch 14/600 28/37 loss: 1.1290 acc [ 0.686  0.195  0.208]  time 0.07s 
Epoch 14/600 29/37 loss: 1.1398 acc [ 0.683  0.197  0.213]  time 0.06s 
Epoch 14/600 30/37 loss: 1.1521 acc [ 0.682  0.197  0.215]  time 0.06s 
Epoch 14/600 31/37 loss: 1.1522 acc [ 0.682  0.194  0.211]  time 0.06s 
Epoch 14/600 32/37 loss: 1.1467 acc [ 0.681  0.199  0.210]  time 0.06s 
Epoch 14/600 33/37 loss: 1.1598 acc [ 0.683  0.202  0.212]  time 0.06s 
Epoch 14/600 34/37 loss: 1.1602 acc [ 0.682  0.206  0.216]  time 0.06s 
Epoch 14/600 35/37 loss: 1.1537 acc [ 0.680  0.206  0.218]  time 0.06s 
Epoch 14/600 36/37 loss: 1.1508 acc [ 0.678  0.214  0.226]  time 0.04s 
Final training  14/599 loss: 1.1508 acc_avg: 0.3727 acc [ 0.678  0.214  0.226] time 2.45s  lr: 1.9983e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 14, 'best_metric': -1}, save_time 0.14s
Estimated remaining training time for the current model fold 0 is 20.08 hr, running time 0.31 hr, est total time 20.39 hr 

Epoch 15/600 0/37 loss: 1.2156 acc [ 0.733  0.313  0.318]  time 0.30s 
Epoch 15/600 1/37 loss: 1.1426 acc [ 0.708  0.308  0.319]  time 0.06s 
Epoch 15/600 2/37 loss: 1.1032 acc [ 0.700  0.283  0.287]  time 0.06s 
Epoch 15/600 3/37 loss: 1.0799 acc [ 0.690  0.286  0.304]  time 0.07s 
Epoch 15/600 4/37 loss: 1.0547 acc [ 0.678  0.325  0.354]  time 0.06s 
Epoch 15/600 5/37 loss: 1.0998 acc [ 0.675  0.307  0.339]  time 0.06s 
Epoch 15/600 6/37 loss: 1.1394 acc [ 0.663  0.283  0.315]  time 0.06s 
Epoch 15/600 7/37 loss: 1.1714 acc [ 0.640  0.283  0.311]  time 0.06s 
Epoch 15/600 8/37 loss: 1.1745 acc [ 0.611  0.263  0.285]  time 0.07s 
Epoch 15/600 9/37 loss: 1.1595 acc [ 0.628  0.270  0.296]  time 0.06s 
Epoch 15/600 10/37 loss: 1.1571 acc [ 0.610  0.270  0.300]  time 0.06s 
Epoch 15/600 11/37 loss: 1.1629 acc [ 0.618  0.272  0.305]  time 0.06s 
Epoch 15/600 12/37 loss: 1.1689 acc [ 0.612  0.265  0.299]  time 0.06s 
Epoch 15/600 13/37 loss: 1.1679 acc [ 0.611  0.261  0.297]  time 0.07s 
Epoch 15/600 14/37 loss: 1.1731 acc [ 0.597  0.261  0.294]  time 0.06s 
Epoch 15/600 15/37 loss: 1.1640 acc [ 0.602  0.259  0.297]  time 0.06s 
Epoch 15/600 16/37 loss: 1.1636 acc [ 0.610  0.261  0.299]  time 0.06s 
Epoch 15/600 17/37 loss: 1.1571 acc [ 0.609  0.265  0.303]  time 0.06s 
Epoch 15/600 18/37 loss: 1.1599 acc [ 0.605  0.263  0.299]  time 0.06s 
Epoch 15/600 19/37 loss: 1.1599 acc [ 0.610  0.258  0.295]  time 0.06s 
Epoch 15/600 20/37 loss: 1.1626 acc [ 0.605  0.253  0.291]  time 0.06s 
Epoch 15/600 21/37 loss: 1.1519 acc [ 0.608  0.259  0.294]  time 0.06s 
Epoch 15/600 22/37 loss: 1.1437 acc [ 0.607  0.258  0.294]  time 0.06s 
Epoch 15/600 23/37 loss: 1.1374 acc [ 0.609  0.262  0.298]  time 0.06s 
Epoch 15/600 24/37 loss: 1.1331 acc [ 0.608  0.262  0.295]  time 0.06s 
Epoch 15/600 25/37 loss: 1.1314 acc [ 0.613  0.254  0.285]  time 0.06s 
Epoch 15/600 26/37 loss: 1.1308 acc [ 0.611  0.250  0.279]  time 0.06s 
Epoch 15/600 27/37 loss: 1.1352 acc [ 0.606  0.248  0.275]  time 0.07s 
Epoch 15/600 28/37 loss: 1.1256 acc [ 0.603  0.253  0.279]  time 0.06s 
Epoch 15/600 29/37 loss: 1.1234 acc [ 0.606  0.249  0.278]  time 0.06s 
Epoch 15/600 30/37 loss: 1.1203 acc [ 0.610  0.250  0.278]  time 0.06s 
Epoch 15/600 31/37 loss: 1.1241 acc [ 0.613  0.250  0.277]  time 0.06s 
Epoch 15/600 32/37 loss: 1.1305 acc [ 0.609  0.247  0.273]  time 0.06s 
Epoch 15/600 33/37 loss: 1.1194 acc [ 0.615  0.258  0.283]  time 0.06s 
Epoch 15/600 34/37 loss: 1.1156 acc [ 0.617  0.260  0.285]  time 0.06s 
Epoch 15/600 35/37 loss: 1.1163 acc [ 0.619  0.265  0.284]  time 0.05s 
Epoch 15/600 36/37 loss: 1.1183 acc [ 0.616  0.260  0.283]  time 0.04s 
Final training  15/599 loss: 1.1183 acc_avg: 0.3860 acc [ 0.616  0.260  0.283] time 2.45s  lr: 1.9980e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 15, 'best_metric': -1}, save_time 0.14s
Estimated remaining training time for the current model fold 0 is 20.08 hr, running time 0.31 hr, est total time 20.39 hr 

Epoch 16/600 0/37 loss: 1.0354 acc [ 0.718  0.241  0.311]  time 0.33s 
Epoch 16/600 1/37 loss: 1.1681 acc [ 0.780  0.231  0.251]  time 0.06s 
Epoch 16/600 2/37 loss: 1.2219 acc [ 0.669  0.178  0.198]  time 0.06s 
Epoch 16/600 3/37 loss: 1.1983 acc [ 0.671  0.209  0.223]  time 0.07s 
Epoch 16/600 4/37 loss: 1.1553 acc [ 0.682  0.200  0.227]  time 0.06s 
Epoch 16/600 5/37 loss: 1.1253 acc [ 0.677  0.220  0.254]  time 0.07s 
Epoch 16/600 6/37 loss: 1.1204 acc [ 0.671  0.218  0.249]  time 0.07s 
Epoch 16/600 7/37 loss: 1.1513 acc [ 0.661  0.214  0.240]  time 0.06s 
Epoch 16/600 8/37 loss: 1.1436 acc [ 0.659  0.215  0.238]  time 0.06s 
Epoch 16/600 9/37 loss: 1.1418 acc [ 0.636  0.217  0.245]  time 0.06s 
Epoch 16/600 10/37 loss: 1.1276 acc [ 0.641  0.225  0.256]  time 0.06s 
Epoch 16/600 11/37 loss: 1.1332 acc [ 0.644  0.238  0.268]  time 0.06s 
Epoch 16/600 12/37 loss: 1.1216 acc [ 0.646  0.251  0.284]  time 0.06s 
Epoch 16/600 13/37 loss: 1.1048 acc [ 0.655  0.260  0.296]  time 0.06s 
Epoch 16/600 14/37 loss: 1.1038 acc [ 0.656  0.278  0.317]  time 0.06s 
Epoch 16/600 15/37 loss: 1.1014 acc [ 0.662  0.288  0.327]  time 0.06s 
Epoch 16/600 16/37 loss: 1.1264 acc [ 0.665  0.283  0.318]  time 0.31s 
Epoch 16/600 17/37 loss: 1.1166 acc [ 0.663  0.286  0.320]  time 0.06s 
Epoch 16/600 18/37 loss: 1.1240 acc [ 0.664  0.285  0.317]  time 0.06s 
Epoch 16/600 19/37 loss: 1.1330 acc [ 0.662  0.289  0.322]  time 0.06s 
Epoch 16/600 20/37 loss: 1.1369 acc [ 0.655  0.283  0.318]  time 0.06s 
Epoch 16/600 21/37 loss: 1.1331 acc [ 0.653  0.282  0.315]  time 0.06s 
Epoch 16/600 22/37 loss: 1.1294 acc [ 0.656  0.290  0.326]  time 0.06s 
Epoch 16/600 23/37 loss: 1.1258 acc [ 0.655  0.288  0.323]  time 0.06s 
Epoch 16/600 24/37 loss: 1.1294 acc [ 0.650  0.286  0.318]  time 0.06s 
Epoch 16/600 25/37 loss: 1.1326 acc [ 0.654  0.282  0.313]  time 0.06s 
Epoch 16/600 26/37 loss: 1.1301 acc [ 0.650  0.282  0.311]  time 0.06s 
Epoch 16/600 27/37 loss: 1.1223 acc [ 0.655  0.289  0.320]  time 0.07s 
Epoch 16/600 28/37 loss: 1.1181 acc [ 0.659  0.294  0.324]  time 0.06s 
Epoch 16/600 29/37 loss: 1.1192 acc [ 0.660  0.289  0.320]  time 0.06s 
Epoch 16/600 30/37 loss: 1.1358 acc [ 0.659  0.286  0.320]  time 0.06s 
Epoch 16/600 31/37 loss: 1.1350 acc [ 0.658  0.280  0.317]  time 0.06s 
Epoch 16/600 32/37 loss: 1.1347 acc [ 0.660  0.282  0.318]  time 0.05s 
Epoch 16/600 33/37 loss: 1.1349 acc [ 0.660  0.282  0.317]  time 0.05s 
Epoch 16/600 34/37 loss: 1.1325 acc [ 0.657  0.282  0.316]  time 0.05s 
Epoch 16/600 35/37 loss: 1.1313 acc [ 0.656  0.282  0.315]  time 0.06s 
Epoch 16/600 36/37 loss: 1.1277 acc [ 0.658  0.285  0.318]  time 0.04s 
Final training  16/599 loss: 1.1277 acc_avg: 0.4203 acc [ 0.658  0.285  0.318] time 2.71s  lr: 1.9977e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 16, 'best_metric': -1}, save_time 0.14s
Estimated remaining training time for the current model fold 0 is 20.12 hr, running time 0.31 hr, est total time 20.43 hr 

Epoch 17/600 0/37 loss: 1.1667 acc [ 0.564  0.171  0.177]  time 0.29s 
Epoch 17/600 1/37 loss: 1.0864 acc [ 0.636  0.177  0.172]  time 0.06s 
Epoch 17/600 2/37 loss: 1.0814 acc [ 0.668  0.264  0.257]  time 0.06s 
Epoch 17/600 3/37 loss: 1.0920 acc [ 0.705  0.220  0.211]  time 0.06s 
Epoch 17/600 4/37 loss: 1.0789 acc [ 0.723  0.258  0.248]  time 0.06s 
Epoch 17/600 5/37 loss: 1.0659 acc [ 0.740  0.265  0.256]  time 0.06s 
Epoch 17/600 6/37 loss: 1.0700 acc [ 0.746  0.254  0.246]  time 0.06s 
Epoch 17/600 7/37 loss: 1.0772 acc [ 0.733  0.230  0.224]  time 0.06s 
Epoch 17/600 8/37 loss: 1.0730 acc [ 0.724  0.231  0.232]  time 0.06s 
Epoch 17/600 9/37 loss: 1.0697 acc [ 0.717  0.229  0.229]  time 0.06s 
Epoch 17/600 10/37 loss: 1.0969 acc [ 0.701  0.211  0.215]  time 0.06s 
Epoch 17/600 11/37 loss: 1.0834 acc [ 0.706  0.238  0.243]  time 0.06s 
Epoch 17/600 12/37 loss: 1.0863 acc [ 0.703  0.231  0.246]  time 0.06s 
Epoch 17/600 13/37 loss: 1.0847 acc [ 0.702  0.235  0.250]  time 0.06s 
Epoch 17/600 14/37 loss: 1.0726 acc [ 0.700  0.242  0.261]  time 0.06s 
Epoch 17/600 15/37 loss: 1.0838 acc [ 0.701  0.253  0.280]  time 0.06s 
Epoch 17/600 16/37 loss: 1.0798 acc [ 0.696  0.259  0.286]  time 0.06s 
Epoch 17/600 17/37 loss: 1.0869 acc [ 0.680  0.252  0.274]  time 0.06s 
Epoch 17/600 18/37 loss: 1.0710 acc [ 0.686  0.259  0.285]  time 0.06s 
Epoch 17/600 19/37 loss: 1.0768 acc [ 0.682  0.257  0.282]  time 0.06s 
Epoch 17/600 20/37 loss: 1.0820 acc [ 0.666  0.253  0.276]  time 0.06s 
Epoch 17/600 21/37 loss: 1.0712 acc [ 0.671  0.250  0.274]  time 0.06s 
Epoch 17/600 22/37 loss: 1.0791 acc [ 0.662  0.247  0.270]  time 0.06s 
Epoch 17/600 23/37 loss: 1.0809 acc [ 0.662  0.244  0.269]  time 0.06s 
Epoch 17/600 24/37 loss: 1.0804 acc [ 0.663  0.239  0.265]  time 0.06s 
Epoch 17/600 25/37 loss: 1.0760 acc [ 0.659  0.236  0.263]  time 0.06s 
Epoch 17/600 26/37 loss: 1.0805 acc [ 0.651  0.235  0.262]  time 0.06s 
Epoch 17/600 27/37 loss: 1.0824 acc [ 0.650  0.231  0.258]  time 0.06s 
Epoch 17/600 28/37 loss: 1.0845 acc [ 0.649  0.232  0.258]  time 0.07s 
Epoch 17/600 29/37 loss: 1.0877 acc [ 0.651  0.228  0.251]  time 0.06s 
Epoch 17/600 30/37 loss: 1.0884 acc [ 0.657  0.235  0.259]  time 0.06s 
Epoch 17/600 31/37 loss: 1.0860 acc [ 0.658  0.238  0.263]  time 0.06s 
Epoch 17/600 32/37 loss: 1.0826 acc [ 0.658  0.239  0.263]  time 0.06s 
Epoch 17/600 33/37 loss: 1.0790 acc [ 0.659  0.238  0.263]  time 0.06s 
Epoch 17/600 34/37 loss: 1.0795 acc [ 0.660  0.239  0.264]  time 0.06s 
Epoch 17/600 35/37 loss: 1.0790 acc [ 0.664  0.238  0.263]  time 0.06s 
Epoch 17/600 36/37 loss: 1.0800 acc [ 0.668  0.238  0.261]  time 0.04s 
Final training  17/599 loss: 1.0800 acc_avg: 0.3889 acc [ 0.668  0.238  0.261] time 2.42s  lr: 1.9973e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 17, 'best_metric': -1}, save_time 0.14s
Estimated remaining training time for the current model fold 0 is 20.07 hr, running time 0.31 hr, est total time 20.39 hr 

Epoch 18/600 0/37 loss: 1.1441 acc [ 0.779  0.058  0.080]  time 0.32s 
Epoch 18/600 1/37 loss: 1.1996 acc [ 0.743  0.118  0.166]  time 0.06s 
Epoch 18/600 2/37 loss: 1.1860 acc [ 0.741  0.181  0.235]  time 0.06s 
Epoch 18/600 3/37 loss: 1.1296 acc [ 0.772  0.220  0.260]  time 0.06s 
Epoch 18/600 4/37 loss: 1.1561 acc [ 0.764  0.216  0.242]  time 0.07s 
Epoch 18/600 5/37 loss: 1.1545 acc [ 0.737  0.218  0.236]  time 0.06s 
Epoch 18/600 6/37 loss: 1.1114 acc [ 0.727  0.238  0.252]  time 0.06s 
Epoch 18/600 7/37 loss: 1.0917 acc [ 0.725  0.269  0.285]  time 0.06s 
Epoch 18/600 8/37 loss: 1.0871 acc [ 0.724  0.264  0.276]  time 0.11s 
Epoch 18/600 9/37 loss: 1.0902 acc [ 0.716  0.250  0.268]  time 0.06s 
Epoch 18/600 10/37 loss: 1.1106 acc [ 0.710  0.240  0.268]  time 0.07s 
Epoch 18/600 11/37 loss: 1.1257 acc [ 0.695  0.230  0.255]  time 0.06s 
Epoch 18/600 12/37 loss: 1.1042 acc [ 0.695  0.230  0.263]  time 0.06s 
Epoch 18/600 13/37 loss: 1.1015 acc [ 0.682  0.241  0.274]  time 0.06s 
Epoch 18/600 14/37 loss: 1.1025 acc [ 0.683  0.244  0.276]  time 0.06s 
Epoch 18/600 15/37 loss: 1.0953 acc [ 0.680  0.240  0.275]  time 0.06s 
Epoch 18/600 16/37 loss: 1.0928 acc [ 0.683  0.238  0.271]  time 0.06s 
Epoch 18/600 17/37 loss: 1.0811 acc [ 0.676  0.242  0.273]  time 0.07s 
Epoch 18/600 18/37 loss: 1.0729 acc [ 0.674  0.245  0.280]  time 0.06s 
Epoch 18/600 19/37 loss: 1.0646 acc [ 0.676  0.248  0.285]  time 0.06s 
Epoch 18/600 20/37 loss: 1.0697 acc [ 0.678  0.251  0.286]  time 0.06s 
Epoch 18/600 21/37 loss: 1.0662 acc [ 0.679  0.262  0.297]  time 0.06s 
Epoch 18/600 22/37 loss: 1.0837 acc [ 0.680  0.256  0.289]  time 0.06s 
Epoch 18/600 23/37 loss: 1.0869 acc [ 0.671  0.256  0.294]  time 0.07s 
Epoch 18/600 24/37 loss: 1.0930 acc [ 0.668  0.248  0.288]  time 0.06s 
Epoch 18/600 25/37 loss: 1.0901 acc [ 0.668  0.250  0.283]  time 0.06s 
Epoch 18/600 26/37 loss: 1.0933 acc [ 0.673  0.252  0.285]  time 0.06s 
Epoch 18/600 27/37 loss: 1.0927 acc [ 0.675  0.248  0.281]  time 0.06s 
Epoch 18/600 28/37 loss: 1.0864 acc [ 0.681  0.248  0.282]  time 0.06s 
Epoch 18/600 29/37 loss: 1.0916 acc [ 0.683  0.249  0.279]  time 0.06s 
Epoch 18/600 30/37 loss: 1.0980 acc [ 0.677  0.239  0.268]  time 0.06s 
Epoch 18/600 31/37 loss: 1.1035 acc [ 0.675  0.238  0.266]  time 0.05s 
Epoch 18/600 32/37 loss: 1.1027 acc [ 0.670  0.243  0.269]  time 0.06s 
Epoch 18/600 33/37 loss: 1.1019 acc [ 0.675  0.241  0.264]  time 0.06s 
Epoch 18/600 34/37 loss: 1.1002 acc [ 0.672  0.244  0.264]  time 0.06s 
Epoch 18/600 35/37 loss: 1.1156 acc [ 0.673  0.246  0.266]  time 0.05s 
Epoch 18/600 36/37 loss: 1.1121 acc [ 0.673  0.249  0.269]  time 0.04s 
Final training  18/599 loss: 1.1121 acc_avg: 0.3968 acc [ 0.673  0.249  0.269] time 2.52s  lr: 1.9969e-04
Saving checkpoint process: /workspace/workspace-kits23/exp/kits23-train_val/segresnet2d_0/model/model_final.pt, {'epoch': 18, 'best_metric': -1}, save_time 0.14s
Estimated remaining training time for the current model fold 0 is 20.09 hr, running time 0.32 hr, est total time 20.40 hr 

Epoch 19/600 0/37 loss: 1.0687 acc [ 0.855  0.072  0.167]  time 0.29s 
Epoch 19/600 1/37 loss: 1.2832 acc [ 0.742  0.274  0.356]  time 0.06s 
Epoch 19/600 2/37 loss: 1.2986 acc [ 0.689  0.280  0.324]  time 0.06s 
Epoch 19/600 3/37 loss: 1.2310 acc [ 0.676  0.281  0.333]  time 0.07s 
Epoch 19/600 4/37 loss: 1.1986 acc [ 0.683  0.274  0.326]  time 0.06s 
Epoch 19/600 5/37 loss: 1.1936 acc [ 0.668  0.296  0.333]  time 0.06s 
Epoch 19/600 6/37 loss: 1.1589 acc [ 0.689  0.286  0.322]  time 0.06s 
Epoch 19/600 7/37 loss: 1.1574 acc [ 0.695  0.286  0.319]  time 0.06s 
Epoch 19/600 8/37 loss: 1.1476 acc [ 0.689  0.295  0.328]  time 0.06s 
Epoch 19/600 9/37 loss: 1.1512 acc [ 0.683  0.283  0.311]  time 0.06s 
Epoch 19/600 10/37 loss: 1.1432 acc [ 0.694  0.285  0.306]  time 0.06s 
Epoch 19/600 11/37 loss: 1.1188 acc [ 0.698  0.295  0.316]  time 0.06s 
Epoch 19/600 12/37 loss: 1.1002 acc [ 0.692  0.307  0.328]  time 0.06s 
Epoch 19/600 13/37 loss: 1.0760 acc [ 0.700  0.312  0.342]  time 0.06s 
Epoch 19/600 14/37 loss: 1.0853 acc [ 0.696  0.318  0.341]  time 0.06s 
Epoch 19/600 15/37 loss: 1.0743 acc [ 0.701  0.320  0.341]  time 0.06s 
Epoch 19/600 16/37 loss: 1.0686 acc [ 0.701  0.325  0.344]  time 0.06s 
Epoch 19/600 17/37 loss: 1.0637 acc [ 0.700  0.317  0.333]  time 0.06s 
Epoch 19/600 18/37 loss: 1.0666 acc [ 0.704  0.315  0.331]  time 0.06s 
Epoch 19/600 19/37 loss: 1.1078 acc [ 0.702  0.312  0.327]  time 0.06s 
Epoch 19/600 20/37 loss: 1.1341 acc [ 0.691  0.297  0.310]  time 0.06s 
Epoch 19/600 21/37 loss: 1.1358 acc [ 0.688  0.293  0.308]  time 0.06s 
Epoch 19/600 22/37 loss: 1.1321 acc [ 0.688  0.293  0.306]  time 0.06s 
Epoch 19/600 23/37 loss: 1.1299 acc [ 0.677  0.295  0.308]  time 0.06s 
Epoch 19/600 24/37 loss: 1.1219 acc [ 0.680  0.300  0.312]  time 0.06s 
Epoch 19/600 25/37 loss: 1.1223 acc [ 0.679  0.297  0.308]  time 0.06s 
Epoch 19/600 26/37 loss: 1.1143 acc [ 0.681  0.297  0.308]  time 0.06s 
Epoch 19/600 27/37 loss: 1.1073 acc [ 0.676  0.298  0.309]  time 0.06s 
Epoch 19/600 28/37 loss: 1.1037 acc [ 0.676  0.293  0.306]  time 0.06s 
Epoch 19/600 29/37 loss: 1.0999 acc [ 0.679  0.295  0.310]  time 0.06s 
Epoch 19/600 30/37 loss: 1.0975 acc [ 0.678  0.288  0.306]  time 0.06s 
Epoch 19/600 31/37 loss: 1.0914 acc [ 0.677  0.290  0.308]  time 0.05s 
Epoch 19/600 32/37 loss: 1.0859 acc [ 0.678  0.288  0.312]  time 0.06s 
Epoch 19/600 33/37 loss: 1.0887 acc [ 0.676  0.281  0.305]  time 0.06s 
Epoch 19/600 34/37 loss: 1.0891 acc [ 0.673  0.277  0.300]  time 0.05s 
Epoch 19/600 35/37 loss: 1.0859 acc [ 0.676  0.274  0.293]  time 0.06s 
Epoch 19/600 36/37 loss: 1.0903 acc [ 0.674  0.272  0.293]  time 0.04s 
Final training  19/599 loss: 1.0903 acc_avg: 0.4132 acc [ 0.674  0.272  0.293] time 2.37s  lr: 1.9965e-04
Val 19/600 0/196  loss: 0.8872 acc [ 0.250  0.036  0.039]  time 0.75s ['/workspace/workspace-kits23/kits23/dataset/case_00004/imaging.nii.gz']
Val 19/600 1/196  loss: 0.8963 acc [ 0.145  0.011  0.010]  time 17.51s ['/workspace/workspace-kits23/kits23/dataset/case_00005/imaging.nii.gz']
Val 19/600 2/196  loss: 0.9506 acc [ 0.105  0.005  0.000]  time 2.85s ['/workspace/workspace-kits23/kits23/dataset/case_00006/imaging.nii.gz']
Val 19/600 3/196  loss: 0.9379 acc [ 0.135  0.000  0.000]  time 1.20s ['/workspace/workspace-kits23/kits23/dataset/case_00011/imaging.nii.gz']
Val 19/600 4/196  loss: 0.9518 acc [ 0.118  0.000  0.000]  time 1.81s ['/workspace/workspace-kits23/kits23/dataset/case_00017/imaging.nii.gz']
Val 19/600 5/196  loss: 0.8352 acc [ 0.149  0.216  0.222]  time 2.03s ['/workspace/workspace-kits23/kits23/dataset/case_00029/imaging.nii.gz']
Val 19/600 6/196  loss: 0.9492 acc [ 0.113  0.000  0.000]  time 2.39s ['/workspace/workspace-kits23/kits23/dataset/case_00031/imaging.nii.gz']
Val 19/600 7/196  loss: 0.9390 acc [ 0.148  0.000  0.000]  time 1.82s ['/workspace/workspace-kits23/kits23/dataset/case_00034/imaging.nii.gz']
Val 19/600 8/196  loss: 0.8722 acc [ 0.112  0.154  0.160]  time 2.69s ['/workspace/workspace-kits23/kits23/dataset/case_00047/imaging.nii.gz']
Val 19/600 9/196  loss: 0.9279 acc [ 0.181  0.000  0.000]  time 1.03s ['/workspace/workspace-kits23/kits23/dataset/case_00062/imaging.nii.gz']
Val 19/600 10/196  loss: 0.9090 acc [ 0.227  0.008  0.009]  time 1.62s ['/workspace/workspace-kits23/kits23/dataset/case_00065/imaging.nii.gz']
Val 19/600 11/196  loss: 0.9405 acc [ 0.083  0.018  0.018]  time 9.07s ['/workspace/workspace-kits23/kits23/dataset/case_00066/imaging.nii.gz']
Val 19/600 12/196  loss: 0.7851 acc [ 0.256  0.048  0.045]  time 5.88s ['/workspace/workspace-kits23/kits23/dataset/case_00067/imaging.nii.gz']
Val 19/600 13/196  loss: 0.9250 acc [ 0.160  0.035  0.036]  time 1.15s ['/workspace/workspace-kits23/kits23/dataset/case_00085/imaging.nii.gz']
Val 19/600 14/196  loss: 0.7215 acc [ 0.191  0.400  0.404]  time 0.94s ['/workspace/workspace-kits23/kits23/dataset/case_00090/imaging.nii.gz']
Val 19/600 15/196  loss: 0.7160 acc [ 0.239  0.286  0.295]  time 1.89s ['/workspace/workspace-kits23/kits23/dataset/case_00092/imaging.nii.gz']
Val 19/600 16/196  loss: 0.8439 acc [ 0.143  0.006  0.004]  time 6.08s ['/workspace/workspace-kits23/kits23/dataset/case_00102/imaging.nii.gz']
Val 19/600 17/196  loss: 0.6327 acc [ 0.213  0.469  0.468]  time 1.09s ['/workspace/workspace-kits23/kits23/dataset/case_00107/imaging.nii.gz']
Val 19/600 18/196  loss: 0.8121 acc [ 0.325  0.105  0.081]  time 0.58s ['/workspace/workspace-kits23/kits23/dataset/case_00110/imaging.nii.gz']
Val 19/600 19/196  loss: 0.7606 acc [ 0.408  0.134  0.132]  time 1.02s ['/workspace/workspace-kits23/kits23/dataset/case_00117/imaging.nii.gz']
Val 19/600 20/196  loss: 0.8999 acc [ 0.162  0.038  0.030]  time 1.61s ['/workspace/workspace-kits23/kits23/dataset/case_00119/imaging.nii.gz']
Val 19/600 21/196  loss: 0.9586 acc [ 0.095  0.000  0.000]  time 3.92s ['/workspace/workspace-kits23/kits23/dataset/case_00128/imaging.nii.gz']
Val 19/600 22/196  loss: 0.9156 acc [ 0.222  0.007  0.006]  time 11.27s ['/workspace/workspace-kits23/kits23/dataset/case_00132/imaging.nii.gz']
Val 19/600 23/196  loss: 0.9082 acc [ 0.179  0.005  0.005]  time 4.71s ['/workspace/workspace-kits23/kits23/dataset/case_00142/imaging.nii.gz']
Val 19/600 24/196  loss: 0.8884 acc [ 0.221  0.046  0.045]  time 1.09s ['/workspace/workspace-kits23/kits23/dataset/case_00147/imaging.nii.gz']
Val 19/600 25/196  loss: 0.8731 acc [ 0.206  0.003  0.002]  time 17.35s ['/workspace/workspace-kits23/kits23/dataset/case_00159/imaging.nii.gz']
Val 19/600 26/196  loss: 0.9409 acc [ 0.136  0.003  0.002]  time 1.15s ['/workspace/workspace-kits23/kits23/dataset/case_00164/imaging.nii.gz']
Val 19/600 27/196  loss: 0.9758 acc [ 0.059  0.003  0.000]  time 15.48s ['/workspace/workspace-kits23/kits23/dataset/case_00165/imaging.nii.gz']
Val 19/600 28/196  loss: 0.6416 acc [ 0.255  0.444  0.449]  time 1.33s ['/workspace/workspace-kits23/kits23/dataset/case_00172/imaging.nii.gz']
Val 19/600 29/196  loss: 0.9372 acc [ 0.130  0.029  0.023]  time 1.62s ['/workspace/workspace-kits23/kits23/dataset/case_00173/imaging.nii.gz']
Val 19/600 30/196  loss: 0.9383 acc [ 0.126  0.012  0.008]  time 5.38s ['/workspace/workspace-kits23/kits23/dataset/case_00185/imaging.nii.gz']
Val 19/600 31/196  loss: 0.8806 acc [ 0.144  0.009  0.009]  time 2.69s ['/workspace/workspace-kits23/kits23/dataset/case_00189/imaging.nii.gz']
Val 19/600 32/196  loss: 0.6519 acc [ 0.218  0.475  0.482]  time 2.16s ['/workspace/workspace-kits23/kits23/dataset/case_00192/imaging.nii.gz']
Val 19/600 33/196  loss: 0.8644 acc [ 0.114  0.121  0.122]  time 3.21s ['/workspace/workspace-kits23/kits23/dataset/case_00197/imaging.nii.gz']
Val 19/600 34/196  loss: 0.8771 acc [ 0.320  0.000  0.000]  time 1.90s ['/workspace/workspace-kits23/kits23/dataset/case_00201/imaging.nii.gz']
Val 19/600 35/196  loss: 0.9431 acc [ 0.121  0.000  0.000]  time 1.60s ['/workspace/workspace-kits23/kits23/dataset/case_00208/imaging.nii.gz']
Val 19/600 36/196  loss: 0.9713 acc [ 0.062  0.005  0.007]  time 11.88s ['/workspace/workspace-kits23/kits23/dataset/case_00213/imaging.nii.gz']
Val 19/600 37/196  loss: 0.9388 acc [ 0.127  0.005  0.002]  time 3.21s ['/workspace/workspace-kits23/kits23/dataset/case_00218/imaging.nii.gz']
Val 19/600 38/196  loss: 0.9092 acc [ 0.177  0.066  0.066]  time 1.91s ['/workspace/workspace-kits23/kits23/dataset/case_00225/imaging.nii.gz']
Val 19/600 39/196  loss: 0.8375 acc [ 0.150  0.193  0.198]  time 1.72s ['/workspace/workspace-kits23/kits23/dataset/case_00230/imaging.nii.gz']
Val 19/600 40/196  loss: 0.7783 acc [ 0.270  0.098  0.096]  time 1.71s ['/workspace/workspace-kits23/kits23/dataset/case_00233/imaging.nii.gz']
Val 19/600 41/196  loss: 0.7551 acc [ 0.157  0.214  0.208]  time 3.08s ['/workspace/workspace-kits23/kits23/dataset/case_00245/imaging.nii.gz']
Val 19/600 42/196  loss: 0.8555 acc [ 0.112  0.081  0.083]  time 13.56s ['/workspace/workspace-kits23/kits23/dataset/case_00246/imaging.nii.gz']
Val 19/600 43/196  loss: 0.9512 acc [ 0.111  0.000  0.000]  time 4.79s ['/workspace/workspace-kits23/kits23/dataset/case_00250/imaging.nii.gz']
Val 19/600 44/196  loss: 0.7899 acc [ 0.300  0.417  0.001]  time 1.14s ['/workspace/workspace-kits23/kits23/dataset/case_00256/imaging.nii.gz']
Val 19/600 45/196  loss: 0.8270 acc [ 0.188  0.160  0.158]  time 0.54s ['/workspace/workspace-kits23/kits23/dataset/case_00260/imaging.nii.gz']
Val 19/600 46/196  loss: 0.9047 acc [ 0.152  0.002  0.002]  time 13.24s ['/workspace/workspace-kits23/kits23/dataset/case_00261/imaging.nii.gz']
Val 19/600 47/196  loss: 0.9255 acc [ 0.152  0.002  0.002]  time 4.82s ['/workspace/workspace-kits23/kits23/dataset/case_00273/imaging.nii.gz']
Val 19/600 48/196  loss: 0.8293 acc [ 0.201  0.174  0.188]  time 1.72s ['/workspace/workspace-kits23/kits23/dataset/case_00275/imaging.nii.gz']
Val 19/600 49/196  loss: 0.8564 acc [ 0.114  0.174  0.184]  time 4.82s ['/workspace/workspace-kits23/kits23/dataset/case_00284/imaging.nii.gz']
Val 19/600 50/196  loss: 0.8697 acc [ 0.305  0.000  0.000]  time 0.55s ['/workspace/workspace-kits23/kits23/dataset/case_00287/imaging.nii.gz']
Val 19/600 51/196  loss: 0.9169 acc [ 0.175  0.002  0.001]  time 13.56s ['/workspace/workspace-kits23/kits23/dataset/case_00290/imaging.nii.gz']
Val 19/600 52/196  loss: 0.9247 acc [ 0.113  0.000  0.000]  time 2.26s ['/workspace/workspace-kits23/kits23/dataset/case_00291/imaging.nii.gz']
Val 19/600 53/196  loss: 0.9478 acc [ 0.111  0.001  0.001]  time 13.39s ['/workspace/workspace-kits23/kits23/dataset/case_00294/imaging.nii.gz']
Val 19/600 54/196  loss: 0.9130 acc [ 0.223  0.000  0.000]  time 12.47s ['/workspace/workspace-kits23/kits23/dataset/case_00295/imaging.nii.gz']
Val 19/600 55/196  loss: 0.8329 acc [ 0.211  0.080  0.076]  time 14.23s ['/workspace/workspace-kits23/kits23/dataset/case_00298/imaging.nii.gz']
Val 19/600 56/196  loss: 0.9646 acc [ 0.074  0.014  0.000]  time 2.03s ['/workspace/workspace-kits23/kits23/dataset/case_00400/imaging.nii.gz']
Val 19/600 57/196  loss: 0.8040 acc [ 0.311  0.254  0.081]  time 0.61s ['/workspace/workspace-kits23/kits23/dataset/case_00403/imaging.nii.gz']
Val 19/600 58/196  loss: 0.9365 acc [ 0.095  0.026  0.008]  time 2.69s ['/workspace/workspace-kits23/kits23/dataset/case_00404/imaging.nii.gz']
Val 19/600 59/196  loss: 0.9348 acc [ 0.160  0.001  0.001]  time 1.82s ['/workspace/workspace-kits23/kits23/dataset/case_00414/imaging.nii.gz']
Val 19/600 60/196  loss: 0.8513 acc [ 0.318  0.098  0.096]  time 2.53s ['/workspace/workspace-kits23/kits23/dataset/case_00415/imaging.nii.gz']
Val 19/600 61/196  loss: 0.5975 acc [ 0.441  0.375  0.375]  time 0.58s ['/workspace/workspace-kits23/kits23/dataset/case_00418/imaging.nii.gz']
Val 19/600 62/196  loss: 0.8918 acc [ 0.280  0.001  0.001]  time 0.58s ['/workspace/workspace-kits23/kits23/dataset/case_00422/imaging.nii.gz']
Val 19/600 63/196  loss: 0.7472 acc [ 0.162  0.316  0.327]  time 1.44s ['/workspace/workspace-kits23/kits23/dataset/case_00426/imaging.nii.gz']
Val 19/600 64/196  loss: 0.6804 acc [ 0.505  0.395  0.388]  time 0.52s ['/workspace/workspace-kits23/kits23/dataset/case_00430/imaging.nii.gz']
Val 19/600 65/196  loss: 0.8878 acc [ 0.120  0.034  0.033]  time 2.02s ['/workspace/workspace-kits23/kits23/dataset/case_00431/imaging.nii.gz']
Val 19/600 66/196  loss: 0.9028 acc [ 0.234  0.000  0.000]  time 0.64s ['/workspace/workspace-kits23/kits23/dataset/case_00434/imaging.nii.gz']
Val 19/600 67/196  loss: 0.8582 acc [ 0.216  0.012  0.008]  time 1.14s ['/workspace/workspace-kits23/kits23/dataset/case_00439/imaging.nii.gz']
Val 19/600 68/196  loss: 0.9525 acc [ 0.120  0.000  0.000]  time 4.05s ['/workspace/workspace-kits23/kits23/dataset/case_00447/imaging.nii.gz']
Val 19/600 69/196  loss: 0.8697 acc [ 0.197  0.099  0.102]  time 1.34s ['/workspace/workspace-kits23/kits23/dataset/case_00458/imaging.nii.gz']
Val 19/600 70/196  loss: 0.9534 acc [ 0.100  0.013  0.012]  time 1.94s ['/workspace/workspace-kits23/kits23/dataset/case_00462/imaging.nii.gz']
Val 19/600 71/196  loss: 0.8430 acc [ 0.358  0.004  0.005]  time 0.66s ['/workspace/workspace-kits23/kits23/dataset/case_00464/imaging.nii.gz']
Val 19/600 72/196  loss: 0.8437 acc [ 0.226  0.152  0.021]  time 0.59s ['/workspace/workspace-kits23/kits23/dataset/case_00470/imaging.nii.gz']
Val 19/600 73/196  loss: 0.8400 acc [ 0.227  0.176  0.196]  time 1.84s ['/workspace/workspace-kits23/kits23/dataset/case_00475/imaging.nii.gz']
Val 19/600 74/196  loss: 0.8815 acc [ 0.260  0.013  0.014]  time 0.56s ['/workspace/workspace-kits23/kits23/dataset/case_00476/imaging.nii.gz']
Val 19/600 75/196  loss: 0.9012 acc [ 0.207  0.009  0.009]  time 1.10s ['/workspace/workspace-kits23/kits23/dataset/case_00485/imaging.nii.gz']
Val 19/600 76/196  loss: 0.8482 acc [ 0.184  0.024  0.025]  time 3.42s ['/workspace/workspace-kits23/kits23/dataset/case_00489/imaging.nii.gz']
Val 19/600 77/196  loss: 0.8482 acc [ 0.147  0.164  0.156]  time 1.63s ['/workspace/workspace-kits23/kits23/dataset/case_00492/imaging.nii.gz']
Val 19/600 78/196  loss: 0.9149 acc [ 0.162  0.002  0.000]  time 1.61s ['/workspace/workspace-kits23/kits23/dataset/case_00494/imaging.nii.gz']
Val 19/600 79/196  loss: 0.9482 acc [ 0.106  0.017  0.000]  time 4.81s ['/workspace/workspace-kits23/kits23/dataset/case_00504/imaging.nii.gz']
Val 19/600 80/196  loss: 0.9581 acc [ 0.082  0.000  0.000]  time 3.75s ['/workspace/workspace-kits23/kits23/dataset/case_00509/imaging.nii.gz']
Val 19/600 81/196  loss: 0.9147 acc [ 0.094  0.020  0.018]  time 16.25s ['/workspace/workspace-kits23/kits23/dataset/case_00510/imaging.nii.gz']
Val 19/600 82/196  loss: 0.8782 acc [ 0.239  0.097  0.004]  time 1.02s ['/workspace/workspace-kits23/kits23/dataset/case_00512/imaging.nii.gz']
Val 19/600 83/196  loss: 0.9583 acc [ 0.184  0.025  0.025]  time 1.84s ['/workspace/workspace-kits23/kits23/dataset/case_00516/imaging.nii.gz']
Val 19/600 84/196  loss: 0.9419 acc [ 0.122  0.014  0.014]  time 1.65s ['/workspace/workspace-kits23/kits23/dataset/case_00520/imaging.nii.gz']
Val 19/600 85/196  loss: 0.9329 acc [ 0.166  0.002  0.001]  time 10.89s ['/workspace/workspace-kits23/kits23/dataset/case_00528/imaging.nii.gz']
Val 19/600 86/196  loss: 0.9300 acc [ 0.115  0.015  0.001]  time 2.41s ['/workspace/workspace-kits23/kits23/dataset/case_00532/imaging.nii.gz']
Val 19/600 87/196  loss: 0.8569 acc [ 0.287  0.006  0.007]  time 0.52s ['/workspace/workspace-kits23/kits23/dataset/case_00534/imaging.nii.gz']
Val 19/600 88/196  loss: 0.9349 acc [ 0.079  0.008  0.006]  time 12.81s ['/workspace/workspace-kits23/kits23/dataset/case_00535/imaging.nii.gz']
Val 19/600 89/196  loss: 0.9770 acc [ 0.029  0.016  0.000]  time 9.79s ['/workspace/workspace-kits23/kits23/dataset/case_00540/imaging.nii.gz']
Val 19/600 90/196  loss: 0.6180 acc [ 0.485  0.355  0.356]  time 0.48s ['/workspace/workspace-kits23/kits23/dataset/case_00546/imaging.nii.gz']
Val 19/600 91/196  loss: 0.7793 acc [ 0.201  0.195  0.202]  time 2.84s ['/workspace/workspace-kits23/kits23/dataset/case_00552/imaging.nii.gz']
Val 19/600 92/196  loss: 0.9221 acc [ 0.132  0.006  0.008]  time 9.23s ['/workspace/workspace-kits23/kits23/dataset/case_00553/imaging.nii.gz']
Val 19/600 93/196  loss: 0.9315 acc [ 0.161  0.002  0.002]  time 3.06s ['/workspace/workspace-kits23/kits23/dataset/case_00557/imaging.nii.gz']
Val 19/600 94/196  loss: 0.8910 acc [ 0.224  0.004  0.003]  time 10.16s ['/workspace/workspace-kits23/kits23/dataset/case_00559/imaging.nii.gz']
Val 19/600 95/196  loss: 0.8358 acc [ 0.433  0.000  0.000]  time 0.52s ['/workspace/workspace-kits23/kits23/dataset/case_00569/imaging.nii.gz']
Val 19/600 96/196  loss: 0.8753 acc [ 0.303  0.001  0.001]  time 0.55s ['/workspace/workspace-kits23/kits23/dataset/case_00574/imaging.nii.gz']
Val 19/600 97/196  loss: 0.8802 acc [ 0.119  0.071  0.001]  time 2.83s ['/workspace/workspace-kits23/kits23/dataset/case_00586/imaging.nii.gz']
Val 19/600 98/196  loss: 0.7888 acc [ 0.182  0.180  0.182]  time 1.71s ['/workspace/workspace-kits23/kits23/dataset/case_00012/imaging.nii.gz']
Val 19/600 99/196  loss: 0.8983 acc [ 0.117  0.045  0.043]  time 1.63s ['/workspace/workspace-kits23/kits23/dataset/case_00013/imaging.nii.gz']
Val 19/600 100/196  loss: 0.9026 acc [ 0.232  0.001  0.001]  time 2.32s ['/workspace/workspace-kits23/kits23/dataset/case_00018/imaging.nii.gz']
Val 19/600 101/196  loss: 0.9032 acc [ 0.235  0.002  0.002]  time 1.52s ['/workspace/workspace-kits23/kits23/dataset/case_00023/imaging.nii.gz']
Val 19/600 102/196  loss: 0.9380 acc [ 0.154  0.000  0.000]  time 15.38s ['/workspace/workspace-kits23/kits23/dataset/case_00027/imaging.nii.gz']
Val 19/600 103/196  loss: 0.9392 acc [ 0.145  0.000  0.000]  time 1.78s ['/workspace/workspace-kits23/kits23/dataset/case_00039/imaging.nii.gz']
Val 19/600 104/196  loss: 0.9435 acc [ 0.116  0.006  0.007]  time 3.96s ['/workspace/workspace-kits23/kits23/dataset/case_00040/imaging.nii.gz']
Val 19/600 105/196  loss: 0.8380 acc [ 0.327  0.058  0.058]  time 0.54s ['/workspace/workspace-kits23/kits23/dataset/case_00041/imaging.nii.gz']
Val 19/600 106/196  loss: 0.8994 acc [ 0.137  0.069  0.063]  time 2.67s ['/workspace/workspace-kits23/kits23/dataset/case_00046/imaging.nii.gz']
Val 19/600 107/196  loss: 0.8538 acc [ 0.204  0.158  0.081]  time 1.81s ['/workspace/workspace-kits23/kits23/dataset/case_00058/imaging.nii.gz']
Val 19/600 108/196  loss: 0.9127 acc [ 0.184  0.019  0.021]  time 15.18s ['/workspace/workspace-kits23/kits23/dataset/case_00059/imaging.nii.gz']
Val 19/600 109/196  loss: 0.6872 acc [ 0.533  0.078  0.084]  time 0.57s ['/workspace/workspace-kits23/kits23/dataset/case_00061/imaging.nii.gz']
Val 19/600 110/196  loss: 0.8815 acc [ 0.172  0.134  0.000]  time 1.71s ['/workspace/workspace-kits23/kits23/dataset/case_00069/imaging.nii.gz']
Val 19/600 111/196  loss: 0.7921 acc [ 0.169  0.216  0.218]  time 2.82s ['/workspace/workspace-kits23/kits23/dataset/case_00073/imaging.nii.gz']
Val 19/600 112/196  loss: 0.7560 acc [ 0.308  0.030  0.030]  time 6.01s ['/workspace/workspace-kits23/kits23/dataset/case_00078/imaging.nii.gz']
Val 19/600 113/196  loss: 0.8914 acc [ 0.158  0.106  0.105]  time 1.08s ['/workspace/workspace-kits23/kits23/dataset/case_00080/imaging.nii.gz']
Val 19/600 114/196  loss: 0.9279 acc [ 0.189  0.010  0.009]  time 2.68s ['/workspace/workspace-kits23/kits23/dataset/case_00081/imaging.nii.gz']
Val 19/600 115/196  loss: 0.9453 acc [ 0.124  0.001  0.001]  time 2.01s ['/workspace/workspace-kits23/kits23/dataset/case_00082/imaging.nii.gz']
Val 19/600 116/196  loss: 0.8618 acc [ 0.283  0.002  0.002]  time 0.54s ['/workspace/workspace-kits23/kits23/dataset/case_00089/imaging.nii.gz']
Val 19/600 117/196  loss: 0.9221 acc [ 0.185  0.000  0.000]  time 16.45s ['/workspace/workspace-kits23/kits23/dataset/case_00091/imaging.nii.gz']
Val 19/600 118/196  loss: 0.9204 acc [ 0.207  0.000  0.000]  time 19.40s ['/workspace/workspace-kits23/kits23/dataset/case_00093/imaging.nii.gz']
Val 19/600 119/196  loss: 0.9044 acc [ 0.122  0.115  0.116]  time 6.29s ['/workspace/workspace-kits23/kits23/dataset/case_00095/imaging.nii.gz']
Val 19/600 120/196  loss: 0.9017 acc [ 0.199  0.005  0.004]  time 4.98s ['/workspace/workspace-kits23/kits23/dataset/case_00098/imaging.nii.gz']
Val 19/600 121/196  loss: 0.9129 acc [ 0.224  0.000  0.000]  time 9.33s ['/workspace/workspace-kits23/kits23/dataset/case_00101/imaging.nii.gz']
Val 19/600 122/196  loss: 0.9455 acc [ 0.135  0.012  0.013]  time 1.59s ['/workspace/workspace-kits23/kits23/dataset/case_00106/imaging.nii.gz']
Val 19/600 123/196  loss: 0.9673 acc [ 0.080  0.000  0.000]  time 6.43s ['/workspace/workspace-kits23/kits23/dataset/case_00120/imaging.nii.gz']
Val 19/600 124/196  loss: 0.8456 acc [ 0.349  0.038  0.036]  time 0.61s ['/workspace/workspace-kits23/kits23/dataset/case_00122/imaging.nii.gz']
Val 19/600 125/196  loss: 0.9639 acc [ 0.092  0.000  0.000]  time 3.01s ['/workspace/workspace-kits23/kits23/dataset/case_00125/imaging.nii.gz']
Val 19/600 126/196  loss: 0.9124 acc [ 0.197  0.001  0.000]  time 1.40s ['/workspace/workspace-kits23/kits23/dataset/case_00127/imaging.nii.gz']
